version: '3.8'

# Docker Compose para ambiente GPU (CUDA 12.1)
# Sprint 1: Infraestrutura GPU para RVC + XTTS

services:
  audio-voice-service:
    build:
      context: .
      dockerfile: docker/Dockerfile-gpu
    container_name: audio-voice-api-gpu
    ports:
      - "${PORT:-8005}:8005"
    volumes:
      - ./app:/app/app
      - ./uploads:/app/uploads
      - ./processed:/app/processed
      - ./temp:/app/temp
      - ./voice_profiles:/app/voice_profiles
      - ./models:/app/models
      - ./logs:/app/logs
      - ./tests:/app/tests  # Para rodar testes dentro do container
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - FORCE_CUDA=1
      - MPLCONFIGDIR=/app/temp/.matplotlib
      - XTTS_DEVICE=cuda
      - XTTS_MODEL=tts_models/multilingual/multi-dataset/xtts_v2
      - XTTS_FALLBACK_CPU=true
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import torch; assert torch.cuda.is_available()' && curl -f http://localhost:8005/ || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 120s
    labels:
      - "com.example.service=audio-voice"
      - "com.example.version=2.1.0-gpu"
      - "com.example.sprint=sprint-1"

  celery-worker:
    build:
      context: .
      dockerfile: docker/Dockerfile-gpu
    container_name: audio-voice-celery-gpu
    command: python -m celery -A app.celery_config worker --loglevel=info --concurrency=1 --pool=solo --queues=audio_voice_queue
    volumes:
      - ./app:/app/app
      - ./uploads:/app/uploads
      - ./processed:/app/processed
      - ./temp:/app/temp
      - ./voice_profiles:/app/voice_profiles
      - ./models:/app/models
      - ./logs:/app/logs
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
      - C_FORCE_ROOT=true
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - FORCE_CUDA=1
      - MPLCONFIGDIR=/app/temp/.matplotlib
      - XTTS_DEVICE=cuda
      - XTTS_MODEL=tts_models/multilingual/multi-dataset/xtts_v2
      - XTTS_FALLBACK_CPU=true
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          memory: 8G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    depends_on:
      audio-voice-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "python -m celery -A app.celery_config inspect ping || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 90s
    labels:
      - "com.example.service=audio-voice-worker"
      - "com.example.version=2.1.0-gpu"
      - "com.example.sprint=sprint-1"

# Network padrão para comunicação entre containers
networks:
  default:
    name: audio-voice-network
    driver: bridge
