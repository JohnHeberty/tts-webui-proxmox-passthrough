# ğŸ” AnÃ¡lise: Checkpoints e Samples nÃ£o sendo Gerados

**Data:** 2025-12-06  
**Problema:** Treinamento nÃ£o estÃ¡ salvando checkpoints numerados nem gerando samples de Ã¡udio

---

## ğŸ“Š Estado Atual

### Checkpoints Encontrados
```
train/output/ptbr_finetuned2/
â”œâ”€â”€ model_last.pt                (5.1GB - update 25100)
â”œâ”€â”€ model_last.metadata.json
â”œâ”€â”€ pretrained_model_200000.pt   (5.1GB - modelo base)
â””â”€â”€ samples/                     (vazio!)
```

### ConfiguraÃ§Ã£o Atual
```yaml
# train/config/base_config.yaml
checkpoints:
  save_per_updates: 500          # Salvar checkpoint numerado a cada 500 updates
  last_per_updates: 100          # Salvar model_last.pt a cada 100 updates
  keep_last_n_checkpoints: 3     # Manter Ãºltimos 3 checkpoints
  log_samples: true              # Gerar samples de Ã¡udio
  log_samples_per_updates: 500   # Gerar samples a cada 500 updates
```

---

## ğŸ› Causa Raiz

### Fluxo de Salvamento da Lib F5-TTS

O cÃ³digo em `/root/.local/lib/python3.11/site-packages/f5_tts/model/trainer.py` funciona assim:

```python
# A cada last_per_updates (100 updates)
if global_update % self.last_per_updates == 0:
    self.save_checkpoint(global_update, last=True)  # Salva model_last.pt

# A cada save_per_updates (500 updates)  
if global_update % self.save_per_updates == 0:
    self.save_checkpoint(global_update)  # Salva model_{update}.pt
    
    # E gera samples
    if self.log_samples:
        # Gera audio inference
        torchaudio.save(f"{samples}/update_{global_update}_gen.wav", ...)
        torchaudio.save(f"{samples}/update_{global_update}_ref.wav", ...)
```

### Por que nÃ£o funcionou?

#### 1. Update 25000 (do checkpoint prÃ©-treinado)
- âœ… 25000 % 100 == 0 â†’ Salvou `model_last.pt`
- âŒ 25000 % 500 == 0 â†’ MAS era o checkpoint inicial, nÃ£o gerou sample
- **RazÃ£o:** O update 25000 veio do modelo prÃ©-treinado carregado, nÃ£o de treinamento real

#### 2. Treinamento interrompido em 25188
- âœ… Salvou `model_last.pt` em 25100 (25100 % 100 == 0)
- âŒ NÃ£o chegou em 25500 para salvar checkpoint numerado
- âŒ NÃ£o gerou samples (sÃ³ aconteceria em 25500)

---

## âœ… SoluÃ§Ãµes

### OpÃ§Ã£o 1: Reduzir `save_per_updates` (RECOMENDADO)

Mudar de 500 para 100 updates para alinhar com `last_per_updates`:

```yaml
# train/config/base_config.yaml
checkpoints:
  save_per_updates: 100          # â† Mudar de 500 para 100
  last_per_updates: 100
  log_samples_per_updates: 100   # â† Mudar de 500 para 100
```

**Vantagens:**
- âœ… Gera samples a cada 2-3 minutos (100 updates â‰ˆ 3.5min com batch=2)
- âœ… Checkpoints mais frequentes (menos perda em caso de crash)
- âœ… Melhor monitoramento da qualidade de Ã¡udio

**Desvantagens:**
- âš ï¸ Mais espaÃ§o em disco (mas `keep_last_n_checkpoints: 3` limita)
- âš ï¸ Overhead de I/O (mas mÃ­nimo)

---

### OpÃ§Ã£o 2: Manter 500 updates mas ajustar `last_per_updates`

```yaml
checkpoints:
  save_per_updates: 500
  last_per_updates: 500          # â† Igualar com save_per_updates
  log_samples_per_updates: 500
```

**Vantagens:**
- âœ… Menos I/O (salva menos vezes)
- âœ… Menos espaÃ§o temporÃ¡rio usado

**Desvantagens:**
- âŒ Samples sÃ³ a cada 17-18 minutos (500 updates â‰ˆ 17.5min)
- âŒ Maior risco de perder progresso em crash

---

### OpÃ§Ã£o 3: Valores intermediÃ¡rios (BALANCED)

```yaml
checkpoints:
  save_per_updates: 200          # Checkpoint numerado a cada 200 updates
  last_per_updates: 50           # model_last.pt a cada 50 updates (backup)
  log_samples_per_updates: 200   # Samples a cada 200 updates
  keep_last_n_checkpoints: 5     # Manter 5 checkpoints (1000 updates = ~35min)
```

**Vantagens:**
- âœ… Balance entre frequÃªncia e performance
- âœ… Samples a cada 7 minutos (200 updates â‰ˆ 7min)
- âœ… Backup frequente com `model_last.pt` (50 updates â‰ˆ 1.7min)

**Timing estimado** (batch_size=2, grad_accum=8):
- 1 update â‰ˆ 2.1 segundos
- 50 updates â‰ˆ 1.7 minutos
- 100 updates â‰ˆ 3.5 minutos
- 200 updates â‰ˆ 7 minutos
- 500 updates â‰ˆ 17.5 minutos

---

## ğŸ“ˆ Comportamento Esperado (apÃ³s fix)

### Com OpÃ§Ã£o 1 (save_per_updates=100):

```
train/output/ptbr_finetuned2/
â”œâ”€â”€ model_last.pt              (sempre o mais recente)
â”œâ”€â”€ model_25100.pt            (checkpoint numerado)
â”œâ”€â”€ model_25200.pt
â”œâ”€â”€ model_25300.pt            (sÃ³ mantÃ©m Ãºltimos 3)
â””â”€â”€ samples/
    â”œâ”€â”€ update_25100_gen.wav  (gerado)
    â”œâ”€â”€ update_25100_ref.wav  (referÃªncia)
    â”œâ”€â”€ update_25200_gen.wav
    â”œâ”€â”€ update_25200_ref.wav
    â”œâ”€â”€ update_25300_gen.wav
    â””â”€â”€ update_25300_ref.wav
```

### Com OpÃ§Ã£o 3 (save_per_updates=200):

```
train/output/ptbr_finetuned2/
â”œâ”€â”€ model_last.pt              (atualizado a cada 50 updates)
â”œâ”€â”€ model_25200.pt            (checkpoint numerado)
â”œâ”€â”€ model_25400.pt
â”œâ”€â”€ model_25600.pt
â”œâ”€â”€ model_25800.pt
â”œâ”€â”€ model_26000.pt            (sÃ³ mantÃ©m Ãºltimos 5)
â””â”€â”€ samples/
    â”œâ”€â”€ update_25200_gen.wav
    â”œâ”€â”€ update_25200_ref.wav
    â”œâ”€â”€ update_25400_gen.wav
    â”œâ”€â”€ update_25400_ref.wav
    â””â”€â”€ ...
```

---

## ğŸ¯ RecomendaÃ§Ã£o Final

**Use OpÃ§Ã£o 3 (Balanced):**

```yaml
checkpoints:
  save_per_updates: 200
  last_per_updates: 50
  keep_last_n_checkpoints: 5
  log_samples: true
  log_samples_per_updates: 200
  log_samples_per_epochs: 1
```

**Justificativa:**
1. âœ… **Samples a cada 7min** - RÃ¡pido feedback de qualidade
2. âœ… **Backup a cada 1.7min** - SeguranÃ§a contra crashes
3. âœ… **5 checkpoints = 1000 updates â‰ˆ 35min** - HistÃ³rico razoÃ¡vel
4. âœ… **Uso de disco controlado** - ~25GB para 5 checkpoints
5. âœ… **Performance OK** - Overhead mÃ­nimo de I/O

---

## ğŸ”§ Como Aplicar o Fix

### 1. Editar configuraÃ§Ã£o

```bash
nano train/config/base_config.yaml
```

Alterar seÃ§Ã£o `checkpoints`:

```yaml
checkpoints:
  # Checkpoint paths
  checkpoint_base_dir: "train/output"
  checkpoint_dir: "ptbr_finetuned2"
  
  # Save frequency
  save_per_updates: 200          # â† MUDOU de 500
  last_per_updates: 50           # â† MUDOU de 100
  keep_last_n_checkpoints: 5     # â† MUDOU de 3
  
  # Samples
  log_samples: true
  log_samples_per_updates: 200   # â† MUDOU de 500
  log_samples_per_epochs: 1
```

### 2. Reiniciar treinamento

```bash
# Parar treinamento atual (se rodando)
pkill -f run_training.py

# Iniciar novo treinamento
cd /home/tts-webui-proxmox-passthrough
python3 -m train.run_training --epochs 1000 --batch-size 2
```

### 3. Monitorar samples

```bash
# Ver samples gerados
watch -n 10 'ls -lh train/output/ptbr_finetuned2/samples/'

# Ouvir Ãºltimo sample gerado
ls -t train/output/ptbr_finetuned2/samples/*_gen.wav | head -1 | xargs -I {} ffplay -nodisp -autoexit {}
```

---

## ğŸ“Š Uso de Disco Projetado

### Com save_per_updates=200, keep_last_n=5:

```
Checkpoints:
- model_last.pt:     5.1GB (sempre)
- model_{N}.pt Ã— 5:  25.5GB (rotacionados)
- Samples (200):     ~2GB (200 epochs Ã— ~10MB)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:              ~32.6GB
```

**EspaÃ§o disponÃ­vel:** 85.4GB  
**Margem de seguranÃ§a:** 52.8GB (61% livre) âœ…

---

## âœ… Checklist de ValidaÃ§Ã£o

ApÃ³s aplicar o fix e reiniciar treinamento:

- [ ] `model_last.pt` atualiza a cada 50 updates (~1.7min)
- [ ] Checkpoint numerado salvo a cada 200 updates (~7min)
- [ ] Samples gerados em `samples/update_{N}_gen.wav` e `samples/update_{N}_ref.wav`
- [ ] Apenas 5 checkpoints numerados mantidos (rotaÃ§Ã£o automÃ¡tica)
- [ ] EspaÃ§o em disco controlado (~30-35GB total)

---

**PrÃ³ximos Passos:**
1. Aplicar OpÃ§Ã£o 3 (editar base_config.yaml)
2. Reiniciar treinamento
3. Validar apÃ³s 200 updates (~7min) se samples foram gerados
4. Monitorar TensorBoard para ver progresso

---

**ReferÃªncias:**
- F5-TTS Trainer: `/root/.local/lib/python3.11/site-packages/f5_tts/model/trainer.py`
- Config Schema: `train/config/schemas.py`
- Run Training: `train/run_training.py`
