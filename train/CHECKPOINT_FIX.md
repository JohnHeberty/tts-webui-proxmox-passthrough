# ğŸ”§ CorreÃ§Ã£o: Checkpoint Corrompido no Treinamento

## âŒ Problema Identificado

```
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
```

### Causa Raiz

1. **Checkpoint corrompido**: `pretrained_model_200000.pt` tinha apenas 1.7GB (vs 5.1GB esperado)
2. **Workers excessivos**: DataLoader criando 16 workers em sistema com 12 cores
3. **Falta de validaÃ§Ã£o**: Script nÃ£o verificava integridade antes de usar checkpoint

## âœ… CorreÃ§Ãµes Implementadas

### 1. ValidaÃ§Ã£o AutomÃ¡tica de Checkpoints

```python
def validate_checkpoint(self, checkpoint_path: str) -> bool:
    """Valida se checkpoint pode ser carregado"""
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # Verificar tamanho (checkpoints vÃ¡lidos ~5GB)
        file_size_gb = Path(checkpoint_path).stat().st_size / (1024**3)
        if file_size_gb < 1.0:
            logger.error(f"âŒ Checkpoint muito pequeno ({file_size_gb:.1f}GB)")
            return False
        
        return True
    except Exception as e:
        logger.error(f"âŒ Checkpoint corrompido: {e}")
        return False
```

**BenefÃ­cios:**
- âœ… Detecta checkpoints corrompidos automaticamente
- âœ… Renomeia arquivos invÃ¡lidos (.pt â†’ .pt.corrupted)
- âœ… Busca prÃ³ximo checkpoint vÃ¡lido disponÃ­vel

### 2. Ajuste AutomÃ¡tico de Workers

```python
import multiprocessing
cpu_count = multiprocessing.cpu_count()
max_workers = max(1, cpu_count - 4)  # Deixa 4 cores livres

if self.config['dataloader_workers'] > max_workers:
    logger.warning(f"âš ï¸  Ajustando workers: {config} â†’ {max_workers}")
    self.config['dataloader_workers'] = max_workers
```

**Antes:**
```
UserWarning: This DataLoader will create 16 worker processes in total.
Our suggested max number of worker in current system is 12
```

**Depois:**
- Sistema com 12 cores â†’ 8 workers (12 - 4)
- Sistema com 8 cores â†’ 4 workers (8 - 4)
- Evita sobrecarga e freeze

### 3. PriorizaÃ§Ã£o de Checkpoints

**Nova ordem de busca:**

1. âœ… `train/output/ptbr_finetuned2/model_last.pt` (mais recente, validado)
2. âœ… `train/output/ptbr_finetuned2/model_*.pt` (numerados, validados)
3. âœ… `ckpts/ptbr_finetuned2/model_last.pt` (F5-TTS dir, validado)
4. âœ… `models/f5tts/pt-br/model_last.pt` (prÃ©-treinado local, validado)

**Cada checkpoint Ã© validado antes de ser usado!**

### 4. RecuperaÃ§Ã£o de Checkpoint VÃ¡lido

```bash
# Checkpoint corrompido renomeado
pretrained_model_200000.pt â†’ pretrained_model_200000.pt.corrupted (1.7GB)

# Checkpoint vÃ¡lido copiado
cp train/output/ptbr_finetuned/model_last.pt \
   train/output/ptbr_finetuned2/model_last.pt (5.1GB âœ“)
```

## ğŸ¯ Como Usar

### OpÃ§Ã£o 1: Continuar do Checkpoint VÃ¡lido

```bash
# O script agora detecta automaticamente
python3 -m train.run_training

# SaÃ­da esperada:
# âœ… Checkpoint vÃ¡lido encontrado: model_last.pt
# ğŸ”„ Modo: Continuar treinamento do checkpoint
```

### OpÃ§Ã£o 2: ComeÃ§ar do Zero

```bash
# Remova checkpoints corrompidos primeiro
rm train/output/ptbr_finetuned2/*.pt.corrupted

# Execute
python3 -m train.run_training
```

### OpÃ§Ã£o 3: Ajustar Workers Manualmente

```bash
# Edite train/.env
DATALOADER_WORKERS=4  # Reduzir se necessÃ¡rio
```

## ğŸ“Š ValidaÃ§Ã£o de Checkpoints

```bash
# Validar checkpoint manualmente
python3 -c "
import torch
from pathlib import Path

ckpt_path = 'train/output/ptbr_finetuned2/model_last.pt'
ckpt = torch.load(ckpt_path, map_location='cpu', weights_only=False)

file_size_gb = Path(ckpt_path).stat().st_size / (1024**3)
print(f'âœ“ Checkpoint vÃ¡lido ({file_size_gb:.1f}GB)')
print(f'Keys: {list(ckpt.keys())}')
if 'update' in ckpt:
    print(f'Update: {ckpt[\"update\"]}')
"
```

**SaÃ­da esperada:**
```
âœ“ Checkpoint vÃ¡lido (5.1GB)
Keys: ['model_state_dict', 'optimizer_state_dict', 'ema_model_state_dict', 'scheduler_state_dict', 'update']
Update: 200000
```

## ğŸ›¡ï¸ PrevenÃ§Ã£o de Problemas Futuros

### 1. Monitorar EspaÃ§o em Disco

```bash
# Checkpoints precisam ~5GB cada
df -h /home/tts-webui-proxmox-passthrough/train/output
```

### 2. Backup de Checkpoints VÃ¡lidos

```bash
# Backup periÃ³dico
cp train/output/ptbr_finetuned2/model_last.pt \
   train/output/backups/model_$(date +%Y%m%d_%H%M%S).pt
```

### 3. Verificar Logs de Treinamento

```bash
# Logs em tempo real
tail -f train/logs/training.log

# Buscar erros
grep -i "error\|warning" train/logs/training.log
```

## ğŸ“š Arquivos Modificados

1. **`train/run_training.py`**
   - âœ… Adicionado `validate_checkpoint()`
   - âœ… Ajuste automÃ¡tico de workers
   - âœ… ValidaÃ§Ã£o em todas as buscas de checkpoint

2. **`train/utils/env_loader.py`**
   - âœ… Adicionado `DATALOADER_WORKERS` config

3. **Checkpoints**
   - âœ… `pretrained_model_200000.pt.corrupted` (renomeado)
   - âœ… `model_last.pt` (copiado, vÃ¡lido)

## âœ… Status Atual

- âœ… Checkpoint corrompido detectado e renomeado
- âœ… Checkpoint vÃ¡lido (5.1GB) pronto para uso
- âœ… Workers ajustados automaticamente
- âœ… ValidaÃ§Ã£o implementada
- âœ… Pronto para retomar treinamento

## ğŸš€ PrÃ³ximos Passos

```bash
# 1. Verificar configuraÃ§Ã£o
cat train/.env | grep -E "DATALOADER_WORKERS|OUTPUT_DIR"

# 2. Executar treinamento
python3 -m train.run_training

# 3. Monitorar
watch -n 2 'ls -lh train/output/ptbr_finetuned2/ | grep model'
```

---

**Data da CorreÃ§Ã£o:** 2025-12-05  
**Status:** âœ… Resolvido e testado
