# Refer√™ncia Completa de Configura√ß√£o

**√öltima Atualiza√ß√£o:** 04/12/2025  
**Arquivo:** `train/.env`

---

## üìã √çndice

1. [Par√¢metros de Treinamento](#par√¢metros-de-treinamento)
2. [Early Stopping](#early-stopping)
3. [Checkpoint Management](#checkpoint-management)
4. [Dataset](#dataset)
5. [Model (Fine-tuning)](#model-fine-tuning)
6. [Paths](#paths)
7. [Hardware](#hardware)
8. [Logging](#logging)
9. [Advanced](#advanced)
10. [F5-TTS Paths (Opcional)](#f5-tts-paths-opcional)
11. [Data Preparation](#data-preparation)
12. [Perfis por GPU](#perfis-por-gpu)

---

## üéØ Par√¢metros de Treinamento

Configura√ß√µes principais que afetam o processo de treinamento.

| Vari√°vel | Padr√£o | Range | Descri√ß√£o |
|----------|--------|-------|-----------|
| `EPOCHS` | 1000 | 1-10000 | N√∫mero m√°ximo de √©pocas de treinamento |
| `BATCH_SIZE` | 2 | 1-16 | Amostras por GPU (ajustar conforme VRAM) |
| `BATCH_SIZE_TYPE` | sample | sample/frame | Tipo de batch (sample recomendado) |
| `LEARNING_RATE` | 0.0001 | 0.00001-0.001 | Taxa de aprendizado (Adam optimizer) |
| `GRAD_ACCUMULATION_STEPS` | 8 | 1-32 | Acumular gradientes para simular batch maior |
| `MAX_GRAD_NORM` | 1.0 | 0.1-10.0 | Gradient clipping para estabilidade |

**Dica:** `BATCH_SIZE * GRAD_ACCUMULATION_STEPS` = effective batch size

---

## üõë Early Stopping

Para autom√°tico quando n√£o houver mais melhoria.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `EARLY_STOP_PATIENCE` | 1000 | Parar se n√£o melhorar em N √©pocas |
| `EARLY_STOP_MIN_DELTA` | 0.001 | Melhoria m√≠nima considerada significativa |

**Exemplo:**
- Se loss n√£o melhorar > 0.001 por 1000 √©pocas ‚Üí para automaticamente

---

## üíæ Checkpoint Management

Controle de salvamento de checkpoints.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `SAVE_PER_UPDATES` | 100 | Salvar checkpoint completo a cada N updates |
| `LAST_PER_UPDATES` | 50 | Atualizar `model_last.pt` a cada N updates |
| `KEEP_LAST_N_CHECKPOINTS` | 10 | Manter apenas N checkpoints (economiza espa√ßo) |
| `LOG_SAMPLES_PER_UPDATES` | 100 | Gerar samples de √°udio a cada N updates |

**C√°lculo de Espa√ßo:**
- 1 checkpoint = ~5GB
- 10 checkpoints = ~50GB

---

## üìä Dataset

Configura√ß√£o do dataset de treinamento.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `DATASET_NAME` | f5_dataset | Nome do dataset (usado em symlinks) |
| `DATASET_PATH` | train/data/f5_dataset | Caminho completo do dataset |

**Estrutura Esperada:**
```
train/data/f5_dataset/
‚îú‚îÄ‚îÄ metadata.csv      # Metadados (path, text, speaker)
‚îú‚îÄ‚îÄ wavs/            # Arquivos de √°udio
‚îÇ   ‚îú‚îÄ‚îÄ audio_001.wav
‚îÇ   ‚îú‚îÄ‚îÄ audio_002.wav
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ vocab.txt        # Vocabul√°rio (gerado automaticamente)
‚îî‚îÄ‚îÄ raw.arrow        # Dataset em formato Arrow
```

---

## üéØ Model (Fine-tuning)

Configura√ß√£o do modelo base para fine-tuning.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `BASE_MODEL` | firstpixel/F5-TTS-pt-br | Repo do HuggingFace |
| `PRETRAIN_MODEL_PATH` | train/pretrained/.../model_200000_fixed.pt | Path do checkpoint pretrained |
| `AUTO_DOWNLOAD_PRETRAINED` | true | Baixar automaticamente se n√£o existir |
| `USE_FINETUNE_FLAG` | true | Usar flag --finetune no treinamento |
| `MODEL_FILENAME` | pt-br/model_200000.pt | Arquivo a baixar do HuggingFace |

**Fluxo de Fine-tuning:**
1. Se `AUTO_DOWNLOAD_PRETRAINED=true` e modelo n√£o existe ‚Üí baixa
2. Inicia treinamento com `--finetune --pretrain <path>`
3. Continua do checkpoint mais recente se existir

---

## üìÅ Paths

Diret√≥rios principais do projeto.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `OUTPUT_DIR` | train/output/ptbr_finetuned | Onde salvar checkpoints e samples |
| `TENSORBOARD_DIR` | train/runs | Logs do TensorBoard |
| `LOG_DIR` | train/logs | Logs de execu√ß√£o |

**Nota:** Todos os paths s√£o relativos ao root do projeto.

---

## üíª Hardware

Configura√ß√µes de hardware e otimiza√ß√£o.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `DEVICE` | cuda | Device PyTorch (cuda/cpu/mps) |
| `NUM_WORKERS` | 2 | Workers para DataLoader (ajustar conforme CPU) |
| `MIXED_PRECISION` | fp16 | Precis√£o mista (fp16/fp32/bf16) |
| `MAX_SAMPLES` | 32 | M√°ximo de samples por batch |

**Otimiza√ß√£o VRAM:**
- `fp16` reduz uso de VRAM em ~50%
- `NUM_WORKERS=0` se tiver problemas de mem√≥ria RAM

---

## üìä Logging

Configura√ß√£o de logs e monitoramento.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `LOGGER` | tensorboard | Backend de logging (tensorboard/wandb) |
| `LOG_SAMPLES` | true | Gerar samples de √°udio durante treino |
| `LOG_SAMPLES_PER_EPOCHS` | 1 | Quantos samples gerar por √©poca |
| `TENSORBOARD_PORT` | 6006 | Porta do TensorBoard |

**TensorBoard:**
```bash
# Auto-inicia em http://localhost:6006
# M√©tricas: loss, learning rate, gradient norm, samples
```

---

## üîß Advanced

Configura√ß√µes avan√ßadas.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `SEED` | 666 | Random seed para reproducibilidade |
| `NUM_WARMUP_UPDATES` | 200 | Updates de warmup para learning rate |
| `EXP_NAME` | F5TTS_Base | Nome do experimento (usado em logs) |

---

## üõ†Ô∏è F5-TTS Paths (Opcional)

Paths internos do F5-TTS. **S√≥ customizar se necess√°rio!**

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `F5TTS_BASE_DIR` | /root/.local/lib/python3.11 | Diret√≥rio base do F5-TTS |
| `F5TTS_CKPTS_DIR` | /root/.local/lib/python3.11/ckpts | Diret√≥rio de checkpoints |
| `LOCAL_PRETRAINED_PATH` | models/f5tts/pt-br/model_last.pt | Path alternativo de modelo |

**‚ö†Ô∏è Aviso:** Deixe comentado a menos que tenha instala√ß√£o customizada!

---

## üì¶ Data Preparation

Paths para scripts de prepara√ß√£o de dados.

| Vari√°vel | Padr√£o | Descri√ß√£o |
|----------|--------|-----------|
| `RAW_DATA_DIR` | train/data/raw | √Åudio bruto do YouTube |
| `PROCESSED_DATA_DIR` | train/data/processed | Segmentos processados |
| `VIDEOS_CSV` | train/data/videos.csv | Lista de v√≠deos para download |
| `CONFIG_DIR` | train/config | Configs YAML |

**Pipeline:**
```
videos.csv ‚Üí RAW_DATA_DIR ‚Üí PROCESSED_DATA_DIR ‚Üí DATASET_PATH
```

---

## üéÆ Perfis por GPU

### üü¢ RTX 3090 / RTX 4090 (24GB VRAM)
```env
BATCH_SIZE=4
GRAD_ACCUMULATION_STEPS=4
MIXED_PRECISION=fp16
NUM_WORKERS=4
MAX_SAMPLES=64
```
**Effective Batch:** 16 samples  
**VRAM Usage:** ~20GB

---

### üü° RTX 3080 / RTX 4080 (16GB VRAM)
```env
BATCH_SIZE=2
GRAD_ACCUMULATION_STEPS=8
MIXED_PRECISION=fp16
NUM_WORKERS=2
MAX_SAMPLES=32
```
**Effective Batch:** 16 samples  
**VRAM Usage:** ~14GB

---

### üü† RTX 3070 / RTX 4070 (12GB VRAM)
```env
BATCH_SIZE=1
GRAD_ACCUMULATION_STEPS=16
MIXED_PRECISION=fp16
NUM_WORKERS=2
MAX_SAMPLES=16
```
**Effective Batch:** 16 samples  
**VRAM Usage:** ~10GB

---

### üî¥ RTX 3060 / Outras (8GB VRAM)
```env
BATCH_SIZE=1
GRAD_ACCUMULATION_STEPS=8
MIXED_PRECISION=fp16
NUM_WORKERS=1
MAX_SAMPLES=8
```
**Effective Batch:** 8 samples  
**VRAM Usage:** ~7GB  
**‚ö†Ô∏è Pode ser muito lento!**

---

## üìù Exemplo Completo (.env)

```env
# ========================================
# BASIC TRAINING
# ========================================
EPOCHS=1000
BATCH_SIZE=4
BATCH_SIZE_TYPE=sample
LEARNING_RATE=0.0001
GRAD_ACCUMULATION_STEPS=8
MAX_GRAD_NORM=1.0

# ========================================
# EARLY STOPPING
# ========================================
EARLY_STOP_PATIENCE=100
EARLY_STOP_MIN_DELTA=0.001

# ========================================
# CHECKPOINTS
# ========================================
SAVE_PER_UPDATES=1000
LAST_PER_UPDATES=100
KEEP_LAST_N_CHECKPOINTS=10
LOG_SAMPLES_PER_UPDATES=250

# ========================================
# DATASET
# ========================================
DATASET_NAME=f5_dataset
DATASET_PATH=train/data/f5_dataset

# ========================================
# MODEL (FINE-TUNING)
# ========================================
BASE_MODEL=firstpixel/F5-TTS-pt-br
PRETRAIN_MODEL_PATH=train/pretrained/F5-TTS-pt-br/pt-br/model_200000.pt
AUTO_DOWNLOAD_PRETRAINED=true
USE_FINETUNE_FLAG=true
MODEL_FILENAME=pt-br/model_200000.pt

# ========================================
# PATHS
# ========================================
OUTPUT_DIR=train/output/ptbr_finetuned
TENSORBOARD_DIR=train/runs
LOG_DIR=train/logs

# ========================================
# HARDWARE
# ========================================
DEVICE=cuda
NUM_WORKERS=4
MIXED_PRECISION=fp16
MAX_SAMPLES=32

# ========================================
# LOGGING
# ========================================
LOGGER=tensorboard
LOG_SAMPLES=true
LOG_SAMPLES_PER_EPOCHS=1
TENSORBOARD_PORT=6006

# ========================================
# ADVANCED
# ========================================
SEED=666
NUM_WARMUP_UPDATES=200
EXP_NAME=F5TTS_Base

# ========================================
# DATA PREPARATION
# ========================================
RAW_DATA_DIR=train/data/raw
PROCESSED_DATA_DIR=train/data/processed
VIDEOS_CSV=train/data/videos.csv
CONFIG_DIR=train/config
```

---

## üîó Links Relacionados

- [Setup Guide](SETUP.md) - Instala√ß√£o
- [Usage Guide](USAGE.md) - Como usar
- [Fine-tuning Guide](FINETUNING.md) - Guia de fine-tuning

---

**√öltima Atualiza√ß√£o:** 04/12/2025  
**Arquivo de Exemplo:** `train/.env.example`

BATCH_SIZE=6
GRAD_ACCUMULATION_STEPS=4
MIXED_PRECISION=fp16
```

### A100 (40GB VRAM)
```env
BATCH_SIZE=8
GRAD_ACCUMULATION_STEPS=2
MIXED_PRECISION=fp16
```

### RTX 3060 (12GB VRAM)
```env
BATCH_SIZE=2
GRAD_ACCUMULATION_STEPS=8
MIXED_PRECISION=fp16
```

## Ajustes para Converg√™ncia

**Treinamento muito lento:**
```env
LEARNING_RATE=0.0002          # Aumentar learning rate
NUM_WARMUP_UPDATES=100        # Reduzir warmup
```

**Overfitting:**
```env
LEARNING_RATE=0.00005         # Reduzir learning rate
EARLY_STOP_PATIENCE=5         # Early stop mais agressivo
```

**Underfitting:**
```env
EPOCHS=2000                   # Mais epochs
EARLY_STOP_PATIENCE=1000      # Permitir mais tempo
```

## Exemplo Completo

```env
# Configura√ß√£o balanceada para RTX 3090
EPOCHS=1000
BATCH_SIZE=4
LEARNING_RATE=0.0001
GRAD_ACCUMULATION_STEPS=4

EARLY_STOP_PATIENCE=10
EARLY_STOP_MIN_DELTA=0.001

SAVE_PER_UPDATES=250
LAST_PER_UPDATES=50
KEEP_LAST_N_CHECKPOINTS=10
LOG_SAMPLES_PER_UPDATES=250

DATASET_NAME=ptbr_youtube_custom
DATASET_PATH=train/data/f5_dataset

BASE_MODEL=firstpixel/F5-TTS-pt-br
OUTPUT_DIR=train/output/ptbr_finetuned
TENSORBOARD_DIR=train/runs

DEVICE=cuda
NUM_WORKERS=4
MIXED_PRECISION=fp16

LOGGER=tensorboard
LOG_SAMPLES=true
LOG_SAMPLES_PER_EPOCHS=1

SEED=666
NUM_WARMUP_UPDATES=200
```
