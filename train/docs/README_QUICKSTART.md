# ğŸ¯ F5-TTS Fine-tuning - Guia RÃ¡pido

## âœ… ConfiguraÃ§Ã£o ConcluÃ­da

O projeto de treinamento foi configurado com as seguintes correÃ§Ãµes:

### 1. Dataset
- âœ… Removido symlink `f5_dataset_pinyin`
- âœ… Configurado para usar `train/data/f5_dataset` diretamente

### 2. Modelo PrÃ©-treinado PT-BR
- âœ… Modelo baixado: `train/pretrained/F5-TTS-pt-br/pt-br/model_200000.pt`
- âœ… Modelo corrigido: `model_200000_fixed.pt` (estrutura compatÃ­vel com F5-TTS)
- âœ… EMA verificado e funcionando (337.1M parÃ¢metros)
- âœ… Configurado no `.env`

### 3. Scripts Ãšteis
- âœ… `scripts/check_model.py` - Verifica e corrige modelos .pt
- âœ… DocumentaÃ§Ã£o em `docs/FINETUNING.md`

## ğŸš€ Como Usar

### Passo 1: Preparar Dataset

Certifique-se de que seu dataset estÃ¡ em `train/data/f5_dataset/`:

```
f5_dataset/
â”œâ”€â”€ metadata.csv       # formato: audio_path|text
â”œâ”€â”€ duration.json      # {"duration": [1.5, 2.3, ...]}
â”œâ”€â”€ vocab.txt          # um token por linha
â””â”€â”€ wavs/              # arquivos .wav em 24kHz
```

### Passo 2: Ajustar HiperparÃ¢metros (Opcional)

Edite `train/.env` conforme sua GPU:

```bash
# Para GPU com pouca VRAM (< 12GB)
BATCH_SIZE=1
GRAD_ACCUMULATION_STEPS=8
LEARNING_RATE=5e-5

# Para GPU com VRAM mÃ©dia (12-24GB)
BATCH_SIZE=4
GRAD_ACCUMULATION_STEPS=4
LEARNING_RATE=1e-4

# Para GPU com alta VRAM (> 24GB)
BATCH_SIZE=8
GRAD_ACCUMULATION_STEPS=2
LEARNING_RATE=7.5e-5
```

### Passo 3: Iniciar Treinamento

```bash
cd /home/tts-webui-proxmox-passthrough
python3 -m train.run_training
```

O treinamento irÃ¡:
1. âœ… Carregar modelo prÃ©-treinado PT-BR com EMA
2. âœ… Continuar fine-tuning a partir de 200k iteraÃ§Ãµes
3. âœ… Salvar checkpoints em `train/output/ptbr_finetuned/`
4. âœ… Gerar samples de Ã¡udio a cada 250 updates
5. âœ… Logar mÃ©tricas no TensorBoard em `train/runs/`

### Passo 4: Monitorar Progresso

Em outro terminal:

```bash
cd /home/tts-webui-proxmox-passthrough/train
tensorboard --logdir runs --port 6006
```

Acesse: http://localhost:6006

## ğŸ“Š Estrutura de Arquivos

```
train/
â”œâ”€â”€ .env                          # ConfiguraÃ§Ãµes (EDITÃVEL)
â”œâ”€â”€ run_training.py              # Script principal
â”œâ”€â”€ data/
â”‚   â””â”€â”€ f5_dataset/              # Seu dataset (OBRIGATÃ“RIO)
â”œâ”€â”€ pretrained/
â”‚   â””â”€â”€ F5-TTS-pt-br/
â”‚       â””â”€â”€ pt-br/
â”‚           â”œâ”€â”€ model_200000.pt         # Original
â”‚           â””â”€â”€ model_200000_fixed.pt   # Corrigido (USADO)
â”œâ”€â”€ output/
â”‚   â””â”€â”€ ptbr_finetuned/          # Checkpoints gerados
â”‚       â”œâ”€â”€ model_last.pt        # Ãšltimo checkpoint
â”‚       â”œâ”€â”€ model_1000.pt        # Checkpoints salvos
â”‚       â””â”€â”€ samples/             # Ãudios gerados
â”œâ”€â”€ runs/                         # TensorBoard logs
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ check_model.py           # Verificar/corrigir modelos
â””â”€â”€ docs/
    â””â”€â”€ FINETUNING.md            # DocumentaÃ§Ã£o detalhada
```

## ğŸ”§ Troubleshooting

### Erro de EMA

Se encontrar erro relacionado a EMA:

```bash
cd train
python3 scripts/check_model.py pretrained/F5-TTS-pt-br/pt-br/model_200000.pt --fix
```

### Verificar Modelo

```bash
cd train
python3 scripts/check_model.py pretrained/F5-TTS-pt-br/pt-br/model_200000_fixed.pt
```

### Out of Memory

Reduza BATCH_SIZE no `.env`:

```bash
BATCH_SIZE=1  # ou 2
GRAD_ACCUMULATION_STEPS=8  # aumentar para compensar
```

### Dataset NÃ£o Encontrado

Verifique se existe:
- `train/data/f5_dataset/metadata.csv`
- `train/data/f5_dataset/wavs/` com arquivos .wav
- `train/data/f5_dataset/vocab.txt`

## ğŸ“š DocumentaÃ§Ã£o Completa

Para detalhes sobre fine-tuning, EMA, hiperparÃ¢metros e boas prÃ¡ticas, veja:

- **Guia completo**: `train/docs/FINETUNING.md`
- **F5-TTS oficial**: https://github.com/SWivid/F5-TTS/tree/main/src/f5_tts/train
- **Discussion #57**: https://github.com/SWivid/F5-TTS/discussions/57

## âš¡ Quick Start (TL;DR)

```bash
# 1. Certifique-se de que o dataset estÃ¡ em train/data/f5_dataset
# 2. Ajuste BATCH_SIZE em train/.env se necessÃ¡rio
# 3. Execute:
cd /home/tts-webui-proxmox-passthrough
python3 -m train.run_training

# 4. Em outro terminal, monitore:
cd train
tensorboard --logdir runs --port 6006
```

## ğŸ¯ Resultados Esperados

- **Primeiras 1k iteraÃ§Ãµes**: Modelo aprende caracterÃ­sticas bÃ¡sicas do dataset
- **1k - 10k iteraÃ§Ãµes**: Melhora na pronÃºncia e naturalidade
- **10k - 50k iteraÃ§Ãµes**: Fine-tuning refinado, resultados estÃ¡veis
- **50k+ iteraÃ§Ãµes**: Risco de overfitting se dataset for pequeno

**Dica**: Use Early Stopping para evitar overfitting (configurado em `.env`)

## ğŸ“ Suporte

Para problemas especÃ­ficos, consulte:
1. `train/docs/FINETUNING.md` (troubleshooting avanÃ§ado)
2. Logs do TensorBoard
3. Issues no GitHub do F5-TTS

---

**Status**: âœ… Pronto para treinar!
