# ========================================
# F5-TTS FINE-TUNING CONFIGURATION
# ========================================
# Configuração para fine-tuning do modelo firstpixel/F5-TTS-pt-br
# usando dataset customizado de vídeos do YouTube

# ========================================
# MODEL CONFIGURATION
# ========================================
model:
  # Modelo base para fine-tuning
  base_model: "firstpixel/F5-TTS-pt-br"
  checkpoint_path: "./models/f5tts/pt-br/model_last.safetensors"
  
  # Usar checkpoint local ou baixar do HuggingFace
  use_local_checkpoint: true
  
  # Vocab file (mesmo do modelo base pt-br)
  vocab_file: "./models/f5tts/pt-br/vocab.txt"
  
  # Model architecture
  model_type: "DiT"  # DiT ou UNetT
  dim: 1024
  depth: 22
  heads: 16
  ff_mult: 2
  text_dim: 512
  conv_layers: 4

# ========================================
# MEL SPECTROGRAM CONFIGURATION
# ========================================
mel_spec:
  target_sample_rate: 24000
  n_mel_channels: 100
  hop_length: 256
  win_length: 1024
  n_fft: 1024
  mel_spec_type: "vocos"  # vocos ou bigvgan

# ========================================
# TRAINING HYPERPARAMETERS
# ========================================
training:
  # Dataset
  dataset_name: "ptbr_youtube_custom"
  dataset_path: "./train/data/f5_dataset"
  
  # Optimization
  learning_rate: 1.0e-4
  batch_size_per_gpu: 4  # Ajuste conforme VRAM disponível
  batch_size_type: "sample"  # "sample" ou "frame"
  max_samples: 64  # Máximo de samples por batch (para batch_size_type="frame")
  grad_accumulation_steps: 4  # Gradient accumulation para simular batch maior
  max_grad_norm: 1.0
  
  # Scheduler
  num_warmup_updates: 200  # Warmup steps
  warmup_start_lr: 1.0e-6
  warmup_end_lr: 1.0e-4
  
  # Training duration
  epochs: 10
  max_steps: null  # Se null, usa epochs; se definido, limita por steps
  
  # EMA (Exponential Moving Average)
  ema:
    use_ema: true
    ema_decay: 0.9999
    update_every: 10
    update_after_step: 100

# ========================================
# CHECKPOINT MANAGEMENT
# ========================================
checkpoints:
  # Diretório de saída
  output_dir: "./train/output/ptbr_finetuned"
  
  # Salvamento
  save_per_updates: 500  # Salvar a cada N updates
  keep_last_n_checkpoints: 5  # Manter apenas os N últimos checkpoints
  last_per_updates: 100  # Salvar checkpoint "last" a cada N updates
  
  # Resume training
  resume_from_checkpoint: null  # Path para checkpoint, ou null para novo treino
  
  # Logging de samples de áudio
  log_samples: true
  log_samples_per_updates: 500

# ========================================
# OPTIMIZER CONFIGURATION
# ========================================
optimizer:
  type: "AdamW"  # AdamW ou Adam8bit
  betas: [0.9, 0.95]
  weight_decay: 0.0
  eps: 1.0e-8
  
  # 8-bit optimizer (economiza VRAM)
  use_8bit_adam: false

# ========================================
# MIXED PRECISION
# ========================================
mixed_precision:
  enabled: true
  dtype: "fp16"  # fp16, bf16, ou null

# ========================================
# LOGGING & MONITORING
# ========================================
logging:
  # Logger type
  logger: "tensorboard"  # tensorboard, wandb, ou null
  
  # TensorBoard
  tensorboard_dir: "./train/logs/tensorboard"
  
  # Weights & Biases (opcional)
  wandb:
    enabled: false
    project: "f5tts-ptbr-finetune"
    entity: null  # Seu username do W&B
    run_name: null  # Auto-gerado se null
  
  # Logging frequency
  log_every_n_steps: 10

# ========================================
# HARDWARE CONFIGURATION
# ========================================
hardware:
  # Device
  device: "cuda"  # cuda, cpu, ou auto
  
  # Multi-GPU (se disponível)
  num_gpus: 1
  
  # DataLoader workers
  num_workers: 4
  pin_memory: true
  persistent_workers: true

# ========================================
# VOCODER (para log samples)
# ========================================
vocoder:
  name: "vocos"
  is_local: false
  local_path: null

# ========================================
# VALIDATION
# ========================================
validation:
  enabled: false  # Desabilitado por padrão (dataset pequeno)
  val_dataset_path: null
  val_every_n_updates: 1000
  num_val_samples: 100

# ========================================
# ADVANCED OPTIONS
# ========================================
advanced:
  # Gradient checkpointing (economiza VRAM)
  gradient_checkpointing: false
  
  # Resumable training com seed
  resumable_with_seed: 666  # Seed para reproducibilidade
  
  # Compile model (PyTorch 2.0+)
  compile_model: false
