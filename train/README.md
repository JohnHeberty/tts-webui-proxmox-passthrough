# ğŸ™ï¸ XTTS-v2 Training Pipeline

Pipeline completo de preparaÃ§Ã£o de dados e treinamento fine-tuning para XTTS-v2 (Coqui TTS).

## ğŸ“ Estrutura

```
train/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ dataset_config.yaml      # ConfiguraÃ§Ã£o de preparaÃ§Ã£o de dados
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ videos.csv               # CatÃ¡logo de vÃ­deos do YouTube
â”‚   â”œâ”€â”€ raw/                     # Ãudios baixados (22050Hz mono)
â”‚   â”œâ”€â”€ processed/               # Segmentos processados com VAD
â”‚   â””â”€â”€ MyTTSDataset/            # Dataset final (formato LJSpeech)
â”‚       â”œâ”€â”€ wavs/                # Arquivos de Ã¡udio
â”‚       â”œâ”€â”€ metadata.csv         # Metadata completo
â”‚       â”œâ”€â”€ metadata_train.csv   # Split de treino (90%)
â”‚       â””â”€â”€ metadata_val.csv     # Split de validaÃ§Ã£o (10%)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_youtube.py      # Download de Ã¡udios do YouTube
â”‚   â”œâ”€â”€ segment_audio.py         # SegmentaÃ§Ã£o com VAD
â”‚   â”œâ”€â”€ transcribe_audio.py      # TranscriÃ§Ã£o com Whisper
â”‚   â”œâ”€â”€ build_ljs_dataset.py     # ConstruÃ§Ã£o do dataset LJSpeech
â”‚   â””â”€â”€ pipeline.py              # Orquestrador completo
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ checkpoints/             # Checkpoints do fine-tuning
â”‚   â””â”€â”€ samples/                 # Amostras geradas durante treino
â””â”€â”€ logs/                        # Logs de execuÃ§Ã£o
```

## ğŸš€ Quickstart

### 1. Preparar Dataset

**OpÃ§Ã£o A: Pipeline completo (recomendado)**
```bash
# Executar todos os steps
python -m train.scripts.pipeline
```

**OpÃ§Ã£o B: Steps individuais**
```bash
# 1. Download de Ã¡udios do YouTube
python -m train.scripts.download_youtube

# 2. SegmentaÃ§Ã£o com VAD (7-12s por segmento)
python -m train.scripts.segment_audio

# 3. TranscriÃ§Ã£o com Whisper
python -m train.scripts.transcribe_audio

# 4. ConstruÃ§Ã£o do dataset LJSpeech
python -m train.scripts.build_ljs_dataset
```

**OpÃ§Ã£o C: Pular steps jÃ¡ executados**
```bash
# Se jÃ¡ baixou os vÃ­deos
python -m train.scripts.pipeline --skip-download

# Se jÃ¡ segmentou
python -m train.scripts.pipeline --skip-download --skip-segment
```

### 2. Configurar Dataset

Edite `train/config/dataset_config.yaml` para ajustar:
- **Audio**: Sample rate (22050Hz), canais (mono)
- **SegmentaÃ§Ã£o**: DuraÃ§Ã£o min/max (7-12s), threshold VAD
- **TranscriÃ§Ã£o**: Modelo Whisper (base/small/medium)
- **Qualidade**: Filtros de palavras, duraÃ§Ã£o

### 3. Adicionar VÃ­deos

Edite `train/data/videos.csv`:
```csv
id,youtube_url,speaker,emotion,language,split,notes
1,https://www.youtube.com/watch?v=xxxxx,narrator1,neutral,pt-br,train,Podcast EP1
2,https://www.youtube.com/watch?v=yyyyy,narrator1,happy,pt-br,train,Podcast EP2
```

## ğŸ¯ EspecificaÃ§Ãµes XTTS-v2

**Requisitos do modelo:**
- âœ… Sample rate: **22050Hz** (nÃ£o 24000!)
- âœ… Formato: **WAV mono 16-bit**
- âœ… DuraÃ§Ã£o ideal: **7-12 segundos** por segmento
- âœ… Idioma: **pt-BR** (PortuguÃªs Brasil)
- âœ… Formato dataset: **LJSpeech** (`wavs/audio_00001.wav|texto aqui`)

**DiferenÃ§as vs F5-TTS:**
| Feature | F5-TTS | XTTS-v2 |
|---------|--------|---------|
| Sample rate | 24000Hz | **22050Hz** |
| DuraÃ§Ã£o ideal | 3-30s | **7-12s** |
| Formato metadata | `path|text` | `path|text` (igual) |
| NormalizaÃ§Ã£o texto | Case-sensitive | **Lowercase** |

## ğŸ“Š Pipeline de Dados

### 1. Download YouTube (`download_youtube.py`)
- LÃª `videos.csv`
- Baixa Ã¡udio com yt-dlp
- Converte para WAV 22050Hz mono
- Salva em `data/raw/video_XXXXX.wav`

### 2. SegmentaÃ§Ã£o VAD (`segment_audio.py`)
- Voice Activity Detection (energia RMS)
- Streaming (nÃ£o carrega arquivo inteiro na RAM)
- Segmenta em 7-12s (ideal para XTTS-v2)
- Aplica fade in/out, normalizaÃ§Ã£o RMS
- Salva em `data/processed/video_XXXXX_YYYY.wav`

### 3. TranscriÃ§Ã£o (`transcribe_audio.py`)
- **Prioriza legendas do YouTube** (mais rÃ¡pido, exato)
- **Fallback para Whisper** se nÃ£o houver legendas
- NormalizaÃ§Ã£o pt-BR:
  - NÃºmeros expandidos ("123" â†’ "cento e vinte e trÃªs")
  - Lowercase (XTTS funciona melhor)
  - RemoÃ§Ã£o de caracteres especiais
- Salva em `data/processed/transcriptions.json`

### 4. Build Dataset (`build_ljs_dataset.py`)
- Copia WAVs para `MyTTSDataset/wavs/`
- Gera `metadata.csv` (formato LJSpeech)
- Aplica filtros de qualidade:
  - DuraÃ§Ã£o: 7-12s
  - Palavras: 3-50
- Split train/val (90/10)

## ğŸ”§ Troubleshooting

### Erro: "yt-dlp nÃ£o encontrado"
```bash
pip install yt-dlp
```

### Erro: "ffmpeg not found"
```bash
# Ubuntu/Debian
sudo apt install ffmpeg

# macOS
brew install ffmpeg
```

### Erro: "Whisper model not found"
```bash
pip install openai-whisper
```

### Erro: "Memory error during segmentation"
Reduza `vad_chunk_duration` em `dataset_config.yaml`:
```yaml
segmentation:
  vad_chunk_duration: 5.0  # Reduzir de 10.0 para 5.0
```

### Erro: "Too many segments filtered out"
Ajuste filtros em `dataset_config.yaml`:
```yaml
quality_filters:
  enabled: false  # Desabilitar filtros temporariamente
```

## ğŸ“ˆ MÃ©tricas Esperadas

Para um dataset de qualidade:
- âœ… **1-2 horas** de Ã¡udio total (mÃ­nimo)
- âœ… **500-1000 segmentos** (7-12s cada)
- âœ… **Taxa de filtro < 20%** (poucos segmentos descartados)
- âœ… **DuraÃ§Ã£o mÃ©dia ~10s** (ideal para XTTS-v2)

## ğŸ”œ PrÃ³ximos Passos

ApÃ³s preparar o dataset:
1. **Treinar XTTS-v2**: `python -m train.scripts.train_xtts`
2. **Avaliar checkpoints**: `python -m train.scripts.evaluate`
3. **Integrar com API**: Modificar `app/engines/xtts_engine.py`

## ğŸ“š ReferÃªncias

- [XTTS-v2 Paper](https://arxiv.org/abs/2406.04904)
- [Coqui TTS Docs](https://docs.coqui.ai/)
- [LJSpeech Dataset Format](https://keithito.com/LJ-Speech-Dataset/)
- [Whisper by OpenAI](https://github.com/openai/whisper)
