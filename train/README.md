# ğŸ“ XTTS-v2 Training Pipeline

Pipeline completo de fine-tuning XTTS-v2 com LoRA e configuraÃ§Ã£o type-safe via Pydantic.

## ğŸ“š DocumentaÃ§Ã£o

- **[ğŸ¯ Guia Completo de Treinamento](docs/GUIA_COMPLETO_TREINAMENTO.md)** - Para iniciantes (passo-a-passo detalhado)
- **[ğŸ”§ DocumentaÃ§Ã£o TÃ©cnica](docs/DOCUMENTACAO_TECNICA.md)** - Para desenvolvedores (arquitetura, API)

## ğŸ“Š Status

**VersÃ£o**: v2.0 (Pydantic Settings)  
**Status**: âœ… Production-ready

- âœ… Sprint 0: SeguranÃ§a (100%)
- âœ… Sprint 1: Dataset Pipeline (100%)  
- âœ… Sprint 2: Training Script (100%)
- âœ… Sprint 3: API Integration (100%)
- âœ… Sprint 4: Pydantic Migration (100%)

## ğŸš€ Quick Start

### 1. PreparaÃ§Ã£o do Dataset

```bash
# Passo 1: Download de Ã¡udio do YouTube
python3 -m train.scripts.download_youtube

# Passo 2: SegmentaÃ§Ã£o (5-15s chunks)
python3 -m train.scripts.segment_audio

# Passo 3: TranscriÃ§Ã£o com Whisper (paralelo, 15x faster)
python3 -m train.scripts.transcribe_audio_parallel

# Passo 4: Criar dataset LJSpeech format
python3 -m train.scripts.build_ljs_dataset
```

**Output**: Dataset em `train/data/MyTTSDataset/` pronto para treinamento

### 2. Treinamento (v2.0 - Pydantic Settings)

```bash
# Modo TEMPLATE (demonstraÃ§Ã£o, usa placeholders)
python3 -m train.scripts.train_xtts

# Customizar via variÃ¡veis de ambiente:
export TRAIN_NUM_EPOCHS=50
export TRAIN_LEARNING_RATE=0.00001
export TRAIN_BATCH_SIZE=4
python3 -m train.scripts.train_xtts

# Ou editar: train/train_settings.py
```

**Nota v2.0**: âŒ NÃ£o usa mais `--config train_config.yaml`! Tudo via Pydantic Settings.

### 3. Monitoramento

```bash
# TensorBoard
tensorboard --logdir train/runs

# Acesse: http://localhost:6006
```

### 4. InferÃªncia

```bash
# SÃ­ntese com checkpoint treinado
python3 -m train.scripts.xtts_inference \
    --checkpoint train/checkpoints/best_model.pt \
    --text "Texto para sintetizar" \
    --speaker_wav reference.wav \
    --output output.wav
```

## ğŸ“‚ Estrutura

```
train/
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ train_settings.py            # âš™ï¸ ConfiguraÃ§Ã£o Pydantic
â”œâ”€â”€ scripts/                     # Scripts de treinamento
â”‚   â”œâ”€â”€ train_xtts.py           # ğŸ“ Script principal (582 linhas)
â”‚   â”œâ”€â”€ download_youtube.py     # ğŸ“¥ Download YouTube
â”‚   â”œâ”€â”€ segment_audio.py        # âœ‚ï¸  SegmentaÃ§Ã£o
â”‚   â”œâ”€â”€ transcribe_audio_parallel.py  # âš¡ TranscriÃ§Ã£o (15x faster)
â”‚   â”œâ”€â”€ build_ljs_dataset.py    # ğŸ“¦ Dataset builder
â”‚   â””â”€â”€ xtts_inference.py       # ğŸ”Š SÃ­ntese
â”œâ”€â”€ data/                        # Datasets
â”‚   â”œâ”€â”€ raw/                    # Ãudios brutos
â”‚   â”œâ”€â”€ processed/              # Segmentos + transcriÃ§Ãµes
â”‚   â””â”€â”€ MyTTSDataset/           # Dataset final
â”œâ”€â”€ checkpoints/                 # Checkpoints salvos
â”œâ”€â”€ runs/                        # TensorBoard logs
â””â”€â”€ docs/                        # ğŸ“š DocumentaÃ§Ã£o
    â”œâ”€â”€ GUIA_COMPLETO_TREINAMENTO.md
    â””â”€â”€ DOCUMENTACAO_TECNICA.md
```

## ğŸ¯ Features

- **Pipeline Completo**: Download â†’ Segment â†’ Transcribe â†’ Build â†’ Train
- **Pydantic Settings**: Type-safe config, sem YAML
- **LoRA Training**: Parameter-efficient fine-tuning
- **Parallel Processing**: 15x speedup (6-8 workers)
- **Template Mode**: Testa pipeline sem baixar XTTS (~2GB)
- **Auto Checkpointing**: Salva best model + checkpoints periÃ³dicos
- **TensorBoard**: MÃ©tricas em tempo real
- **REST API**: 6 endpoints (`/v1/finetune/*`)

## ğŸ“Š Requisitos

### Hardware

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| **GPU** | NVIDIA 8GB VRAM | NVIDIA 12GB+ VRAM |
| **CUDA** | 11.8+ | 12.1+ |
| **RAM** | 16GB | 32GB+ |
| **Disco** | 20GB | 50GB+ (datasets) |

### Software

- Python 3.11+
- PyTorch 2.0.1+cu118
- TTS (Coqui)
- PEFT
- Transformers

## ğŸ”§ ConfiguraÃ§Ã£o v2.0 (Pydantic Settings)

### VariÃ¡veis de Ambiente (.env)

Crie `train/.env` para customizar configuraÃ§Ãµes:

```bash
# Hardware
TRAIN_DEVICE=cuda
TRAIN_CUDA_DEVICE_ID=0

# Dataset
TRAIN_DATASET_DIR=train/data/MyTTSDataset
TRAIN_BATCH_SIZE=2
TRAIN_NUM_WORKERS=2

# Model & LoRA
TRAIN_MODEL_NAME=tts_models/multilingual/multi-dataset/xtts_v2
TRAIN_USE_LORA=true

## âš™ï¸ ConfiguraÃ§Ã£o v2.0 (Pydantic Settings)

### Principais ParÃ¢metros

| ParÃ¢metro | Default | DescriÃ§Ã£o | RecomendaÃ§Ã£o |
|-----------|---------|-----------|--------------|
| `num_epochs` | 1000 | NÃºmero de Ã©pocas | 50-1000 (depende dataset) |
| `batch_size` | 2 | Batch size | 1-4 (depende VRAM) |
| `learning_rate` | 1e-5 | Taxa aprendizado | 1e-5 a 1e-4 |
| `lora_rank` | 8 | Rank LoRA | 4-32 (maior = mais params) |
| `lora_alpha` | 16 | Alpha LoRA | Geralmente 2x rank |
| `use_amp` | False | Mixed precision | True se GPU moderna |

### MÃ©todos de ConfiguraÃ§Ã£o

**MÃ©todo 1: Defaults (sem editar)**
```python
from train.train_settings import get_train_settings
settings = get_train_settings()  # Usa valores padrÃ£o
```

**MÃ©todo 2: VariÃ¡veis de Ambiente**
```bash
export TRAIN_NUM_EPOCHS=50
export TRAIN_BATCH_SIZE=4
python3 -m train.scripts.train_xtts
```

**MÃ©todo 3: Arquivo .env**
```bash
# train/.env
TRAIN_NUM_EPOCHS=1000
TRAIN_BATCH_SIZE=2
TRAIN_LEARNING_RATE=0.00001
TRAIN_LORA_RANK=16
```

**MÃ©todo 4: Editar train_settings.py**
```python
# train/train_settings.py
class TrainingSettings(BaseModel):
    num_epochs: int = 50        # â† Alterar aqui
    batch_size: int = 4         # â† Alterar aqui
```

## ğŸ”§ Modo TEMPLATE vs REAL

### TEMPLATE Mode (Atual - DemonstraÃ§Ã£o)

**CaracterÃ­sticas:**
- âœ… Roda sem baixar modelo XTTS completo (~2GB)
- âœ… Usa DummyModel placeholder
- âœ… Dataset dummy (10 samples)
- âœ… Demonstra loop completo de treinamento
- âŒ NÃƒO treina modelo real
- âŒ NÃƒO gera checkpoints utilizÃ¡veis

**Quando usar:** Testar pipeline, validar cÃ³digo, smoke tests

### REAL Mode (Para ProduÃ§Ã£o)

**Requer:**
1. Instalar TTS: `pip install TTS transformers peft`
2. Adaptar `load_pretrained_model()` em `train_xtts.py`
3. Adaptar `setup_lora()` com target modules corretos
4. Criar dataset real com pipeline completo
5. Executar treinamento (pode levar horas)

**Ver:** [DocumentaÃ§Ã£o TÃ©cnica](docs/DOCUMENTACAO_TECNICA.md#implementaÃ§Ã£o-xtts-real)

## ğŸ› Troubleshooting

### Erro: "CUDA out of memory"

**SoluÃ§Ã£o:** Reduzir batch_size
```bash
export TRAIN_BATCH_SIZE=1
python3 -m train.scripts.train_xtts
```

### Erro: "FileNotFoundError: metadata.csv"

**SoluÃ§Ã£o:** Executar pipeline de dataset completo
```bash
python3 -m train.scripts.build_ljs_dataset
```

### Erro: "DummyModel has no attribute ..."

**Normal:** Modo TEMPLATE usa placeholder. Para implementaÃ§Ã£o real, ver [DocumentaÃ§Ã£o TÃ©cnica](docs/DOCUMENTACAO_TECNICA.md#implementaÃ§Ã£o-xtts-real).

### Config YAML nÃ£o funciona (v2.0)

**Problema:** `--config train_config.yaml` dÃ¡ erro.

**SoluÃ§Ã£o:** v2.0 NÃƒO usa mais YAML! Use Pydantic Settings:
```bash
# MÃ©todo correto v2.0
export TRAIN_NUM_EPOCHS=50

## ğŸ“– ReferÃªncias

- **[XTTS-v2 Paper](https://arxiv.org/abs/2406.04904)** - Arquitetura do modelo
- **[LoRA Paper](https://arxiv.org/abs/2106.09685)** - Fine-tuning eficiente
- **[Coqui TTS Docs](https://docs.coqui.ai/)** - DocumentaÃ§Ã£o oficial
- **[Pydantic Docs](https://docs.pydantic.dev)** - Settings type-safe
- **[PEFT GitHub](https://github.com/huggingface/peft)** - LoRA implementation
- **[Whisper](https://github.com/openai/whisper)** - TranscriÃ§Ã£o

## ğŸ¤ Suporte

- **Issues:** GitHub Issues
- **Discord:** TTS Community
- **Docs:** `train/docs/`
  - [Guia Completo](docs/GUIA_COMPLETO_TREINAMENTO.md) - Para iniciantes
  - [Doc TÃ©cnica](docs/DOCUMENTACAO_TECNICA.md) - Para desenvolvedores

---

**VersÃ£o**: v2.0 (Pydantic Settings)  
**Ãšltima atualizaÃ§Ã£o**: 2025-12-07  
**Status**: âœ… Production-ready

**MudanÃ§as v2.0:**
- âœ… Migrado de YAML â†’ Pydantic Settings (type-safe)
- âœ… Train script bugfixes (4 issues resolvidos)
- âœ… Template mode para testes sem XTTS completo
- âœ… DocumentaÃ§Ã£o completa (iniciante + tÃ©cnica)

