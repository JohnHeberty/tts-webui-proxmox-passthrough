# Performance Tuning Guide

Guia completo de otimiza√ß√£o de performance para o sistema multi-engine TTS.

---

## üìã √çndice

1. [Quick Wins](#quick-wins)
2. [GPU Optimization](#gpu-optimization)
3. [CPU Fallback](#cpu-fallback)
4. [Cache Strategies](#cache-strategies)
5. [Profiling](#profiling)
6. [Benchmarking](#benchmarking)

---

## ‚ö° Quick Wins

### 1. Use GPU Sempre Que Poss√≠vel

```python
# ‚ùå For√ßar CPU (LENTO)
settings.tts_engines['xtts']['fallback_to_cpu'] = False
os.environ['CUDA_VISIBLE_DEVICES'] = ''

# ‚úÖ Usar GPU (R√ÅPIDO)
settings.tts_engines['xtts']['device'] = 'cuda:0'
settings.tts_engines['f5tts']['device'] = 'cuda:0'
```

**Ganho**: ~10-20x mais r√°pido

### 2. Cache de Voice Profiles

```python
# ‚ùå Reprocessar a cada request
voice = await processor.process_voice_upload(audio_bytes)

# ‚úÖ Usar cache (Redis)
voice_id = await processor.save_voice_profile(audio_bytes)
# Reusar voice_id nas pr√≥ximas chamadas
result = await processor.process_tts(text, voice_id=voice_id)
```

**Ganho**: ~5-10s economizados por request

### 3. Pr√©-carregamento de Modelos

```python
# ‚ùå Lazy loading (lento na primeira chamada)
processor = MultiEngineTTSProcessor()

# ‚úÖ Pr√©-carregar engines na inicializa√ß√£o
@app.on_event("startup")
async def startup():
    # Force load
    processor.get_engine('xtts')
    processor.get_engine('f5tts')
```

**Ganho**: ~30-60s economizados na primeira request

---

## üéÆ GPU Optimization

### 1. VRAM Management

**Monitorar uso:**
```python
import torch

def get_gpu_memory():
    if torch.cuda.is_available():
        return {
            'allocated': torch.cuda.memory_allocated() / 1024**3,  # GB
            'reserved': torch.cuda.memory_reserved() / 1024**3,
            'free': torch.cuda.mem_get_info()[0] / 1024**3
        }
```

**Limpar cache quando necess√°rio:**
```python
import gc

def clear_gpu_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    gc.collect()
```

### 2. Otimizar Batch Size

```python
# F5-TTS (usa mais VRAM)
settings.tts_engines['f5tts']['batch_size'] = 1  # Se VRAM < 12GB
settings.tts_engines['f5tts']['batch_size'] = 2  # Se VRAM >= 12GB

# XTTS (mais eficiente)
settings.tts_engines['xtts']['batch_size'] = 4
```

### 3. FP16 (Half Precision)

```python
# ‚ùå FP32 (mais VRAM)
model = load_model().to('cuda')

# ‚úÖ FP16 (metade da VRAM, ~mesma qualidade)
model = load_model().to('cuda').half()
```

**Ganho**: ~50% menos VRAM, ~10-20% mais r√°pido

### 4. Multi-GPU

```python
# docker-compose.yml
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          device_ids: ['0', '1']  # GPUs 0 e 1
          capabilities: [gpu]

# app/config.py
settings.tts_engines = {
    'xtts': {'device': 'cuda:0'},
    'f5tts': {'device': 'cuda:1'}
}
```

**Ganho**: 2x throughput (processar 2 jobs simultaneamente)

---

## üíª CPU Fallback

### 1. Quando Usar

- **GPU indispon√≠vel** (no CUDA, VRAM insuficiente)
- **Jobs de baixa prioridade** (batch processing)
- **Textos curtos** (< 50 caracteres, overhead de GPU n√£o compensa)

### 2. Otimiza√ß√µes CPU

```python
# Usar todas as threads dispon√≠veis
import torch
torch.set_num_threads(os.cpu_count())

# ONNXRuntime (mais r√°pido em CPU)
import onnxruntime as ort
ort.set_default_logger_severity(3)  # Desabilitar warnings
providers = ['CPUExecutionProvider']
```

### 3. Compara√ß√£o de Performance

| M√©trica | GPU (RTX 3090) | CPU (32 cores) | Diferen√ßa |
|---------|----------------|----------------|-----------|
| RTF (XTTS) | 0.08 | 1.2 | **15x mais lento** |
| RTF (F5-TTS) | 0.12 | 2.5 | **20x mais lento** |
| VRAM/RAM | 6GB | 12GB | 2x mais RAM |
| Lat√™ncia | 2s | 30s | **15x mais lento** |

**Recomenda√ß√£o**: Usar CPU apenas como fallback de emerg√™ncia.

---

## üóÇÔ∏è Cache Strategies

### 1. Voice Profile Cache (Redis)

```python
# app/redis_store.py
class VoiceProfileCache:
    def __init__(self, ttl=2592000):  # 30 dias
        self.ttl = ttl
    
    async def get(self, voice_id: str):
        """Cache hit: ~5ms, Cache miss: ~5000ms"""
        data = await redis.get(f"voice:{voice_id}")
        if data:
            return pickle.loads(data)
        return None
    
    async def set(self, voice_id: str, embeddings):
        await redis.setex(
            f"voice:{voice_id}",
            self.ttl,
            pickle.dumps(embeddings)
        )
```

**Ganho**: ~5-10s por request (evita reprocessar audio de refer√™ncia)

### 2. Model Cache (Disk)

```bash
# Pr√©-baixar modelos
export HF_HOME=/mnt/ssd/models/huggingface
export TTS_HOME=/mnt/ssd/models/coqui

# Primeira vez (download)
python -c "from app.engines.factory import TTSEngineFactory; factory = TTSEngineFactory(); factory.get_engine('xtts'); factory.get_engine('f5tts')"

# Pr√≥ximas vezes (carrega do cache)
# 30-60s mais r√°pido
```

### 3. HTTP Cache (Outputs)

```nginx
# nginx.conf
location /outputs/ {
    alias /var/www/outputs/;
    expires 7d;  # Cache de 7 dias
    add_header Cache-Control "public, immutable";
    
    # Compress√£o
    gzip on;
    gzip_types audio/wav audio/mpeg;
}
```

**Ganho**: Reduz largura de banda em ~50%

---

## üî¨ Profiling

### 1. cProfile (CPU)

```python
import cProfile
import pstats

# Profiling
profiler = cProfile.Profile()
profiler.enable()

result = await processor.process_tts(text, voice_id)

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 fun√ß√µes
```

**Output:**
```
   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
        1    0.001    0.001   10.234   10.234 processor.py:45(process_tts)
        1    8.456    8.456    8.456    8.456 xtts_engine.py:78(synthesize)
       10    1.234    0.123    1.234    0.123 torch/nn/functional.py:2345(conv1d)
```

### 2. NVIDIA Profiler (GPU)

```bash
# Instalar
pip install nvidia-pyprof

# Profiling
nsys profile -o profile.qdrep python run.py

# Visualizar
nsys-ui profile.qdrep
```

**M√©tricas:**
- Kernel launch overhead
- Memory transfer time (CPU ‚Üí GPU)
- Compute time
- Memory bandwidth utilization

### 3. Line Profiler

```python
# pip install line_profiler

from line_profiler import LineProfiler

lp = LineProfiler()
lp.add_function(processor.process_tts)
lp.enable()

result = await processor.process_tts(text, voice_id)

lp.disable()
lp.print_stats()
```

**Output:**
```
Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    45         1        100.0    100.0      0.0      engine = self.get_engine(tts_engine)
    46         1    8456000.0 8456000.0     82.6      audio = await engine.synthesize(...)
    47         1    1780000.0 1780000.0     17.4      audio = await self.normalize(audio)
```

---

## üìä Benchmarking

### 1. RTF (Real-Time Factor)

```python
import time

def measure_rtf(audio_duration: float, processing_time: float) -> float:
    """
    RTF < 1.0 = Mais r√°pido que real-time (bom)
    RTF > 1.0 = Mais lento que real-time (ruim)
    """
    return processing_time / audio_duration

# Exemplo
start = time.time()
audio = await engine.synthesize(text, voice_id)
processing_time = time.time() - start

audio_duration = len(audio) / 24000  # 24kHz
rtf = measure_rtf(audio_duration, processing_time)

print(f"RTF: {rtf:.2f} ({'‚úÖ FAST' if rtf < 1.0 else '‚ö†Ô∏è SLOW'})")
```

### 2. Usar Framework de Benchmarks

```bash
cd services/audio-voice/benchmarks

# Executar benchmarks PT-BR
python run_benchmark.py \
  --engines xtts f5tts \
  --dataset dataset_ptbr.json \
  --voices all \
  --output results/

# Analisar resultados
python analyze_results.py results/
```

**Output:**
```
üìä BENCHMARK RESULTS
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Engine: XTTS
  RTF (mean): 0.08 ¬± 0.02
  Quality: 4.2/5.0
  Success: 98%

Engine: F5-TTS
  RTF (mean): 0.12 ¬± 0.03
  Quality: 4.5/5.0
  Success: 95%

Recommendation: Use XTTS for speed, F5-TTS for quality
```

### 3. Load Testing (Locust)

```python
# locustfile.py
from locust import HttpUser, task, between

class TTSUser(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def tts_request(self):
        self.client.post("/tts/synthesize", json={
            "text": "Ol√°, este √© um teste de carga.",
            "tts_engine": "xtts",
            "voice_id": "voice_123"
        })

# Executar
locust -f locustfile.py --host=http://localhost:8000
```

**M√©tricas:**
- Requests/s
- Lat√™ncia (p50, p95, p99)
- Taxa de erro
- Throughput

---

## üéØ Performance Targets

### Lat√™ncia (P95)

| Engine | Target | Aceit√°vel | Inaceit√°vel |
|--------|--------|-----------|-------------|
| XTTS | < 3s | < 5s | > 10s |
| F5-TTS | < 5s | < 8s | > 15s |

### RTF (Real-Time Factor)

| Cen√°rio | Target | Aceit√°vel | Inaceit√°vel |
|---------|--------|-----------|-------------|
| GPU | < 0.2 | < 0.5 | > 1.0 |
| CPU | < 2.0 | < 5.0 | > 10.0 |

### Throughput

| Hardware | Target | Aceit√°vel |
|----------|--------|-----------|
| 1x RTX 3090 | 50 req/min | 30 req/min |
| 2x RTX 3090 | 100 req/min | 60 req/min |
| CPU (32 cores) | 5 req/min | 2 req/min |

---

## üîß Troubleshooting

### Problema: Alto RTF (> 1.0)

**Causas:**
- CPU em vez de GPU
- VRAM insuficiente (swapping)
- Modelo n√£o otimizado

**Solu√ß√µes:**
```bash
# Verificar GPU
nvidia-smi

# Limpar cache CUDA
python -c "import torch; torch.cuda.empty_cache()"

# Reduzir batch size
# config.py: batch_size = 1
```

### Problema: CUDA Out of Memory

**Causas:**
- Batch size muito grande
- M√∫ltiplos modelos carregados simultaneamente
- Memory leak

**Solu√ß√µes:**
```python
# Reduzir batch size
settings.tts_engines['f5tts']['batch_size'] = 1

# Limpar cache ap√≥s cada job
torch.cuda.empty_cache()

# Usar FP16
model.half()
```

### Problema: Lat√™ncia Alta (> 10s)

**Causas:**
- Lazy loading de modelos
- Voice profile n√£o cacheado
- I/O lento (HDD em vez de SSD)

**Solu√ß√µes:**
```python
# Pr√©-carregar modelos
@app.on_event("startup")
async def startup():
    processor.get_engine('xtts')
    processor.get_engine('f5tts')

# Usar Redis para cache
await cache.set(voice_id, embeddings)

# Migrar para SSD
export HF_HOME=/mnt/nvme/models
```

---

## üìö Recursos Adicionais

- [DEPLOYMENT.md](DEPLOYMENT.md) - Deploy em produ√ß√£o
- [MIGRATION.md](MIGRATION.md) - Migra√ß√£o multi-engine
- [benchmarks/README.md](../benchmarks/README.md) - Framework de benchmarks

---

**Performance tuning validado em produ√ß√£o** ‚úÖ
