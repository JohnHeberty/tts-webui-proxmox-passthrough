# ðŸŽ® ConfiguraÃ§Ã£o de GPU no Proxmox LXC

## Problema Identificado

VocÃª estÃ¡ rodando dentro de um **container LXC no Proxmox**, e precisa configurar o passthrough da GPU NVIDIA RTX 3090 corretamente.

## âœ… O que estÃ¡ funcionando
- âœ… `nvidia-smi` funciona dentro do container LXC
- âœ… Driver NVIDIA 550.163.01 detectado
- âœ… GPU RTX 3090 visÃ­vel
- âœ… Docker com runtime nvidia configurado

## âŒ O que NÃƒO estÃ¡ funcionando
- âŒ Devices `/dev/nvidia-uvm` e `/dev/nvidia-uvm-tools` nÃ£o existem/corrompidos
- âŒ PyTorch nÃ£o consegue inicializar CUDA
- âŒ MÃ³dulos do kernel nÃ£o disponÃ­veis dentro do LXC

---

## ðŸ”§ SoluÃ§Ã£o: Configurar no Host Proxmox

### 1ï¸âƒ£ No HOST Proxmox (fora do container)

Conecte-se ao seu servidor Proxmox (nÃ£o ao container) e execute:

```bash
# 1. Carregar mÃ³dulo nvidia-uvm no HOST
modprobe nvidia-uvm

# 2. Criar devices se nÃ£o existirem
if [ ! -e /dev/nvidia-uvm ]; then
  MAJOR=$(grep nvidia-uvm /proc/devices | awk '{print $1}')
  mknod -m 666 /dev/nvidia-uvm c $MAJOR 0
fi

if [ ! -e /dev/nvidia-uvm-tools ]; then
  MAJOR=$(grep nvidia-uvm /proc/devices | awk '{print $1}')
  mknod -m 666 /dev/nvidia-uvm-tools c $MAJOR 1
fi

# 3. Verificar devices criados
ls -la /dev/nvidia*
```

### 2ï¸âƒ£ Configurar o Container LXC

**Edite o arquivo de configuraÃ§Ã£o do container** (substitua `100` pelo ID do seu container):

```bash
# No HOST Proxmox
nano /etc/pve/lxc/100.conf
```

**Adicione estas linhas:**

```conf
# GPU Passthrough - NVIDIA RTX 3090
lxc.cgroup2.devices.allow: c 195:* rwm
lxc.cgroup2.devices.allow: c 508:* rwm
lxc.cgroup2.devices.allow: c 242:* rwm
lxc.mount.entry: /dev/nvidia0 dev/nvidia0 none bind,optional,create=file
lxc.mount.entry: /dev/nvidiactl dev/nvidiactl none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm dev/nvidia-uvm none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-uvm-tools dev/nvidia-uvm-tools none bind,optional,create=file
lxc.mount.entry: /dev/nvidia-modeset dev/nvidia-modeset none bind,optional,create=file
```

### 3ï¸âƒ£ Persistir devices no boot do Proxmox

Crie um script para carregar mÃ³dulos e criar devices no boot:

```bash
# No HOST Proxmox
cat > /etc/systemd/system/nvidia-uvm-init.service << 'EOF'
[Unit]
Description=NVIDIA UVM Init
After=nvidia-persistenced.service

[Service]
Type=oneshot
ExecStart=/sbin/modprobe nvidia-uvm
ExecStart=/bin/bash -c 'MAJOR=$$(grep nvidia-uvm /proc/devices | awk "{print \\$$1}"); [ ! -e /dev/nvidia-uvm ] && mknod -m 666 /dev/nvidia-uvm c $$MAJOR 0 || true'
ExecStart=/bin/bash -c 'MAJOR=$$(grep nvidia-uvm /proc/devices | awk "{print \\$$1}"); [ ! -e /dev/nvidia-uvm-tools ] && mknod -m 666 /dev/nvidia-uvm-tools c $$MAJOR 1 || true'
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target
EOF

# Habilitar serviÃ§o
systemctl daemon-reload
systemctl enable nvidia-uvm-init.service
systemctl start nvidia-uvm-init.service
```

### 4ï¸âƒ£ Reiniciar Container LXC

```bash
# No HOST Proxmox
pct stop 100
pct start 100

# OU via interface web Proxmox
```

---

## ðŸ§ª Testar dentro do Container

ApÃ³s reiniciar o container:

```bash
# 1. Verificar devices
ls -la /dev/nvidia*

# Esperado:
# crw-rw-rw- 1 nobody nogroup 195,   0 /dev/nvidia0
# crw-rw-rw- 1 nobody nogroup 195, 255 /dev/nvidiactl
# crw-rw-rw- 1 nobody nogroup 508,   0 /dev/nvidia-uvm
# crw-rw-rw- 1 nobody nogroup 508,   1 /dev/nvidia-uvm-tools

# 2. Testar CUDA no container Docker
docker exec audio-voice-api python -c "import torch; print('CUDA available:', torch.cuda.is_available())"

# Esperado: CUDA available: True
```

---

## ðŸ› Troubleshooting

### Se os devices ainda nÃ£o aparecem dentro do container LXC:

```bash
# No HOST Proxmox, verificar major numbers
cat /proc/devices | grep nvidia

# Exemplo de saÃ­da:
# 195 nvidia-frontend
# 508 nvidia-uvm
# 242 nvidia-modeset

# Ajustar lxc.cgroup2.devices.allow conforme os major numbers do SEU sistema
```

### Se PyTorch ainda nÃ£o detecta CUDA:

```bash
# Dentro do container LXC
docker exec audio-voice-api bash -c '
  ls -la /dev/nvidia*
  echo "---"
  python -c "import torch; print(torch.cuda.is_available())"
  echo "---"
  ldd /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so | grep cuda
'
```

---

## ðŸ“ Resumo das AÃ§Ãµes

**No Host Proxmox:**
1. âœ… Carregar `nvidia-uvm`: `modprobe nvidia-uvm`
2. âœ… Criar devices `/dev/nvidia-uvm*`
3. âœ… Criar serviÃ§o systemd para persistir
4. âœ… Editar `/etc/pve/lxc/100.conf` com passthrough
5. âœ… Reiniciar container LXC

**Dentro do Container LXC:**
1. âœ… Verificar devices com `ls -la /dev/nvidia*`
2. âœ… Reiniciar containers Docker: `docker compose down && docker compose up -d`
3. âœ… Verificar CUDA: `docker logs audio-voice-api | grep CUDA`

---

## ðŸ”— ReferÃªncias

- [Proxmox LXC GPU Passthrough](https://www.reddit.com/r/homelab/comments/x1bcjt/gpu_passthrough_to_lxc_on_proxmox/)
- [NVIDIA Docker Container Runtime](https://nvidia.github.io/nvidia-docker/)
- [PyTorch CUDA Troubleshooting](https://pytorch.org/get-started/locally/)

---

## âš ï¸ Alternativa: Container LXC Privilegiado

Se a soluÃ§Ã£o acima nÃ£o funcionar, vocÃª pode precisar **converter o container para privilegiado**:

```bash
# No HOST Proxmox
pct stop 100
nano /etc/pve/lxc/100.conf

# Alterar/adicionar:
unprivileged: 0

# Reiniciar
pct start 100
```

**Nota:** Containers privilegiados tÃªm acesso root ao host - usar com cuidado!
