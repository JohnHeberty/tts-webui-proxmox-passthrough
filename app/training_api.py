"""
Training API Endpoints for XTTS-v2 Fine-tuning

Provides endpoints for:
- Dataset management (download, segment, transcribe)
- Training control (start, stop, status)
- Checkpoint management
- Inference testing
"""
import asyncio
import json
import logging
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

from .logging_config import get_logger

logger = get_logger(__name__)

# Router
router = APIRouter(prefix="/training", tags=["training"])

# Global state for training process
training_state = {
    "is_running": False,
    "process": None,
    "status": {
        "state": "idle",
        "epoch": 0,
        "total_epochs": 0,
        "loss": None,
        "progress": 0,
        "logs": ""
    }
}


# ==================== MODELS ====================

class DatasetDownloadRequest(BaseModel):
    """Request to download YouTube videos"""
    urls: List[str] = Field(..., description="List of YouTube URLs")
    folder: str = Field(default="datasets/my_voice", description="Output folder")


class DatasetSegmentRequest(BaseModel):
    """Request to segment audio files"""
    folder: str = Field(..., description="Dataset folder")
    min_duration: float = Field(default=7.0, ge=3.0, le=15.0)
    max_duration: float = Field(default=12.0, ge=5.0, le=20.0)
    vad_threshold: float = Field(default=-40.0, ge=-60.0, le=-20.0)


class DatasetTranscribeRequest(BaseModel):
    """Request to transcribe audio files"""
    folder: str = Field(..., description="Dataset folder")


class TrainingStartRequest(BaseModel):
    """Request to start training"""
    model_name: str = Field(..., description="Name for the model")
    dataset_folder: str = Field(..., description="Path to dataset")
    epochs: int = Field(default=100, ge=10, le=1000)
    batch_size: int = Field(default=4, ge=1, le=16)
    learning_rate: float = Field(default=0.0001, ge=0.00001, le=0.01)
    use_deepspeed: bool = Field(default=False)


class InferenceSynthesizeRequest(BaseModel):
    """Request to synthesize with checkpoint"""
    checkpoint: str = Field(..., description="Path to checkpoint")
    text: str = Field(..., description="Text to synthesize")
    speaker_wav: Optional[str] = Field(default=None, description="Reference speaker audio file")
    temperature: float = Field(default=0.7, ge=0.1, le=1.5)
    speed: float = Field(default=1.0, ge=0.5, le=2.0)


class InferenceABTestRequest(BaseModel):
    """Request for A/B comparison"""
    checkpoint: str = Field(..., description="Path to checkpoint")
    text: str = Field(..., description="Text to synthesize")


# ==================== DATASET MANAGEMENT ====================

@router.post("/dataset/download")
async def download_dataset(request: DatasetDownloadRequest, background_tasks: BackgroundTasks):
    """
    Download YouTube videos and extract audio
    
    Uses train/scripts/download_youtube.py
    """
    logger.info(f"üì• Starting download of {len(request.urls)} videos to {request.folder}")
    
    try:
        # Create output directory
        output_dir = Path(request.folder) / "raw"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Create temporary CSV with URLs
        csv_path = output_dir / "temp_videos.csv"
        with open(csv_path, "w", encoding="utf-8") as f:
            f.write("id,youtube_url,speaker,notes\n")
            for i, url in enumerate(request.urls):
                f.write(f"{i+1},{url},default_speaker,WebUI download\n")
        
        # Run download script
        cmd = [
            "python", "-m", "train.scripts.download_youtube",
            "--csv", str(csv_path),
            "--output", str(output_dir)
        ]
        
        # Execute in background
        background_tasks.add_task(run_download_task, cmd, csv_path)
        
        return {
            "status": "started",
            "videos": len(request.urls),
            "output_folder": str(output_dir)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error starting download: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def run_download_task(cmd: List[str], csv_path: Path):
    """Background task to run download"""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
        if result.returncode == 0:
            logger.info("‚úÖ Download completed successfully")
        else:
            logger.error(f"‚ùå Download failed: {result.stderr}")
    finally:
        # Cleanup temp CSV
        if csv_path.exists():
            csv_path.unlink()


@router.post("/dataset/segment")
async def segment_dataset(request: DatasetSegmentRequest, background_tasks: BackgroundTasks):
    """
    Segment audio files using VAD
    
    Uses train/scripts/segment_audio.py
    """
    logger.info(f"‚úÇÔ∏è Starting segmentation of {request.folder}")
    
    try:
        # Verify folder exists
        raw_dir = Path(request.folder) / "raw"
        if not raw_dir.exists():
            raise HTTPException(status_code=404, detail=f"Folder not found: {raw_dir}")
        
        # Create segments directory
        segments_dir = Path(request.folder) / "segments"
        segments_dir.mkdir(parents=True, exist_ok=True)
        
        # Run segmentation script
        cmd = [
            "python", "-m", "train.scripts.segment_audio",
            "--input", str(raw_dir),
            "--output", str(segments_dir),
            "--min-duration", str(request.min_duration),
            "--max-duration", str(request.max_duration),
            "--vad-threshold", str(request.vad_threshold)
        ]
        
        background_tasks.add_task(run_subprocess_task, cmd, "segmentation")
        
        return {
            "status": "started",
            "input_folder": str(raw_dir),
            "output_folder": str(segments_dir)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error starting segmentation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/dataset/transcribe")
async def transcribe_dataset(request: DatasetTranscribeRequest, background_tasks: BackgroundTasks):
    """
    Transcribe audio files using Whisper
    
    Uses train/scripts/transcribe_audio_parallel.py
    """
    logger.info(f"üìù Starting transcription of {request.folder}")
    
    try:
        segments_dir = Path(request.folder) / "segments"
        if not segments_dir.exists():
            raise HTTPException(status_code=404, detail=f"Folder not found: {segments_dir}")
        
        # Run transcription script
        cmd = [
            "python", "-m", "train.scripts.transcribe_audio_parallel",
            "--input", str(segments_dir)
        ]
        
        background_tasks.add_task(run_subprocess_task, cmd, "transcription")
        
        return {
            "status": "started",
            "folder": str(segments_dir)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error starting transcription: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/dataset/stats")
async def get_dataset_stats(folder: str = "train/data"):
    """
    Get dataset statistics from train/data folder
    
    Scans all datasets in train/data/ (e.g., MyTTSDataset/)
    """
    try:
        data_dir = Path(folder)
        
        if not data_dir.exists():
            logger.warning(f"Dataset folder not found: {data_dir}")
            return {
                "files": 0,
                "total_hours": 0,
                "transcribed_percent": 0,
                "datasets": []
            }
        
        total_files = 0
        total_duration = 0
        transcribed_count = 0
        datasets_info = []
        
        # Scan all dataset folders
        for dataset_dir in data_dir.iterdir():
            if not dataset_dir.is_dir() or dataset_dir.name in ['raw', 'processed', 'subtitles']:
                continue
            
            # Check for wavs folder
            wavs_dir = dataset_dir / "wavs"
            if not wavs_dir.exists():
                continue
            
            audio_files = list(wavs_dir.glob("*.wav"))
            dataset_files = len(audio_files)
            
            # Check metadata.csv for transcriptions
            metadata_file = dataset_dir / "metadata.csv"
            dataset_transcribed = 0
            
            if metadata_file.exists():
                try:
                    with open(metadata_file, "r", encoding="utf-8") as f:
                        dataset_transcribed = sum(1 for _ in f)
                except Exception as e:
                    logger.error(f"Error reading metadata: {e}")
            
            # Check duration.json if available
            duration_file = dataset_dir / "duration.json"
            dataset_duration = 0
            if duration_file.exists():
                try:
                    import json
                    with open(duration_file, "r") as f:
                        duration_data = json.load(f)
                        # duration.json has structure {"duration": [float, float, ...]}
                        if isinstance(duration_data, dict) and "duration" in duration_data:
                            dataset_duration = sum(duration_data["duration"])
                        elif isinstance(duration_data, dict) and "total_duration" in duration_data:
                            dataset_duration = duration_data["total_duration"]
                except Exception as e:
                    logger.error(f"Error reading duration: {e}")
                    dataset_duration = dataset_files * 10  # Estimate 10s per file
            else:
                dataset_duration = dataset_files * 10
            
            total_files += dataset_files
            total_duration += dataset_duration
            transcribed_count += dataset_transcribed
            
            datasets_info.append({
                "name": dataset_dir.name,
                "files": dataset_files,
                "transcribed": dataset_transcribed,
                "duration_seconds": round(dataset_duration, 2)
            })
        
        total_hours = total_duration / 3600
        transcribed_percent = (transcribed_count / total_files * 100) if total_files > 0 else 0
        
        return {
            "files": total_files,
            "total_hours": round(total_hours, 2),
            "transcribed_percent": round(transcribed_percent, 1),
            "datasets": datasets_info
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting dataset stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/datasets")
async def list_datasets(folder: str = "train/data"):
    """
    List available datasets in train/data folder
    
    Returns all folders containing wavs/ subdirectory and metadata.csv
    """
    try:
        data_dir = Path(folder)
        
        if not data_dir.exists():
            logger.warning(f"Dataset folder not found: {data_dir}")
            return {"datasets": []}
        
        datasets = []
        
        for dataset_dir in data_dir.iterdir():
            if not dataset_dir.is_dir() or dataset_dir.name in ['raw', 'processed', 'subtitles']:
                continue
            
            wavs_dir = dataset_dir / "wavs"
            metadata_file = dataset_dir / "metadata.csv"
            
            if wavs_dir.exists():
                audio_files = list(wavs_dir.glob("*.wav"))
                has_metadata = metadata_file.exists()
                
                datasets.append({
                    "name": dataset_dir.name,
                    "path": str(dataset_dir),
                    "files": len(audio_files),
                    "has_metadata": has_metadata
                })
        
        return {"datasets": datasets}
        
    except Exception as e:
        logger.error(f"‚ùå Error listing datasets: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/dataset/files")
async def list_dataset_files(folder: str = "datasets/my_voice", limit: int = 100):
    """
    List dataset files
    """
    try:
        segments_dir = Path(folder) / "segments"
        
        if not segments_dir.exists():
            return {"files": []}
        
        audio_files = sorted(segments_dir.glob("*.wav"))[:limit]
        
        return {
            "files": [
                {
                    "name": f.name,
                    "size": f.stat().st_size,
                    "path": str(f)
                }
                for f in audio_files
            ]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error listing files: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== TRAINING CONTROL ====================

@router.post("/start")
async def start_training(request: TrainingStartRequest, background_tasks: BackgroundTasks):
    """
    Start XTTS-v2 fine-tuning
    
    Uses train/scripts/train_xtts.py
    """
    global training_state
    
    if training_state["is_running"]:
        raise HTTPException(status_code=400, detail="Training already running")
    
    logger.info(f"üéì Starting training: {request.model_name}")
    
    try:
        # Verify dataset exists
        dataset_dir = Path(request.dataset_folder) / "segments"
        if not dataset_dir.exists():
            raise HTTPException(status_code=404, detail=f"Dataset not found: {dataset_dir}")
        
        # Create output directory
        output_dir = Path("train/output") / request.model_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # Build training command
        cmd = [
            "python", "-m", "train.scripts.train_xtts",
            "--dataset", str(dataset_dir),
            "--output", str(output_dir),
            "--epochs", str(request.epochs),
            "--batch-size", str(request.batch_size),
            "--learning-rate", str(request.learning_rate)
        ]
        
        if request.use_deepspeed:
            cmd.append("--deepspeed")
        
        # Start training in background
        background_tasks.add_task(run_training_task, cmd, request.epochs)
        
        training_state["is_running"] = True
        training_state["status"] = {
            "state": "running",
            "epoch": 0,
            "total_epochs": request.epochs,
            "loss": None,
            "progress": 0,
            "logs": "Training started...\n"
        }
        
        return {
            "status": "started",
            "model_name": request.model_name,
            "output_folder": str(output_dir)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error starting training: {e}")
        training_state["is_running"] = False
        raise HTTPException(status_code=500, detail=str(e))


async def run_training_task(cmd: List[str], total_epochs: int):
    """Background task to run training"""
    global training_state
    
    try:
        # Start process
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )
        
        training_state["process"] = process
        
        # Monitor output
        logs = []
        if process.stdout:  # Type check fix
            for line in process.stdout:
                logs.append(line)
                training_state["status"]["logs"] = "".join(logs[-50:])  # Keep last 50 lines
                
                # Parse epoch/loss from logs
                if "Epoch" in line:
                    try:
                        parts = line.split()
                        epoch = int(parts[parts.index("Epoch") + 1].split("/")[0])
                        training_state["status"]["epoch"] = epoch
                        training_state["status"]["progress"] = int(epoch / total_epochs * 100)
                    except:
                        pass
                
                if "Loss:" in line:
                    try:
                        loss_str = line.split("Loss:")[1].split()[0]
                        training_state["status"]["loss"] = float(loss_str)
                    except:
                        pass
        
        # Wait for completion
        process.wait()
        
        if process.returncode == 0:
            training_state["status"]["state"] = "completed"
            logger.info("‚úÖ Training completed successfully")
        else:
            training_state["status"]["state"] = "failed"
            logger.error(f"‚ùå Training failed with code {process.returncode}")
            
    except Exception as e:
        logger.error(f"‚ùå Training error: {e}")
        training_state["status"]["state"] = "failed"
        training_state["status"]["logs"] += f"\nError: {str(e)}"
    
    finally:
        training_state["is_running"] = False
        training_state["process"] = None


@router.post("/stop")
async def stop_training():
    """
    Stop running training
    """
    global training_state
    
    if not training_state["is_running"]:
        raise HTTPException(status_code=400, detail="No training running")
    
    try:
        if training_state["process"]:
            training_state["process"].terminate()
            training_state["process"].wait(timeout=10)
        
        training_state["is_running"] = False
        training_state["status"]["state"] = "stopped"
        training_state["status"]["logs"] += "\nTraining stopped by user\n"
        
        logger.info("‚è∏Ô∏è Training stopped")
        
        return {"status": "stopped"}
        
    except Exception as e:
        logger.error(f"‚ùå Error stopping training: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/status")
async def get_training_status():
    """
    Get current training status
    """
    return training_state["status"]


@router.get("/logs")
async def get_training_logs():
    """
    Get training logs
    """
    return {
        "logs": training_state["status"]["logs"]
    }


# ==================== CHECKPOINT MANAGEMENT ====================

@router.get("/checkpoints")
async def list_checkpoints(model_name: Optional[str] = None):
    """
    List available checkpoints
    """
    try:
        checkpoints = []
        
        if model_name:
            output_dir = Path("train/output") / model_name / "checkpoints"
            if output_dir.exists():
                checkpoints.extend(_scan_checkpoint_dir(output_dir))
        else:
            # Scan all models
            output_root = Path("train/output")
            if output_root.exists():
                # Check direct checkpoints folder (train/output/checkpoints/)
                direct_checkpoint_dir = output_root / "checkpoints"
                if direct_checkpoint_dir.exists():
                    checkpoints.extend(_scan_checkpoint_dir(direct_checkpoint_dir))
                
                # Also check model-specific folders (train/output/{model_name}/checkpoints/)
                for model_dir in output_root.iterdir():
                    if model_dir.is_dir() and model_dir.name != "checkpoints" and model_dir.name != "samples":
                        checkpoint_dir = model_dir / "checkpoints"
                        if checkpoint_dir.exists():
                            checkpoints.extend(_scan_checkpoint_dir(checkpoint_dir))
        
        # Sort by date (newest first)
        checkpoints.sort(key=lambda x: x["date"], reverse=True)
        
        return checkpoints
        
    except Exception as e:
        logger.error(f"‚ùå Error listing checkpoints: {e}")
        raise HTTPException(status_code=500, detail=str(e))


def _scan_checkpoint_dir(checkpoint_dir: Path) -> List[Dict[str, Any]]:
    """Scan checkpoint directory"""
    checkpoints = []
    
    for ckpt_file in checkpoint_dir.glob("*.pt"):
        stat = ckpt_file.stat()
        
        # Extract epoch from filename (e.g., checkpoint_epoch_100.pth)
        epoch = 0
        if "epoch" in ckpt_file.stem:
            try:
                epoch = int(ckpt_file.stem.split("epoch_")[-1])
            except:
                pass
        
        checkpoints.append({
            "name": ckpt_file.stem,
            "path": str(ckpt_file),
            "epoch": epoch,
            "date": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M"),
            "size_mb": round(stat.st_size / 1024 / 1024, 2)
        })
    
    return checkpoints


@router.get("/samples")
async def list_training_samples(model_name: Optional[str] = None):
    """
    List training samples (epoch_N_output.wav files)
    
    Returns audio samples generated during training for quality evaluation.
    Sprint 0 Fix: Enable WebUI to display and play training samples.
    """
    import re
    try:
        samples = []
        samples_root = Path("train/output/samples")
        
        if not samples_root.exists():
            logger.info("Samples directory not found")
            return []
        
        # Scan for epoch_*_output.wav
        for wav_file in sorted(samples_root.glob("epoch_*_output.wav")):
            # Extract epoch number from filename
            epoch_match = re.search(r"epoch_(\d+)", wav_file.stem)
            epoch = int(epoch_match.group(1)) if epoch_match else 0
            
            stat = wav_file.stat()
            
            samples.append({
                "epoch": epoch,
                "filename": wav_file.name,
                "path": f"/static/samples/{wav_file.name}",
                "size_mb": round(stat.st_size / 1024 / 1024, 2),
                "date": datetime.fromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M")
            })
        
        # Sort by epoch (newest first)
        samples.sort(key=lambda x: x["epoch"], reverse=True)
        
        logger.info(f"Found {len(samples)} training samples")
        return samples
        
    except Exception as e:
        logger.error(f"‚ùå Error listing samples: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/checkpoints/load")
async def load_checkpoint(checkpoint_path: str):
    """
    Load a checkpoint for inference
    
    This endpoint validates the checkpoint exists and can be loaded
    """
    try:
        ckpt_path = Path(checkpoint_path)
        
        if not ckpt_path.exists():
            raise HTTPException(status_code=404, detail="Checkpoint not found")
        
        # TODO: Actually load checkpoint into model
        # For now just validate it exists
        
        return {
            "status": "loaded",
            "checkpoint": str(ckpt_path)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error loading checkpoint: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== INFERENCE ====================

@router.post("/inference/synthesize")
async def synthesize_with_checkpoint(request: InferenceSynthesizeRequest):
    """
    Synthesize audio using fine-tuned checkpoint
    
    Uses train/scripts/xtts_inference.py
    """
    logger.info(f"üé§ Synthesizing with checkpoint: {request.checkpoint}")
    
    try:
        # Handle "base" keyword for base model
        if request.checkpoint.lower() == "base":
            ckpt_path = None  # Will use base XTTS-v2 model
        else:
            # Verify checkpoint exists
            ckpt_path = Path(request.checkpoint)
            if not ckpt_path.exists():
                raise HTTPException(status_code=404, detail=f"Checkpoint not found: {request.checkpoint}")
        
        # Auto-select speaker_wav if not provided
        speaker_wav = request.speaker_wav
        if not speaker_wav:
            # Try reference samples first
            reference_samples = list(Path("train/output/samples").glob("*_reference.wav"))
            if reference_samples:
                speaker_wav = str(reference_samples[0])
            else:
                # Fallback to dataset wavs
                dataset_wavs = list(Path("train/data/MyTTSDataset/wavs").glob("*.wav"))
                if dataset_wavs:
                    speaker_wav = str(dataset_wavs[0])
                else:
                    raise HTTPException(status_code=400, detail="No speaker reference found. Provide speaker_wav or add samples to train/output/samples/")
        
        logger.info(f"   Speaker reference: {speaker_wav}")
        
        # Create temp output file
        output_dir = Path("temp/inference")
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"inference_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
        
        # Run inference script
        cmd = [
            "python", "-m", "train.scripts.xtts_inference",
            "--text", request.text,
            "--speaker", speaker_wav,
            "--output", str(output_file),
            "--temperature", str(request.temperature),
            "--speed", str(request.speed)
        ]
        
        # Add checkpoint if not base model
        if ckpt_path:
            cmd.extend(["--checkpoint", str(ckpt_path)])
        
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        if result.returncode != 0:
            raise HTTPException(status_code=500, detail=f"Inference failed: {result.stderr}")
        
        return {
            "status": "success",
            "audio_url": f"/files/inference/{output_file.name}",
            "text": request.text
        }
        
    except subprocess.TimeoutExpired:
        raise HTTPException(status_code=504, detail="Inference timeout")
    except Exception as e:
        logger.error(f"‚ùå Error in inference: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/inference/ab-test")
async def ab_test_comparison(request: InferenceABTestRequest):
    """
    Generate A/B comparison between base model and fine-tuned model
    """
    logger.info(f"üîÑ A/B test: {request.checkpoint}")
    
    try:
        # Verify checkpoint
        ckpt_path = Path(request.checkpoint)
        if not ckpt_path.exists():
            raise HTTPException(status_code=404, detail="Checkpoint not found")
        
        output_dir = Path("temp/ab_test")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Generate with base model
        base_output = output_dir / f"base_{timestamp}.wav"
        cmd_base = [
            "python", "-m", "train.scripts.xtts_inference",
            "--text", request.text,
            "--output", str(base_output)
        ]
        subprocess.run(cmd_base, capture_output=True, timeout=60)
        
        # Generate with fine-tuned model
        finetuned_output = output_dir / f"finetuned_{timestamp}.wav"
        cmd_finetuned = [
            "python", "-m", "train.scripts.xtts_inference",
            "--checkpoint", str(ckpt_path),
            "--text", request.text,
            "--output", str(finetuned_output)
        ]
        subprocess.run(cmd_finetuned, capture_output=True, timeout=60)
        
        # TODO: Calculate similarity metrics
        # For now return dummy metrics
        similarity = 85.5
        mfcc_score = 92.3
        
        return {
            "status": "success",
            "base_audio": f"/files/ab_test/{base_output.name}",
            "finetuned_audio": f"/files/ab_test/{finetuned_output.name}",
            "similarity": similarity,
            "mfcc_score": mfcc_score
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error in A/B test: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== HELPER FUNCTIONS ====================

async def run_subprocess_task(cmd: List[str], task_name: str):
    """Generic background task runner"""
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=3600)
        if result.returncode == 0:
            logger.info(f"‚úÖ {task_name} completed successfully")
        else:
            logger.error(f"‚ùå {task_name} failed: {result.stderr}")
    except subprocess.TimeoutExpired:
        logger.error(f"‚ùå {task_name} timeout")
    except Exception as e:
        logger.error(f"‚ùå {task_name} error: {e}")
