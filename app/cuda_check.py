"""
CUDA Availability Check Module

Executa no startup para validar disponibilidade de GPU e logar informa√ß√µes.
Ajuda a diagnosticar problemas de CUDA/GPU antes do carregamento dos modelos.

Uso:
    from app.cuda_check import check_cuda
    
    # No startup
    check_cuda()
"""
import logging
import os

logger = logging.getLogger(__name__)


def check_cuda() -> bool:
    """
    Verifica disponibilidade de CUDA e loga informa√ß√µes detalhadas.
    
    Returns:
        bool: True se CUDA dispon√≠vel, False caso contr√°rio
    """
    try:
        import torch
    except ImportError:
        logger.error("‚ùå PyTorch n√£o instalado! N√£o √© poss√≠vel verificar CUDA.")
        return False
    
    # Verificar se CUDA est√° dispon√≠vel
    cuda_available = torch.cuda.is_available()
    
    if not cuda_available:
        logger.warning("‚ö†Ô∏è  CUDA N√ÉO DISPON√çVEL!")
        logger.warning("    Modelos TTS rodar√£o em CPU (mais lento)")
        logger.warning("    Verifique:")
        logger.warning("    1. Driver NVIDIA instalado: nvidia-smi")
        logger.warning("    2. NVIDIA Docker runtime: docker info | grep -i nvidia")
        logger.warning("    3. Container com --gpus all ou deploy.resources.devices")
        return False
    
    # CUDA dispon√≠vel - logar informa√ß√µes
    try:
        device_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        gpu_props = torch.cuda.get_device_properties(current_device)
        
        vram_total_gb = gpu_props.total_memory / 1024**3
        vram_total_mb = gpu_props.total_memory / 1024**2
        
        # Log informa√ß√µes b√°sicas
        logger.info("=" * 60)
        logger.info("üéÆ CUDA DISPON√çVEL")
        logger.info("=" * 60)
        logger.info(f"üìä GPU: {gpu_name}")
        logger.info(f"üìä CUDA Version: {torch.version.cuda}")
        logger.info(f"üìä PyTorch Version: {torch.__version__}")
        logger.info(f"üìä Device Count: {device_count}")
        logger.info(f"üìä Current Device: {current_device}")
        logger.info(f"üìä VRAM Total: {vram_total_gb:.2f} GB ({vram_total_mb:.0f} MB)")
        logger.info(f"üìä Compute Capability: {gpu_props.major}.{gpu_props.minor}")
        
        # Verificar mem√≥ria dispon√≠vel
        free_mem, total_mem = torch.cuda.mem_get_info(current_device)
        free_gb = free_mem / 1024**3
        
        logger.info(f"üìä VRAM Livre: {free_gb:.2f} GB ({free_mem / 1024**2:.0f} MB)")
        logger.info("=" * 60)
        
        # Avisos baseados na quantidade de VRAM
        if vram_total_gb < 4.0:
            logger.warning("‚ö†Ô∏è  GPU MUITO PEQUENA (< 4GB)!")
            logger.warning("    Recomenda√ß√£o: Use CPU para TTS")
            logger.warning("    Configure: XTTS_DEVICE=cpu F5TTS_DEVICE=cpu")
        
        elif vram_total_gb < 6.0:
            logger.warning("‚ö†Ô∏è  GPU PEQUENA (< 6GB) DETECTADA!")
            logger.warning("    CRITICAL: Ative LOW_VRAM mode para evitar OOM!")
            logger.warning("    Configure: LOW_VRAM=true no .env")
            logger.warning("")
            
            # Verificar se LOW_VRAM est√° ativado
            low_vram = os.getenv('LOW_VRAM', 'false').lower() == 'true'
            if low_vram:
                logger.info("‚úÖ LOW_VRAM mode ATIVADO (correto para GPU < 6GB)")
            else:
                logger.error("‚ùå LOW_VRAM mode DESATIVADO!")
                logger.error("   GPU pequena sem LOW_VRAM = OOM garantido!")
                logger.error("   A√á√ÉO NECESS√ÅRIA: Configure LOW_VRAM=true no .env")
        
        elif vram_total_gb < 8.0:
            logger.info("‚ÑπÔ∏è  GPU m√©dia detectada (6-8GB)")
            logger.info("   Recomenda√ß√£o: LOW_VRAM=true para maior estabilidade")
        
        else:
            logger.info("‚úÖ GPU grande detectada (>= 8GB)")
            logger.info("   Pode rodar XTTS + F5-TTS simultaneamente")
            logger.info("   LOW_VRAM=false √© seguro")
        
        return True
    
    except Exception as e:
        logger.error(f"‚ùå Erro ao verificar informa√ß√µes CUDA: {e}", exc_info=True)
        return False


def log_cuda_memory_usage(prefix: str = ""):
    """
    Loga uso atual de mem√≥ria CUDA (√∫til para debugging).
    
    Args:
        prefix: Prefixo para a mensagem de log
    """
    try:
        import torch
        
        if not torch.cuda.is_available():
            return
        
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        free, total = torch.cuda.mem_get_info()
        free_gb = free / 1024**3
        total_gb = total / 1024**3
        
        msg = f"{prefix}VRAM: allocated={allocated:.2f}GB, reserved={reserved:.2f}GB, free={free_gb:.2f}GB/{total_gb:.2f}GB"
        logger.info(f"üìä {msg}")
    
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  N√£o foi poss√≠vel logar uso de VRAM: {e}")


if __name__ == "__main__":
    # Teste standalone
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    result = check_cuda()
    print(f"\nCUDA Available: {result}")
