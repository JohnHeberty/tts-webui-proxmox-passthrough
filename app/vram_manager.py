"""
LOW VRAM Mode - Gerenciamento autom√°tico de VRAM

Quando LOW_VRAM=true, este m√≥dulo:
1. Carrega modelo apenas quando necess√°rio
2. Processa √°udio
3. Descarrega modelo da VRAM imediatamente
4. Repete para pr√≥ximo modelo (RVC, etc)

Benef√≠cios:
- Permite rodar em GPUs com pouca VRAM (4GB-6GB)
- Evita OOM (Out of Memory) errors
- Aumenta lat√™ncia (carregamento de modelo a cada uso)
"""

import gc
import torch
from typing import Optional, Callable, Any
from contextlib import contextmanager
from functools import wraps
import logging

from .config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class VRAMManager:
    """
    Gerenciador de VRAM para modo LOW_VRAM.
    
    Controla carregamento/descarregamento autom√°tico de modelos.
    """
    
    def __init__(self):
        settings = get_settings()
        self.low_vram_mode = settings.get('low_vram_mode', False)
        
        # Debug: Logar valor raw da vari√°vel de ambiente e valor parseado
        import os
        env_value = os.getenv('LOW_VRAM', 'NOT_SET')
        logger.info(f"üîç DEBUG: LOW_VRAM env='{env_value}', parsed={self.low_vram_mode}")
        
        self._model_cache = {}  # Cache de modelos (quando LOW_VRAM=false)
        
        if self.low_vram_mode:
            logger.info("üîã LOW VRAM MODE: ATIVADO - Modelos ser√£o carregados/descarregados automaticamente")
            logger.info("    üí° Economia de VRAM: 70-75%")
            logger.info("    ‚ö†Ô∏è  Lat√™ncia aumentada: +2-5s por requisi√ß√£o")
        else:
            logger.info("‚ö° NORMAL MODE: Modelos permanecer√£o na VRAM")
            logger.info("    üí° Melhor performance, maior consumo de VRAM")
    
    @contextmanager
    def load_model(self, model_key: str, load_fn: Callable, *args, **kwargs):
        """
        Context manager para carregar modelo temporariamente.
        
        Uso:
            with vram_manager.load_model('xtts', load_xtts_model, config):
                output = model.process(input)
            # Modelo √© descarregado automaticamente aqui
        
        Args:
            model_key: Identificador √∫nico do modelo
            load_fn: Fun√ß√£o que carrega o modelo
            *args, **kwargs: Argumentos para load_fn
        
        Yields:
            Modelo carregado
        """
        model = None
        
        try:
            # Em modo LOW_VRAM, sempre carrega fresh
            # Em modo NORMAL, usa cache
            if self.low_vram_mode:
                logger.info(f"üîã LOW_VRAM: Carregando modelo '{model_key}' na GPU...")
                
                # Log VRAM antes do load (se CUDA dispon√≠vel)
                if torch.cuda.is_available():
                    before_allocated = torch.cuda.memory_allocated() / 1024**3
                    logger.debug(f"üìä VRAM antes do load: {before_allocated:.2f} GB")
                
                model = load_fn(*args, **kwargs)
                
                # Log VRAM ap√≥s load
                if torch.cuda.is_available():
                    after_allocated = torch.cuda.memory_allocated() / 1024**3
                    delta = after_allocated - before_allocated
                    logger.info(f"üìä VRAM alocada: {after_allocated:.2f} GB (Œî +{delta:.2f} GB)")
            else:
                # Usar cache
                if model_key not in self._model_cache:
                    logger.info(f"‚ö° Carregando modelo '{model_key}' (primeira vez, ser√° cacheado)")
                    self._model_cache[model_key] = load_fn(*args, **kwargs)
                else:
                    logger.debug(f"‚ö° Usando modelo '{model_key}' do cache")
                model = self._model_cache[model_key]
            
            yield model
        
        finally:
            # Descarregar apenas em modo LOW_VRAM
            if self.low_vram_mode and model is not None:
                logger.info(f"üîã LOW_VRAM: Descarregando modelo '{model_key}' da VRAM...")
                
                # Log VRAM antes do unload
                if torch.cuda.is_available():
                    before_free = torch.cuda.memory_allocated() / 1024**3
                
                self._unload_model(model)
                del model
                
                # Log VRAM depois do unload
                if torch.cuda.is_available():
                    after_free = torch.cuda.memory_allocated() / 1024**3
                    freed = before_free - after_free
                    logger.info(f"üìä VRAM liberada: {freed:.2f} GB (antes={before_free:.2f}, depois={after_free:.2f} GB)")
    
    def _unload_model(self, model):
        """
        Descarrega modelo da VRAM.
        
        Args:
            model: Modelo a ser descarregado
        """
        try:
            models_moved = 0
            
            # Estrat√©gia 1: Mover modelo direto (PyTorch nn.Module)
            if hasattr(model, 'to'):
                logger.debug("Moving model to CPU via .to('cpu')")
                model.to('cpu')
                models_moved += 1
            elif hasattr(model, 'cpu'):
                logger.debug("Moving model to CPU via .cpu()")
                model.cpu()
                models_moved += 1
            
            # Estrat√©gia 2: Procurar submodelos (F5TTS API wrapper)
            # F5TTS API tem atributos: .model, .vocoder, etc
            for attr_name in dir(model):
                if attr_name.startswith('_'):
                    continue
                try:
                    attr = getattr(model, attr_name)
                    # Se √© um m√≥dulo PyTorch, mover para CPU
                    if hasattr(attr, 'to') and callable(attr.to):
                        logger.debug(f"Moving submodel '{attr_name}' to CPU")
                        attr.to('cpu')
                        models_moved += 1
                    elif hasattr(attr, 'cpu') and callable(attr.cpu):
                        logger.debug(f"Moving submodel '{attr_name}' to CPU via .cpu()")
                        attr.cpu()
                        models_moved += 1
                except Exception as e:
                    # Ignorar atributos que n√£o s√£o modelos
                    logger.debug(f"Skipping attribute '{attr_name}': {e}")
                    continue
            
            # Estrat√©gia 3: Verificar __dict__ para modelos encapsulados
            if hasattr(model, '__dict__'):
                for key, value in model.__dict__.items():
                    if key.startswith('_'):
                        continue
                    try:
                        if hasattr(value, 'to') and callable(value.to):
                            logger.debug(f"Moving __dict__ model '{key}' to CPU")
                            value.to('cpu')
                            models_moved += 1
                    except Exception as e:
                        logger.debug(f"Skipping __dict__ key '{key}': {e}")
                        continue
            
            logger.debug(f"üì¶ Moved {models_moved} model(s) to CPU")
            
            # Liberar refer√™ncias
            if hasattr(model, 'eval'):
                model.eval()
            
            # Limpar cache CUDA (cr√≠tico!)
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()  # Aguardar opera√ß√µes CUDA completarem
                torch.cuda.ipc_collect()
            
            # Garbage collection agressivo
            gc.collect()
            gc.collect()  # Segunda passagem
            
            logger.debug("‚úÖ Modelo descarregado com sucesso")
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao descarregar modelo: {e}")
    
    def clear_all_cache(self):
        """Limpa todo o cache de modelos (for√ßar reload)."""
        logger.info("üóëÔ∏è Limpando cache de modelos")
        self._model_cache.clear()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
        
        gc.collect()
    
    def get_vram_stats(self) -> dict:
        """
        Retorna estat√≠sticas de uso de VRAM.
        
        Returns:
            Dict com estat√≠sticas de VRAM (GB)
        """
        if not torch.cuda.is_available():
            return {
                "available": False,
                "low_vram_mode": self.low_vram_mode
            }
        
        allocated = torch.cuda.memory_allocated() / 1024**3  # GB
        reserved = torch.cuda.memory_reserved() / 1024**3
        free, total = torch.cuda.mem_get_info()
        free_gb = free / 1024**3
        total_gb = total / 1024**3
        
        return {
            "available": True,
            "low_vram_mode": self.low_vram_mode,
            "allocated_gb": round(allocated, 2),
            "reserved_gb": round(reserved, 2),
            "free_gb": round(free_gb, 2),
            "total_gb": round(total_gb, 2),
            "cached_models": len(self._model_cache) if not self.low_vram_mode else 0
        }


# Singleton global
_vram_manager = None


def get_vram_manager() -> VRAMManager:
    """Retorna o gerenciador global de VRAM (singleton)."""
    global _vram_manager
    if _vram_manager is None:
        _vram_manager = VRAMManager()
    return _vram_manager


def with_vram_management(model_key: str):
    """
    Decorator para gerenciar VRAM automaticamente.
    
    Uso:
        @with_vram_management('xtts')
        def synthesize(self, text, voice):
            # self.model j√° est√° carregado
            return self.model.process(text, voice)
        # Modelo descarregado automaticamente ap√≥s retorno
    
    Args:
        model_key: Identificador √∫nico do modelo
    """
    def decorator(func):
        @wraps(func)
        def wrapper(self, *args, **kwargs):
            vram_mgr = get_vram_manager()
            
            # Se n√£o estiver em LOW_VRAM mode, executar normalmente
            if not vram_mgr.low_vram_mode:
                return func(self, *args, **kwargs)
            
            # Em LOW_VRAM mode, carregar e descarregar
            # Assume que a classe tem um m√©todo _load_model()
            if not hasattr(self, '_load_model'):
                logger.warning(f"Classe {self.__class__.__name__} n√£o tem m√©todo _load_model()")
                return func(self, *args, **kwargs)
            
            with vram_mgr.load_model(model_key, self._load_model):
                result = func(self, *args, **kwargs)
            
            return result
        
        return wrapper
    return decorator


def clear_vram_cache():
    """Helper para limpar cache de VRAM manualmente."""
    vram_mgr = get_vram_manager()
    vram_mgr.clear_all_cache()
    logger.info("‚úÖ Cache de VRAM limpo")


def get_vram_usage() -> dict:
    """Helper para obter estat√≠sticas de VRAM."""
    vram_mgr = get_vram_manager()
    return vram_mgr.get_vram_stats()


# Singleton global instance
vram_manager = get_vram_manager()
