# ðŸš€ Plano de Sprints - TTS WebUI XTTS-v2

**Projeto**: ImplementaÃ§Ã£o completa de pipeline fine-tuning XTTS-v2  
**Baseado em**: [MORE.md](./MORE.md)  
**Tech Lead**: Claude Sonnet 4.5  
**Data inÃ­cio**: 2025-12-06

---

## ðŸ“Š VisÃ£o Geral

| Sprint | Foco | DuraÃ§Ã£o | Status |
|--------|------|---------|--------|
| Sprint 0 | SeguranÃ§a & Cleanup | 1-2h | ðŸ”„ Pronto para iniciar |
| Sprint 1 | Estrutura `train/` + Pipeline Dados | 4-6h | â³ Planejada |
| Sprint 2 | Treinamento XTTS-v2 | 6-8h | â³ Planejada |
| Sprint 3 | IntegraÃ§Ã£o API + InferÃªncia | 3-4h | â³ Planejada |
| Sprint 4 | Qualidade & Testes | 4-5h | â³ Planejada |
| Sprint 5 | Docs & DevOps | 2-3h | â³ Planejada |

**Total estimado**: 20-28 horas de desenvolvimento

---

## ðŸ”’ Sprint 0: SeguranÃ§a & Cleanup (CRÃTICO)

**Objetivo**: Garantir seguranÃ§a do repositÃ³rio e limpar referÃªncias obsoletas.

**DuraÃ§Ã£o**: 1-2 horas  
**Prioridade**: P0 (CRÃTICO)

### Tasks

#### 1. Auditoria de Secrets
- [ ] Verificar se `.env` estÃ¡ no `.gitignore`
  ```bash
  grep -E "^\.env$" .gitignore
  ```
- [ ] Inspecionar histÃ³rico Git por secrets commitados
  ```bash
  git log --all --full-history --source --pickaxe-regex -S "API_KEY|SECRET|PASSWORD" -- .env
  ```
- [ ] Se encontrar secrets expostos:
  - Rotacionar todas as chaves imediatamente
  - Usar `git filter-branch` ou BFG Repo-Cleaner para remover histÃ³rico
  - Documentar em `SECURITY_INCIDENT.md`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `.env` nÃ£o estÃ¡ commitado no repositÃ³rio
- âœ… `.env.example` criado sem valores sensÃ­veis
- âœ… Nenhum secret no histÃ³rico Git

**Riscos**:
- ðŸ”´ **ALTO**: Secrets expostos podem comprometer APIs externas
- ðŸŸ¡ **MÃ‰DIO**: Reescrever histÃ³rico Git pode afetar colaboradores

---

#### 2. Limpar Docs Obsoletas de F5-TTS
- [ ] Atualizar `docs/LOW_VRAM.md`:
  - Adicionar nota no topo: "âš ï¸ DEPRECATED: F5-TTS removed in v2.0"
  - Marcar seÃ§Ãµes F5-TTS como obsoletas
- [ ] Atualizar `docs/QUALITY_PROFILES.md`:
  - Remover seÃ§Ã£o "Perfis PadrÃ£o F5-TTS"
  - Atualizar exemplos para XTTS apenas
- [ ] Atualizar `docs/CHANGELOG.md`:
  - Adicionar entrada para v2.0 destacando remoÃ§Ã£o F5-TTS
  - Link para `F5_TTS_REMOVED.md`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Nenhuma documentaÃ§Ã£o sugere que F5-TTS estÃ¡ funcional
- âœ… Todas as referÃªncias marcadas como deprecated ou removidas
- âœ… Changelog atualizado com v2.0

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas documentaÃ§Ã£o, sem risco tÃ©cnico

---

#### 3. Renomear `scripts/not_remove/`
- [ ] Mover para estrutura mais clara:
  ```bash
  mkdir -p scripts/dataset
  mv scripts/not_remove/* scripts/dataset/
  rmdir scripts/not_remove
  ```
- [ ] Atualizar imports se algum script referencia:
  ```python
  # Antes
  from scripts.not_remove.download_youtube import download
  # Depois
  from scripts.dataset.download_youtube import download
  ```
- [ ] Atualizar README com nova estrutura

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `scripts/not_remove/` nÃ£o existe mais
- âœ… `scripts/dataset/` contÃ©m todos os scripts de preparaÃ§Ã£o de dados
- âœ… Nenhum import quebrado

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Imports podem quebrar se nÃ£o testar bem

---

## ðŸ—ï¸ Sprint 1: Estrutura `train/` + Pipeline de Dados

**Objetivo**: Criar infraestrutura completa para preparar dataset XTTS-v2 em formato LJSpeech.

**DuraÃ§Ã£o**: 4-6 horas  
**Prioridade**: P0 (CRÃTICO - bloqueia Sprint 2)

### Tasks

#### 1.1 Criar Estrutura de DiretÃ³rios
- [ ] Criar Ã¡rvore de pastas:
  ```bash
  mkdir -p train/{config,data/{raw,processed/wavs,MyTTSDataset/wavs},scripts,output/{checkpoints,samples}}
  ```
- [ ] Adicionar `.gitkeep` em pastas vazias
- [ ] Criar `.gitignore` para `train/data/raw/*` e `train/output/*` (nÃ£o commitar datasets/checkpoints grandes)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Estrutura completa criada
- âœ… Git ignora arquivos grandes de dados/modelos
- âœ… README em `train/README.md` explicando estrutura

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas criaÃ§Ã£o de pastas

---

#### 1.2 Criar `train/config/dataset_config.yaml`
- [ ] Implementar configuraÃ§Ã£o:
  ```yaml
  # Dataset preparation config for XTTS-v2
  audio:
    sample_rate: 22050  # XTTS-v2 requirement
    channels: 1         # mono
    bit_depth: 16
    format: wav
  
  segmentation:
    min_duration_sec: 5.0    # minimum viable segment
    max_duration_sec: 12.0   # XTTS-v2 limit
    target_duration_sec: 8.0 # ideal length
    vad_threshold_db: -40    # voice activity detection
    min_silence_duration_ms: 500
    padding_ms: 100          # before/after speech
  
  transcription:
    model: openai/whisper-base  # or whisper-small
    language: pt              # Portuguese
    task: transcribe
    temperature: 0.0          # deterministic
  
  text_normalization:
    expand_numbers: true      # "123" â†’ "cento e vinte e trÃªs"
    lowercase: false          # preserve case
    remove_punctuation: false # keep for prosody
  
  quality_filters:
    min_snr_db: 15.0          # signal-to-noise ratio
    max_silence_ratio: 0.3    # max 30% silence in segment
    min_words: 3              # minimum words per segment
    max_words: 40             # maximum words per segment
  
  dataset:
    train_split: 0.9          # 90% train
    val_split: 0.1            # 10% validation
    shuffle: true
    seed: 42
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… YAML vÃ¡lido e bem documentado
- âœ… Valores alinhados com boas prÃ¡ticas XTTS-v2
- âœ… FÃ¡cil modificar para experimentar

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas arquivo de configuraÃ§Ã£o

---

#### 1.3 Refatorar `download_youtube.py`
- [ ] Mover de `scripts/dataset/download_youtube.py` para `train/scripts/download_youtube.py`
- [ ] Adicionar suporte a batch download via CSV:
  ```python
  # Input: train/data/sources.csv
  # url,title,duration
  # https://youtube.com/watch?v=xxx,Podcast 1,3600
  ```
- [ ] Converter para 22050Hz mono durante download (economizar processamento depois)
- [ ] Salvar em `train/data/raw/{youtube_id}.wav`
- [ ] Gerar `download_manifest.json` com metadados

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Aceita CSV com lista de URLs
- âœ… Download + conversÃ£o para 22050Hz mono 16-bit
- âœ… Logging estruturado (JSON ou rich)
- âœ… Tratamento de erros (retry, skip vÃ­deos privados)

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: yt-dlp pode quebrar com updates do YouTube
- ðŸŸ¡ **MÃ‰DIO**: ConversÃ£o de Ã¡udio pode falhar (ffmpeg)

---

#### 1.4 Refatorar `segment_audio.py`
- [ ] Mover de `scripts/dataset/` para `train/scripts/`
- [ ] Implementar streaming VAD:
  ```python
  def segment_audio_streaming(
      input_wav: Path,
      output_dir: Path,
      config: DatasetConfig
  ) -> List[Segment]:
      """
      Segment audio using VAD, optimized for memory.
      
      Returns list of Segment objects with:
      - file_path: Path to output WAV
      - start_time: float (seconds)
      - end_time: float
      - duration: float
      - rms_db: float (loudness)
      """
  ```
- [ ] Aplicar filtros do config:
  - DuraÃ§Ã£o: 5-12s
  - Target: ~8s (juntar segmentos curtos quando possÃ­vel)
  - RMS threshold para VAD
- [ ] Salvar em `train/data/processed/wavs/` com naming scheme:
  ```
  {youtube_id}_{segment_index:04d}.wav
  ```
- [ ] Gerar `segments_manifest.json`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Processa arquivos grandes (>1GB) sem OOM
- âœ… Segmentos entre 5-12s (idealmente ~8s)
- âœ… 22050Hz mono 16-bit preservado
- âœ… RMS filtering remove silÃªncios/ruÃ­dos

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: VAD pode ser impreciso (ajustar threshold empiricamente)
- ðŸŸ¢ **BAIXO**: Memory-efficient se usar streaming corretamente

---

#### 1.5 Refatorar `transcribe_whisper.py`
- [ ] Mover para `train/scripts/`
- [ ] Integrar com `segments_manifest.json`
- [ ] Implementar transcriÃ§Ã£o batch:
  ```python
  def transcribe_segments(
      segments: List[Segment],
      config: DatasetConfig,
      model: WhisperModel = None
  ) -> List[Transcription]:
      """
      Transcribe all segments using Whisper.
      
      Returns list of Transcription objects:
      - segment_id: str
      - text_raw: str (original transcription)
      - text_normalized: str (apÃ³s normalizaÃ§Ã£o)
      - confidence: float (0-1)
      - language_detected: str
      """
  ```
- [ ] Aplicar normalizaÃ§Ã£o de texto pt-BR:
  - Expandir nÃºmeros (via `num2words` pt-BR)
  - Normalizar pontuaÃ§Ã£o
  - Remover caracteres especiais (manter acentos)
- [ ] Salvar em `train/data/processed/transcriptions.json`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Transcreve todos os segmentos do manifest
- âœ… NormalizaÃ§Ã£o pt-BR correta (nÃºmeros expandidos)
- âœ… Confidence score calculado
- âœ… Logging de progresso (tqdm ou rich)

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Whisper pode demorar muito (usar GPU se disponÃ­vel)
- ðŸŸ¡ **MÃ‰DIO**: Erros de transcriÃ§Ã£o em Ã¡udio ruÃ­doso

---

#### 1.6 Criar `build_ljs_dataset.py`
- [ ] Implementar builder LJSpeech:
  ```python
  def build_ljspeech_dataset(
      transcriptions: List[Transcription],
      segments: List[Segment],
      output_dir: Path,
      config: DatasetConfig
  ) -> LJSpeechDataset:
      """
      Build LJSpeech format dataset:
      - Copy/move wavs to MyTTSDataset/wavs/
      - Generate metadata.csv
      - Apply quality filters
      - Split train/val
      """
  ```
- [ ] Formato `metadata.csv`:
  ```
  filename|raw_text|normalized_text
  seg_0001.wav|OlÃ¡ mundo 123|OlÃ¡ mundo cento e vinte e trÃªs
  ```
- [ ] Aplicar filtros de qualidade:
  - SNR > 15dB
  - Silence ratio < 30%
  - 3-40 palavras por segmento
- [ ] Gerar splits:
  - `train/data/MyTTSDataset/metadata_train.csv` (90%)
  - `train/data/MyTTSDataset/metadata_val.csv` (10%)
- [ ] EstatÃ­sticas em `dataset_stats.json`:
  ```json
  {
    "total_segments": 1500,
    "total_duration_hours": 3.2,
    "filtered_out": 120,
    "train_samples": 1242,
    "val_samples": 138,
    "avg_duration_sec": 7.8
  }
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `metadata.csv` em formato LJSpeech vÃ¡lido
- âœ… Filtros de qualidade aplicados
- âœ… Train/val split correto
- âœ… EstatÃ­sticas geradas

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Filtros podem remover muitos samples (ajustar thresholds)
- ðŸŸ¢ **BAIXO**: Formato LJSpeech Ã© bem definido

---

#### 1.7 Criar `pipeline.py` (Orquestrador)
- [ ] Script master que executa pipeline completo:
  ```python
  # train/scripts/pipeline.py
  
  @click.command()
  @click.option('--config', type=Path, default='train/config/dataset_config.yaml')
  @click.option('--sources', type=Path, default='train/data/sources.csv')
  @click.option('--skip-download', is_flag=True)
  @click.option('--skip-segment', is_flag=True)
  @click.option('--skip-transcribe', is_flag=True)
  def run_pipeline(config, sources, skip_download, skip_segment, skip_transcribe):
      """
      Run complete data pipeline:
      1. Download YouTube videos
      2. Segment audio with VAD
      3. Transcribe with Whisper
      4. Build LJSpeech dataset
      """
      cfg = load_config(config)
      
      if not skip_download:
          logger.info("Step 1: Downloading videos...")
          download_youtube_batch(sources, cfg)
      
      if not skip_segment:
          logger.info("Step 2: Segmenting audio...")
          segment_all_audio(cfg)
      
      if not skip_transcribe:
          logger.info("Step 3: Transcribing...")
          transcribe_all_segments(cfg)
      
      logger.info("Step 4: Building LJSpeech dataset...")
      build_ljspeech_dataset(cfg)
      
      logger.info("âœ… Pipeline complete!")
  ```
- [ ] Logging estruturado (rich progress bars)
- [ ] Tratamento de erros global
- [ ] Checkpoint/resume (se pipeline falhar no meio)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Pipeline roda de ponta a ponta sem intervenÃ§Ã£o
- âœ… Flags para pular etapas jÃ¡ completadas
- âœ… Progress bars claros
- âœ… Logs salvos em `train/logs/pipeline_{timestamp}.log`

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Pipeline pode falhar no meio (precisa ser resiliente)
- ðŸŸ¢ **BAIXO**: LÃ³gica simples se scripts individuais estÃ£o OK

---

### Sprint 1 - Deliverables

- [x] Estrutura `train/` completa
- [x] `dataset_config.yaml` bem documentado
- [x] Scripts refatorados e integrados:
  - `download_youtube.py`
  - `segment_audio.py`
  - `transcribe_whisper.py`
  - `build_ljs_dataset.py`
  - `pipeline.py`
- [x] Dataset LJSpeech de teste gerado (1-2h de Ã¡udio pt-BR)
- [x] DocumentaÃ§Ã£o em `train/README.md`

---

## ðŸŽ“ Sprint 2: Treinamento XTTS-v2

**Objetivo**: Implementar fine-tuning completo do XTTS-v2 com suporte a LoRA.

**DuraÃ§Ã£o**: 6-8 horas  
**Prioridade**: P0 (CRÃTICO - core feature)

### Tasks

#### 2.1 Criar `train/config/train_config.yaml`
- [ ] ConfiguraÃ§Ã£o de treinamento:
  ```yaml
  # XTTS-v2 Fine-tuning Configuration
  
  model:
    name: xtts_v2
    # Base model - auto-download via Coqui TTS
    checkpoint: tts_models/multilingual/multi-dataset/xtts_v2
    # Or custom checkpoint:
    # checkpoint: ./models/xtts_pretrained/model.pth
    
    # LoRA config (memory-efficient)
    use_lora: true
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
  
  training:
    # Hardware
    device: cuda
    mixed_precision: true  # FP16
    num_workers: 4
    
    # Optimization
    batch_size: 4          # adjust for VRAM
    gradient_accumulation_steps: 2  # effective batch = 8
    epochs: 50
    learning_rate: 1.0e-5
    warmup_steps: 100
    scheduler: cosine
    
    # Regularization
    weight_decay: 0.01
    gradient_clip_norm: 1.0
    
    # Constraints
    max_text_length: 200   # characters
    max_audio_length: 12   # seconds
    
    # Checkpointing
    save_every_n_epochs: 5
    keep_last_n_checkpoints: 3
    early_stopping_patience: 10
  
  dataset:
    path: ./train/data/MyTTSDataset
    metadata_train: metadata_train.csv
    metadata_val: metadata_val.csv
    language: pt-BR
    sample_rate: 22050
    
    # Data augmentation (optional)
    augmentation:
      enabled: false
      pitch_shift_semitones: [-2, 2]
      time_stretch_factor: [0.9, 1.1]
  
  logging:
    tensorboard_dir: ./train/output/tensorboard
    log_every_n_steps: 10
    sample_every_n_epochs: 5  # generate audio samples
    num_samples: 3
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… YAML vÃ¡lido e bem documentado
- âœ… Suporta LoRA e full fine-tune
- âœ… Valores reasonable para RTX 3090 (23GB VRAM)

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas arquivo de configuraÃ§Ã£o

---

#### 2.2 Baixar Modelo Pretrained XTTS-v2
- [ ] Script para download automÃ¡tico:
  ```python
  # train/scripts/download_pretrained.py
  
  from TTS.api import TTS
  import shutil
  
  def download_xtts_v2(output_dir: Path):
      """Download XTTS-v2 pretrained model via Coqui TTS."""
      print("Downloading XTTS-v2 base model...")
      tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
      
      # Model files sÃ£o salvos em ~/.local/share/tts/
      # Copiar para projeto para garantir versionamento
      src = Path.home() / ".local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2"
      dst = output_dir / "xtts_v2_base"
      
      if dst.exists():
          print(f"Model already exists at {dst}")
          return dst
      
      shutil.copytree(src, dst)
      print(f"âœ… Model downloaded to {dst}")
      return dst
  ```
- [ ] Salvar em `models/xtts_pretrained/`
- [ ] Adicionar verificaÃ§Ã£o de integridade (checksum)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Modelo baixado automaticamente
- âœ… Arquivos copiados para `models/xtts_pretrained/`
- âœ… Checksum validado (MD5 ou SHA256)

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Download pode falhar (retry logic)
- ðŸŸ¡ **MÃ‰DIO**: Modelo grande (~2GB) pode demorar

---

#### 2.3 Implementar `xtts_train.py`
- [ ] Script de treinamento completo:
  ```python
  # train/scripts/xtts_train.py
  
  import torch
  from TTS.tts.configs.xtts_config import XttsConfig
  from TTS.tts.models.xtts import Xtts
  from TTS.tts.datasets import load_tts_samples
  from torch.utils.data import DataLoader
  from torch.utils.tensorboard import SummaryWriter
  
  class XTTSTrainer:
      def __init__(self, config_path: Path):
          self.config = load_config(config_path)
          self.device = torch.device(self.config.training.device)
          
          # Load model
          self.model = self._load_model()
          
          # Setup LoRA if enabled
          if self.config.model.use_lora:
              self._setup_lora()
          
          # Optimizer & Scheduler
          self.optimizer = self._create_optimizer()
          self.scheduler = self._create_scheduler()
          
          # Data loaders
          self.train_loader, self.val_loader = self._create_data_loaders()
          
          # Logging
          self.writer = SummaryWriter(self.config.logging.tensorboard_dir)
      
      def _load_model(self):
          """Load XTTS-v2 base model."""
          config = XttsConfig()
          config.load_json(self.config.model.checkpoint + "/config.json")
          
          model = Xtts.init_from_config(config)
          model.load_checkpoint(
              config,
              checkpoint_path=self.config.model.checkpoint,
              eval=False
          )
          model.to(self.device)
          return model
      
      def _setup_lora(self):
          """Configure LoRA layers."""
          from peft import LoraConfig, get_peft_model
          
          lora_config = LoraConfig(
              r=self.config.model.lora_rank,
              lora_alpha=self.config.model.lora_alpha,
              lora_dropout=self.config.model.lora_dropout,
              target_modules=["q_proj", "v_proj"],  # XTTS attention layers
              bias="none"
          )
          self.model = get_peft_model(self.model, lora_config)
          print(f"LoRA enabled: {self.model.print_trainable_parameters()}")
      
      def train_epoch(self, epoch: int):
          """Train one epoch."""
          self.model.train()
          total_loss = 0
          
          for batch_idx, batch in enumerate(self.train_loader):
              # Move to device
              text_input = batch["text_input"].to(self.device)
              audio_target = batch["audio"].to(self.device)
              
              # Forward pass
              loss = self.model(text_input, audio_target)
              
              # Backward pass
              loss = loss / self.config.training.gradient_accumulation_steps
              loss.backward()
              
              # Optimizer step
              if (batch_idx + 1) % self.config.training.gradient_accumulation_steps == 0:
                  torch.nn.utils.clip_grad_norm_(
                      self.model.parameters(),
                      self.config.training.gradient_clip_norm
                  )
                  self.optimizer.step()
                  self.scheduler.step()
                  self.optimizer.zero_grad()
              
              total_loss += loss.item()
              
              # Logging
              if batch_idx % self.config.logging.log_every_n_steps == 0:
                  self.writer.add_scalar("Loss/train", loss.item(), epoch * len(self.train_loader) + batch_idx)
          
          return total_loss / len(self.train_loader)
      
      def validate(self, epoch: int):
          """Validate on val set."""
          self.model.eval()
          total_loss = 0
          
          with torch.no_grad():
              for batch in self.val_loader:
                  text_input = batch["text_input"].to(self.device)
                  audio_target = batch["audio"].to(self.device)
                  loss = self.model(text_input, audio_target)
                  total_loss += loss.item()
          
          avg_loss = total_loss / len(self.val_loader)
          self.writer.add_scalar("Loss/val", avg_loss, epoch)
          return avg_loss
      
      def generate_samples(self, epoch: int):
          """Generate audio samples for validation."""
          self.model.eval()
          sample_texts = [
              "OlÃ¡, este Ã© um teste de sÃ­ntese de voz.",
              "O fine-tuning estÃ¡ funcionando corretamente.",
              "PortuguÃªs brasileiro com XTTS versÃ£o dois."
          ]
          
          for idx, text in enumerate(sample_texts):
              audio = self.model.inference(text, language="pt")
              self.writer.add_audio(
                  f"Sample_{idx}",
                  audio,
                  epoch,
                  sample_rate=22050
              )
              # Save to disk
              output_path = self.config.logging.tensorboard_dir / f"epoch_{epoch}_sample_{idx}.wav"
              save_wav(audio, output_path, 22050)
      
      def train(self):
          """Main training loop."""
          best_val_loss = float('inf')
          patience_counter = 0
          
          for epoch in range(self.config.training.epochs):
              print(f"\nEpoch {epoch+1}/{self.config.training.epochs}")
              
              # Train
              train_loss = self.train_epoch(epoch)
              print(f"Train Loss: {train_loss:.4f}")
              
              # Validate
              val_loss = self.validate(epoch)
              print(f"Val Loss: {val_loss:.4f}")
              
              # Generate samples
              if epoch % self.config.logging.sample_every_n_epochs == 0:
                  self.generate_samples(epoch)
              
              # Save checkpoint
              if epoch % self.config.training.save_every_n_epochs == 0:
                  self.save_checkpoint(epoch, val_loss)
              
              # Early stopping
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  patience_counter = 0
                  self.save_checkpoint(epoch, val_loss, is_best=True)
              else:
                  patience_counter += 1
                  if patience_counter >= self.config.training.early_stopping_patience:
                      print(f"Early stopping at epoch {epoch}")
                      break
          
          print("âœ… Training complete!")
      
      def save_checkpoint(self, epoch: int, val_loss: float, is_best: bool = False):
          """Save model checkpoint."""
          checkpoint_dir = Path("train/output/checkpoints")
          checkpoint_dir.mkdir(parents=True, exist_ok=True)
          
          checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
          torch.save({
              'epoch': epoch,
              'model_state_dict': self.model.state_dict(),
              'optimizer_state_dict': self.optimizer.state_dict(),
              'val_loss': val_loss,
              'config': self.config
          }, checkpoint_path)
          
          if is_best:
              best_path = checkpoint_dir / "best_model.pth"
              shutil.copy(checkpoint_path, best_path)
              print(f"âœ… Best model saved: {best_path}")
  
  
  if __name__ == "__main__":
      trainer = XTTSTrainer("train/config/train_config.yaml")
      trainer.train()
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Carrega modelo XTTS-v2 base
- âœ… Suporta LoRA e full fine-tune
- âœ… Training loop funcional com val loss
- âœ… Checkpoints salvos a cada N epochs
- âœ… TensorBoard logging
- âœ… Audio samples gerados para validaÃ§Ã£o manual
- âœ… Early stopping implementado

**Riscos**:
- ðŸ”´ **ALTO**: XTTS API pode mudar (pinned version)
- ðŸŸ¡ **MÃ‰DIO**: OOM se batch_size muito grande
- ðŸŸ¡ **MÃ‰DIO**: ConvergÃªncia pode ser lenta (ajustar LR)

---

#### 2.4 Testes de Treinamento
- [ ] Criar `tests/test_xtts_train.py`:
  ```python
  def test_load_model():
      """Test XTTS-v2 model loads correctly."""
      
  def test_lora_setup():
      """Test LoRA layers are added correctly."""
      
  def test_forward_pass():
      """Test forward pass with dummy data."""
      
  def test_checkpoint_save_load():
      """Test checkpoint save/load cycle."""
  ```
- [ ] Teste de smoke (1 epoch com mini-dataset)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Testes passam sem erros
- âœ… Smoke test completa em <5min

**Riscos**:
- ðŸŸ¢ **BAIXO**: Testes unitÃ¡rios sÃ£o isolados

---

### Sprint 2 - Deliverables

- [x] `train_config.yaml` completo
- [x] Modelo XTTS-v2 base baixado
- [x] `xtts_train.py` implementado e testado
- [x] Primeiro fine-tune rodado (smoke test)
- [x] Checkpoints salvos em `train/output/checkpoints/`
- [x] Audio samples em `train/output/samples/`
- [x] TensorBoard logs funcionando

---

## ðŸŽ¤ Sprint 3: IntegraÃ§Ã£o API + InferÃªncia

**Objetivo**: Integrar modelo fine-tunado na API existente e criar endpoints de inferÃªncia.

**DuraÃ§Ã£o**: 3-4 horas  
**Prioridade**: P1 (IMPORTANTE)

### Tasks

#### 3.1 Criar `train/scripts/xtts_inference.py`
- [ ] Wrapper de inferÃªncia:
  ```python
  # train/scripts/xtts_inference.py
  
  class XTTSInference:
      def __init__(self, checkpoint_path: Path = None):
          """
          Load XTTS-v2 model for inference.
          
          Args:
              checkpoint_path: Path to fine-tuned checkpoint.
                               If None, uses base model.
          """
          if checkpoint_path and checkpoint_path.exists():
              self.model = self._load_finetuned(checkpoint_path)
              logger.info(f"Loaded fine-tuned model: {checkpoint_path}")
          else:
              self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
              logger.info("Loaded base XTTS-v2 model")
          
          self.model.to("cuda" if torch.cuda.is_available() else "cpu")
      
      def synthesize(
          self,
          text: str,
          language: str = "pt",
          speaker_wav: Path = None,
          speed: float = 1.0
      ) -> np.ndarray:
          """
          Generate speech from text.
          
          Args:
              text: Input text
              language: Language code (pt, en, es, etc)
              speaker_wav: Reference audio for voice cloning
              speed: Speech speed multiplier
          
          Returns:
              Audio array (22050 Hz mono)
          """
          if speaker_wav:
              audio = self.model.tts_to_file(
                  text=text,
                  speaker_wav=str(speaker_wav),
                  language=language,
                  speed=speed,
                  file_path=None  # return array
              )
          else:
              audio = self.model.tts(text=text, language=language)
          
          return audio
      
      def clone_voice(
          self,
          text: str,
          reference_wav: Path,
          language: str = "pt"
      ) -> np.ndarray:
          """
          Clone voice from reference audio.
          
          Args:
              text: Text to synthesize
              reference_wav: Path to reference audio (3-10s)
              language: Target language
          
          Returns:
              Cloned audio (22050 Hz mono)
          """
          return self.synthesize(
              text=text,
              speaker_wav=reference_wav,
              language=language
          )
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Carrega modelo base ou fine-tunado
- âœ… SÃ­ntese de texto funciona
- âœ… Voice cloning funciona
- âœ… Retorna audio em 22050 Hz mono

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Checkpoint loading pode falhar (validar formato)

---

#### 3.2 Modificar `app/engines/xtts_engine.py`
- [ ] Adicionar suporte a checkpoint custom:
  ```python
  # app/engines/xtts_engine.py
  
  class XTTSEngine(TTSEngine):
      def __init__(self, config: Dict[str, Any]):
          super().__init__(config)
          
          # Check for custom checkpoint
          custom_checkpoint = os.getenv("XTTS_CUSTOM_CHECKPOINT")
          if custom_checkpoint and Path(custom_checkpoint).exists():
              logger.info(f"Loading custom XTTS checkpoint: {custom_checkpoint}")
              self.model = self._load_custom_checkpoint(custom_checkpoint)
          else:
              logger.info("Loading base XTTS-v2 model")
              self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
      
      def _load_custom_checkpoint(self, checkpoint_path: str):
          """Load fine-tuned checkpoint."""
          # Implementation similar to xtts_inference.py
          ...
  ```
- [ ] Atualizar `.env.example`:
  ```bash
  # XTTS Configuration
  XTTS_CUSTOM_CHECKPOINT=/app/train/output/checkpoints/best_model.pth
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Suporta modelo base e fine-tunado
- âœ… Env var funciona
- âœ… Fallback para base se checkpoint nÃ£o existir

**Riscos**:
- ðŸŸ¢ **BAIXO**: LÃ³gica simples

---

#### 3.3 Criar Endpoint SÃ­ncrono `/tts/synthesize`
- [ ] Adicionar em `app/main.py`:
  ```python
  @app.post("/tts/synthesize", response_class=StreamingResponse)
  async def synthesize_tts(
      text: str = Form(...),
      language: str = Form("pt-BR"),
      reference_audio: UploadFile = File(None),
      speed: float = Form(1.0)
  ):
      """
      SÃ­ntese de voz sÃ­ncrona (retorna WAV imediatamente).
      
      Args:
          text: Texto a ser sintetizado
          language: CÃ³digo do idioma (pt-BR, en-US, etc)
          reference_audio: (Opcional) Ãudio de referÃªncia para clonagem
          speed: Velocidade da fala (0.5-2.0)
      
      Returns:
          Audio WAV (22050 Hz mono)
      """
      try:
          # Get engine
          engine = engine_factory.get_engine("xtts")
          
          # Save reference audio if provided
          speaker_wav = None
          if reference_audio:
              speaker_wav = Path(f"/tmp/ref_{uuid.uuid4()}.wav")
              with open(speaker_wav, "wb") as f:
                  f.write(await reference_audio.read())
          
          # Synthesize
          audio = engine.synthesize(
              text=text,
              language=language.split("-")[0],  # pt-BR -> pt
              speaker_wav=speaker_wav,
              speed=speed
          )
          
          # Convert to WAV bytes
          wav_bytes = audio_array_to_wav(audio, sample_rate=22050)
          
          # Cleanup
          if speaker_wav and speaker_wav.exists():
              speaker_wav.unlink()
          
          return StreamingResponse(
              io.BytesIO(wav_bytes),
              media_type="audio/wav",
              headers={
                  "Content-Disposition": f"attachment; filename=synthesized_{int(time.time())}.wav"
              }
          )
      
      except Exception as e:
          logger.exception("Synthesis failed")
          raise HTTPException(status_code=500, detail=str(e))
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Endpoint `/tts/synthesize` funciona
- âœ… Aceita texto + optional reference audio
- âœ… Retorna WAV diretamente
- âœ… Tratamento de erros

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Requests grandes podem timeout (limitar texto)

---

#### 3.4 Documentar API
- [ ] Atualizar `docs/api-reference.md`:
  ```markdown
  ## POST /tts/synthesize
  
  SÃ­ntese de voz sÃ­ncrona usando XTTS-v2.
  
  ### Request
  
  **Form Data:**
  - `text` (required): Texto a sintetizar (max 500 chars)
  - `language` (optional): CÃ³digo idioma (default: pt-BR)
  - `reference_audio` (optional): Arquivo WAV para clonagem
  - `speed` (optional): Velocidade (0.5-2.0, default: 1.0)
  
  ### Example
  
  ```bash
  # Simple TTS
  curl -X POST http://localhost:8005/tts/synthesize \
    -F "text=OlÃ¡, este Ã© um teste" \
    -F "language=pt-BR" \
    -o output.wav
  
  # Voice cloning
  curl -X POST http://localhost:8005/tts/synthesize \
    -F "text=Texto com voz clonada" \
    -F "reference_audio=@reference.wav" \
    -o cloned.wav
  ```
  
  ### Response
  
  - Status: 200 OK
  - Content-Type: audio/wav
  - Body: WAV file (22050 Hz mono)
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Docs claras com exemplos curl
- âœ… Swagger UI atualizado (automÃ¡tico via FastAPI)

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas documentaÃ§Ã£o

---

### Sprint 3 - Deliverables

- [x] `xtts_inference.py` implementado
- [x] `xtts_engine.py` suporta checkpoint custom
- [x] Endpoint `/tts/synthesize` funcionando
- [x] API documentada
- [x] Testes manuais com curl
- [x] README atualizado com link para Swagger

---

## âœ… Sprint 4: Qualidade & Testes

**Objetivo**: Garantir qualidade de cÃ³digo e cobertura de testes.

**DuraÃ§Ã£o**: 4-5 horas  
**Prioridade**: P1 (IMPORTANTE)

### Tasks

#### 4.1 Testes do Pipeline de Dados
- [ ] `tests/test_download_youtube.py`
- [ ] `tests/test_segment_audio.py`
- [ ] `tests/test_transcribe.py`
- [ ] `tests/test_build_metadata.py`
- [ ] `tests/test_pipeline_integration.py` (end-to-end)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Coverage > 80% nos scripts de pipeline
- âœ… Testes passam em CI

---

#### 4.2 Testes de Treinamento
- [ ] `tests/test_xtts_train.py`
- [ ] Smoke test (1 epoch, mini-dataset)

---

#### 4.3 Testes de API
- [ ] `tests/test_api_synthesize.py`
- [ ] `tests/test_voice_cloning_endpoint.py`
- [ ] `tests/test_custom_checkpoint_loading.py`

---

#### 4.4 Configurar Linting
- [ ] Adicionar pre-commit:
  ```yaml
  # .pre-commit-config.yaml
  repos:
    - repo: https://github.com/psf/black
      rev: 24.3.0
      hooks:
        - id: black
    
    - repo: https://github.com/pycqa/isort
      rev: 5.13.2
      hooks:
        - id: isort
    
    - repo: https://github.com/pre-commit/pre-commit-hooks
      rev: v4.5.0
      hooks:
        - id: trailing-whitespace
        - id: end-of-file-fixer
        - id: check-yaml
  ```
- [ ] Rodar em todo o cÃ³digo:
  ```bash
  black .
  isort .
  ```

---

#### 4.5 Type Hints
- [ ] Adicionar hints em todos os scripts de `train/`
- [ ] Configurar mypy:
  ```ini
  [mypy]
  python_version = 3.11
  warn_return_any = True
  warn_unused_configs = True
  disallow_untyped_defs = True
  ```

---

### Sprint 4 - Deliverables

- [x] Cobertura de testes > 80%
- [x] Linting configurado (black, isort)
- [x] Type hints completos
- [x] CI pipeline bÃ¡sico (GitHub Actions)

---

## ðŸ“š Sprint 5: Docs & DevOps

**Objetivo**: DocumentaÃ§Ã£o completa e melhorias de DevOps.

**DuraÃ§Ã£o**: 2-3 horas  
**Prioridade**: P2 (NICE TO HAVE)

### Tasks

#### 5.1 Criar `ENV_SETUP.md`
- [ ] Guia de setup completo:
  - InstalaÃ§Ã£o de dependÃªncias
  - CriaÃ§Ã£o de venv
  - ConfiguraÃ§Ã£o VSCode
  - Download de modelos

---

#### 5.2 Criar `CONTRIBUTING.md`
- [ ] Guidelines para contribuidores
- [ ] Code style
- [ ] Commit conventions

---

#### 5.3 Atualizar README.md
- [ ] SeÃ§Ã£o "Quick Start" atualizada
- [ ] Link para Swagger docs
- [ ] Badges (build status, coverage)

---

#### 5.4 Criar Diagramas
- [ ] Pipeline de dados (Mermaid)
- [ ] Arquitetura do sistema
- [ ] Fluxo de treinamento

---

#### 5.5 GitHub Actions CI
- [ ] Workflow bÃ¡sico:
  ```yaml
  # .github/workflows/ci.yml
  name: CI
  
  on: [push, pull_request]
  
  jobs:
    test:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v3
        - uses: actions/setup-python@v4
          with:
            python-version: '3.11'
        - run: pip install -r requirements.txt -r requirements-dev.txt
        - run: pytest
        - run: black --check .
        - run: mypy .
  ```

---

### Sprint 5 - Deliverables

- [x] `ENV_SETUP.md` completo
- [x] `CONTRIBUTING.md` criado
- [x] README atualizado com quick start
- [x] Diagramas de arquitetura
- [x] CI pipeline funcionando

---

## ðŸ“ˆ MÃ©tricas de Sucesso

### Sprint 1
- [ ] Dataset LJSpeech gerado (mÃ­n 1h de Ã¡udio pt-BR)
- [ ] Pipeline roda end-to-end sem erros
- [ ] Tempo de processamento < 2h para 1h de Ã¡udio

### Sprint 2
- [ ] Modelo fine-tuna sem OOM
- [ ] Val loss converge (decresce por > 10 epochs)
- [ ] Audio samples tÃªm qualidade aceitÃ¡vel (validaÃ§Ã£o manual)

### Sprint 3
- [ ] API `/tts/synthesize` responde em < 5s para texto curto
- [ ] Voice cloning funciona com latÃªncia < 10s
- [ ] Modelo custom carrega corretamente

### Sprint 4
- [ ] Coverage > 80%
- [ ] Zero warnings de linting
- [ ] Todos os testes passam

### Sprint 5
- [ ] Docs completas (README + contributing)
- [ ] CI passa em todos os PRs
- [ ] Novos devs conseguem setup em < 30min

---

## ðŸš§ Riscos Globais

### TÃ©cnicos
- **XTTS API breaking changes** â†’ Pinned version (coqui-tts==0.27.0)
- **OOM durante treino** â†’ LoRA + gradient accumulation
- **Whisper lento** â†’ Usar GPU, modelo small
- **Dataset pequeno** â†’ ComeÃ§ar com 1-2h, expandir depois

### Processo
- **Sprints muito longas** â†’ Dividir tasks maiores
- **Falta de validaÃ§Ã£o manual** â†’ Audio samples a cada epoch
- **DocumentaÃ§Ã£o desatualizada** â†’ Update docs em cada sprint

---

## ðŸ“ Notas Finais

- **Priorizar Sprint 0 e 1** antes de qualquer outra coisa
- **Commits frequentes** (atomic commits por task)
- **Testes manuais** antes de marcar task como done
- **Documentar decisÃµes tÃ©cnicas** em comentÃ¡rios/docstrings

**PrÃ³xima aÃ§Ã£o**: Executar **Sprint 0 - Task 1** (auditoria de secrets).

---

**Ãšltima atualizaÃ§Ã£o**: 2025-12-06  
**Mantido por**: Tech Lead (Claude Sonnet 4.5)
