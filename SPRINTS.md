# ðŸš€ Plano de Sprints - TTS WebUI XTTS-v2

**Projeto**: ImplementaÃ§Ã£o completa de pipeline fine-tuning XTTS-v2  
**Baseado em**: [MORE.md](./MORE.md)  
**Tech Lead**: Claude Sonnet 4.5  
**Data inÃ­cio**: 2025-12-06

---

## ðŸ“Š VisÃ£o Geral

| Sprint | Foco | DuraÃ§Ã£o | Status |
|--------|------|---------|--------|
| Sprint 0 | SeguranÃ§a & Cleanup | 1-2h | ðŸ”„ Pronto para iniciar |
| Sprint 1 | Estrutura `train/` + Pipeline Dados | 4-6h | â³ Planejada |
| Sprint 2 | Treinamento XTTS-v2 | 6-8h | â³ Planejada |
| Sprint 3 | IntegraÃ§Ã£o API + InferÃªncia | 3-4h | â³ Planejada |
| Sprint 4 | Qualidade & Testes | 4-5h | â³ Planejada |
| Sprint 5 | Docs & DevOps | 2-3h | â³ Planejada |

**Total estimado**: 20-28 horas de desenvolvimento

---

## ðŸ”’ Sprint 0: SeguranÃ§a & Cleanup (CRÃTICO)

**Objetivo**: Garantir seguranÃ§a do repositÃ³rio e limpar referÃªncias obsoletas.

**DuraÃ§Ã£o**: 1-2 horas  
**Prioridade**: P0 (CRÃTICO)

### Tasks

#### 1. Auditoria de Secrets
- [ ] Verificar se `.env` estÃ¡ no `.gitignore`
  ```bash
  grep -E "^\.env$" .gitignore
  ```
- [ ] Inspecionar histÃ³rico Git por secrets commitados
  ```bash
  git log --all --full-history --source --pickaxe-regex -S "API_KEY|SECRET|PASSWORD" -- .env
  ```
- [ ] Se encontrar secrets expostos:
  - Rotacionar todas as chaves imediatamente
  - Usar `git filter-branch` ou BFG Repo-Cleaner para remover histÃ³rico
  - Documentar em `SECURITY_INCIDENT.md`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `.env` nÃ£o estÃ¡ commitado no repositÃ³rio
- âœ… `.env.example` criado sem valores sensÃ­veis
- âœ… Nenhum secret no histÃ³rico Git

**Riscos**:
- ðŸ”´ **ALTO**: Secrets expostos podem comprometer APIs externas
- ðŸŸ¡ **MÃ‰DIO**: Reescrever histÃ³rico Git pode afetar colaboradores

---

#### 2. Limpar Docs Obsoletas de F5-TTS
- [ ] Atualizar `docs/LOW_VRAM.md`:
  - Adicionar nota no topo: "âš ï¸ DEPRECATED: F5-TTS removed in v2.0"
  - Marcar seÃ§Ãµes F5-TTS como obsoletas
- [ ] Atualizar `docs/QUALITY_PROFILES.md`:
  - Remover seÃ§Ã£o "Perfis PadrÃ£o F5-TTS"
  - Atualizar exemplos para XTTS apenas
- [ ] Atualizar `docs/CHANGELOG.md`:
  - Adicionar entrada para v2.0 destacando remoÃ§Ã£o F5-TTS
  - Link para `F5_TTS_REMOVED.md`

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Nenhuma documentaÃ§Ã£o sugere que F5-TTS estÃ¡ funcional
- âœ… Todas as referÃªncias marcadas como deprecated ou removidas
- âœ… Changelog atualizado com v2.0

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas documentaÃ§Ã£o, sem risco tÃ©cnico

---

#### 3. Renomear `scripts/not_remove/`
- [ ] Mover para estrutura mais clara:
  ```bash
  mkdir -p scripts/dataset
  mv scripts/not_remove/* scripts/dataset/
  rmdir scripts/not_remove
  ```
- [ ] Atualizar imports se algum script referencia:
  ```python
  # Antes
  from scripts.not_remove.download_youtube import download
  # Depois
  from scripts.dataset.download_youtube import download
  ```
- [ ] Atualizar README com nova estrutura

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `scripts/not_remove/` nÃ£o existe mais
- âœ… `scripts/dataset/` contÃ©m todos os scripts de preparaÃ§Ã£o de dados
- âœ… Nenhum import quebrado

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Imports podem quebrar se nÃ£o testar bem

---

## ðŸ—ï¸ Sprint 1: Estrutura `train/` + Pipeline de Dados

**Objetivo**: Adaptar scripts existentes em `scripts/not_remove/` para estrutura `train/` com foco em XTTS-v2 (formato LJSpeech).

**DuraÃ§Ã£o**: 4-6 horas  
**Prioridade**: P0 (CRÃTICO - bloqueia Sprint 2)

**ATUALIZAÃ‡ÃƒO**: Scripts jÃ¡ existem em `scripts/not_remove/`! Pipeline completo:
1. `download_youtube.py` â†’ Baixa de `videos.csv` para `raw/`
2. `prepare_segments_optimized.py` â†’ VAD + segmentaÃ§Ã£o â†’ `processed/wavs/`
3. `transcribe_or_subtitles.py` â†’ Whisper + legendas YT â†’ `transcriptions.json`
4. `build_metadata_csv.py` â†’ Gera LJSpeech `metadata.csv`

**Nova abordagem**: Migrar e adaptar para XTTS-v2 (nÃ£o F5-TTS)!

### Tasks

#### 1.1 Criar Estrutura de DiretÃ³rios
- [ ] Criar Ã¡rvore de pastas:
  ```bash
  mkdir -p train/{config,data/{raw,processed},scripts,output/{checkpoints,samples},logs}
  mkdir -p train/data/MyTTSDataset/wavs  # LJSpeech format
  ```
- [ ] Adicionar `.gitkeep` em pastas vazias
- [ ] Criar `.gitignore` para `train/data/raw/*` e `train/output/*` (nÃ£o commitar datasets/checkpoints grandes)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Estrutura completa criada
- âœ… Git ignora arquivos grandes de dados/modelos
- âœ… README em `train/README.md` explicando estrutura

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas criaÃ§Ã£o de pastas

---

#### 1.2 Criar `train/config/dataset_config.yaml`
- [ ] Implementar configuraÃ§Ã£o adaptada para XTTS-v2:
  ```yaml
  # Dataset preparation config for XTTS-v2
  audio:
    target_sample_rate: 22050  # XTTS-v2 requirement (nÃ£o 24000!)
    channels: 1                # mono
    bit_depth: 16
    format: wav
  
  youtube:
    audio_format: "bestaudio/best"
    max_retries: 3
    retry_delay: 5
    subtitles:
      languages: ["pt", "pt-BR"]
      format: "vtt"
  
  segmentation:
    use_vad: true
    vad_threshold: -40.0        # dB, voice activity detection
    vad_frame_size: 512
    vad_chunk_duration: 10.0    # streaming chunks
    min_silence_duration: 0.3   # seconds to split
    
    min_duration: 7.0           # XTTS-v2: 7-12s ideal range!
    max_duration: 12.0
    target_duration: 10.0
    segment_overlap: 0.5        # overlap between segments
    
    fade_duration: 0.05         # fade in/out to avoid clicks
    normalization_method: "rms" # or "loudnorm"
    target_rms_db: -20.0
  
  transcription:
    whisper_model: "base"       # base, small, medium
    whisper_hp_model: "medium"  # high-precision fallback
    language: "pt"
    temperature: 0.0            # deterministic
    oov_threshold: 0.15         # out-of-vocab trigger for HP model
  
  text_processing:
    expand_numbers: true        # "123" â†’ "cento e vinte e trÃªs"
    lowercase: true             # XTTS works better with lowercase
    normalize_whitespace: true
    remove_extra_punctuation: false
  
  quality_filters:
    min_words: 3
    max_words: 50
    # Adicionar filtros especÃ­ficos se necessÃ¡rio
  
  dataset:
    train_split: 0.9
    val_split: 0.1
    shuffle: true
    seed: 42
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… YAML vÃ¡lido com valores XTTS-v2 corretos (22050Hz, 7-12s)
- âœ… ConfiguraÃ§Ã£o de VAD streaming (otimizado para memÃ³ria)
- âœ… ParÃ¢metros de transcriÃ§Ã£o (Whisper + fallback HP)
- âœ… NormalizaÃ§Ã£o de texto para pt-BR

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas arquivo de configuraÃ§Ã£o

---

#### 1.3 Migrar e Adaptar `download_youtube.py`
- [ ] Copiar `scripts/not_remove/download_youtube.py` â†’ `train/scripts/download_youtube.py`
- [ ] **MANTER**: Leitura de `videos.csv` (formato jÃ¡ funcional)
- [ ] **AJUSTAR**: Sample rate de 24000 â†’ **22050Hz** (XTTS-v2)
  ```python
  # Antes
  "-ar", "24000",
  # Depois
  "-ar", str(config["audio"]["target_sample_rate"]),  # 22050
  ```
- [ ] **AJUSTAR**: Paths para `train/data/`:
  ```python
  data_dir = project_root / "train" / "data"
  videos_csv = data_dir / "videos.csv"
  raw_dir = data_dir / "raw"
  ```
- [ ] **MANTER**: Retry logic, logging, yt-dlp options

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… LÃª `train/data/videos.csv` (copiar de `scripts/not_remove/videos.csv`)
- âœ… Download para 22050Hz mono 16-bit
- âœ… Salva em `train/data/raw/video_XXXXX.wav`
- âœ… Logging em `train/logs/download_youtube.log`

**Riscos**:
- ðŸŸ¢ **BAIXO**: Script jÃ¡ funcional, sÃ³ ajustar paths
- ðŸŸ¡ **MÃ‰DIO**: yt-dlp pode quebrar com updates do YouTube

---

#### 1.4 Migrar e Adaptar `prepare_segments_optimized.py`
- [ ] Copiar `scripts/not_remove/prepare_segments_optimized.py` â†’ `train/scripts/segment_audio.py`
- [ ] **MANTER**: Streaming VAD (jÃ¡ otimizado para memÃ³ria!)
  - `iter_voice_regions()` - detecta fala em chunks de 10s
  - `iter_final_segments_from_regions()` - gera segmentos finais
- [ ] **AJUSTAR**: DuraÃ§Ã£o para XTTS-v2 (7-12s, nÃ£o 3-30s)
  ```python
  # Antes (F5-TTS)
  min_duration = 3.0
  max_duration = 30.0
  # Depois (XTTS-v2)
  min_duration = 7.0
  max_duration = 12.0
  target_duration = 10.0
  ```
- [ ] **AJUSTAR**: Sample rate 24000 â†’ 22050Hz
  ```python
  target_sr = config["audio"]["target_sample_rate"]  # 22050
  ```
- [ ] **AJUSTAR**: Paths para `train/data/`:
  ```python
  raw_dir = project_root / "train" / "data" / "raw"
  processed_dir = project_root / "train" / "data" / "processed"
  ```
- [ ] **MANTER**: 
  - Fade in/out para evitar clicks
  - NormalizaÃ§Ã£o RMS ou pyloudnorm
  - Resample usando scipy.signal
  - Logging detalhado

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Processa arquivos grandes (>1GB) sem OOM
- âœ… Segmentos entre 7-12s (target 10s) - XTTS-v2 ideal
- âœ… 22050Hz mono 16-bit preservado
- âœ… VAD remove silÃªncios longos

**Riscos**:
- ðŸŸ¢ **BAIXO**: Script jÃ¡ testado, sÃ³ ajustar parÃ¢metros

---
---

#### 1.5 Migrar e Adaptar `transcribe_or_subtitles.py`
- [ ] Copiar `scripts/not_remove/transcribe_or_subtitles.py` â†’ `train/scripts/transcribe_audio.py`
- [ ] **MANTER**: LÃ³gica completa jÃ¡ implementada!
  - Tenta baixar legendas do YouTube (yt-dlp)
  - Se nÃ£o houver, usa Whisper
  - Cache de modelos Whisper (evita reload)
  - Fallback para modelo HP se OOV alto (>15%)
  - NormalizaÃ§Ã£o pt-BR completa:
    - `num2words` para nÃºmeros
    - Lowercase
    - Whitespace normalization
    - RemoÃ§Ã£o de pontuaÃ§Ã£o extra
  - VocabulÃ¡rio pt-BR embutido para detectar OOV
- [ ] **AJUSTAR**: Paths para `train/data/`:
  ```python
  data_dir = project_root / "train" / "data"
  videos_csv = data_dir / "videos.csv"
  processed_dir = data_dir / "processed"
  transcriptions_file = processed_dir / "transcriptions.json"
  ```
- [ ] **AJUSTAR**: Carregar config de `dataset_config.yaml`:
  ```python
  whisper_model = config["transcription"]["whisper_model"]
  whisper_hp_model = config["transcription"]["whisper_hp_model"]
  oov_threshold = config["transcription"]["oov_threshold"]
  ```
- [ ] **MANTER**: 
  - Rate limit handling (HTTP 429)
  - Retry logic
  - Logging detalhado

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Prioriza legendas do YouTube (mais rÃ¡pido, exato)
- âœ… Fallback para Whisper funcional
- âœ… NormalizaÃ§Ã£o pt-BR completa (nÃºmeros expandidos)
- âœ… Salva `train/data/processed/transcriptions.json`
---

#### 1.6 Migrar e Adaptar `build_metadata_csv.py`
- [ ] Copiar `scripts/not_remove/build_metadata_csv.py` â†’ `train/scripts/build_ljs_dataset.py`
- [ ] **REMOVER**: ReferÃªncias a F5-TTS (comentÃ¡rios, paths `f5_dataset/`)
- [ ] **AJUSTAR**: Formato metadata.csv para XTTS-v2 (compatÃ­vel com LJSpeech):
  ```
  # F5-TTS (ANTIGO)
  wavs/audio_0001.wav|texto em portuguÃªs aqui
  
  # XTTS-v2 / LJSpeech (NOVO - mantÃ©m compatibilidade!)
  wavs/audio_00001.wav|texto em portuguÃªs aqui
  ```
  - **NOTA**: XTTS aceita formato F5 (1 coluna), mas pode expandir para 2 se quiser
- [ ] **AJUSTAR**: Paths para `train/data/`:
  ```python
  data_dir = project_root / "train" / "data"
  processed_dir = data_dir / "processed"
  dataset_dir = data_dir / "MyTTSDataset"  # nÃ£o f5_dataset!
  wavs_dir = dataset_dir / "wavs"
  ```
- [ ] **MANTER**:
  - Copia WAVs de `processed/` para `MyTTSDataset/wavs/`
  - Gera `metadata.csv` com formato `relative_path|text`
  - Logging detalhado
  - EstatÃ­sticas de duraÃ§Ã£o
- [ ] **ADICIONAR** (opcional): Filtros de qualidade
  ```python
  # Filtrar por duraÃ§Ã£o (7-12s)
  if not (7.0 <= item["duration"] <= 12.0):
      logger.warning(f"DuraÃ§Ã£o fora do range: {item['duration']:.1f}s")
      continue
  
  # Filtrar por palavras (3-50)
  word_count = len(item["text"].split())
  if not (3 <= word_count <= 50):
      logger.warning(f"Palavras fora do range: {word_count}")
      continue
  ```
- [ ] **ADICIONAR**: Train/val split
  ```python
  # Shuffle e split
  random.seed(config["dataset"]["seed"])
  random.shuffle(metadata_lines)
  
  split_idx = int(len(metadata_lines) * config["dataset"]["train_split"])
  train_lines = metadata_lines[:split_idx]
  val_lines = metadata_lines[split_idx:]
  
  # Salvar splits
  (dataset_dir / "metadata_train.csv").write_text("\n".join(train_lines))
  (dataset_dir / "metadata_val.csv").write_text("\n".join(val_lines))
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… `metadata.csv` em formato LJSpeech (compatÃ­vel XTTS-v2)
- âœ… WAVs organizados em `MyTTSDataset/wavs/`
---

#### 1.7 Criar `pipeline.py` (Orquestrador)
- [ ] Criar script master para executar pipeline completo:
  ```python
  # train/scripts/pipeline.py
  
  """
  Pipeline completo de preparaÃ§Ã£o de dataset XTTS-v2
  
  Uso:
      python -m train.scripts.pipeline
      python -m train.scripts.pipeline --skip-download
      python -m train.scripts.pipeline --only-step transcribe
  """
  
  import click
  from pathlib import Path
  import yaml
  
  @click.command()
  @click.option('--config', type=Path, default='train/config/dataset_config.yaml')
  @click.option('--skip-download', is_flag=True, help='Pular download (usar raw/ existente)')
  @click.option('--skip-segment', is_flag=True, help='Pular segmentaÃ§Ã£o')
  @click.option('--skip-transcribe', is_flag=True, help='Pular transcriÃ§Ã£o')
  @click.option('--only-step', type=click.Choice(['download', 'segment', 'transcribe', 'build']), 
                help='Executar apenas um step')
  def run_pipeline(config, skip_download, skip_segment, skip_transcribe, only_step):
      """
      Executa pipeline completo de preparaÃ§Ã£o de dataset:
      
      1. download_youtube.py   â†’ raw/
      2. segment_audio.py      â†’ processed/wavs/
      3. transcribe_audio.py   â†’ processed/transcriptions.json
      4. build_ljs_dataset.py  â†’ MyTTSDataset/metadata.csv
      """
      cfg = load_config(config)', is_flag=True)
  @click.option('--skip-transcribe', is_flag=True)
  def run_pipeline(config, sources, skip_download, skip_segment, skip_transcribe):
      """
      Run complete data pipeline:
      1. Download YouTube videos
      2. Segment audio with VAD
      3. Transcribe with Whisper
      4. Build LJSpeech dataset
      """
      cfg = load_config(config)
      
      if not skip_download:
          logger.info("Step 1: Downloading videos...")
          download_youtube_batch(sources, cfg)
      
      if not skip_segment:
          logger.info("Step 2: Segmenting audio...")
          segment_all_audio(cfg)
      
      if not skip_transcribe:
          logger.info("Step 3: Transcribing...")
          transcribe_all_segments(cfg)
      
      logger.info("Step 4: Building LJSpeech dataset...")
      build_ljspeech_dataset(cfg)
      
      logger.info("âœ… Pipeline complete!")
  ```
- [ ] Logging estruturado (rich progress bars)
- [ ] Tratamento de erros global
- [ ] Checkpoint/resume (se pipeline falhar no meio)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Pipeline roda de ponta a ponta sem intervenÃ§Ã£o
- âœ… Flags para pular etapas jÃ¡ completadas
- âœ… Progress bars claros
- âœ… Logs salvos em `train/logs/pipeline_{timestamp}.log`

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Pipeline pode falhar no meio (precisa ser resiliente)
- ðŸŸ¢ **BAIXO**: LÃ³gica simples se scripts individuais estÃ£o OK

---

### Sprint 1 - Deliverables

- [x] Estrutura `train/` completa
- [x] `dataset_config.yaml` bem documentado
- [x] Scripts refatorados e integrados:
  - `download_youtube.py`
  - `segment_audio.py`
  - `transcribe_whisper.py`
  - `build_ljs_dataset.py`
  - `pipeline.py`
- [x] Dataset LJSpeech de teste gerado (1-2h de Ã¡udio pt-BR)
- [x] DocumentaÃ§Ã£o em `train/README.md`

---

## ðŸŽ“ Sprint 2: Treinamento XTTS-v2

**Objetivo**: Implementar fine-tuning completo do XTTS-v2 com suporte a LoRA.

**DuraÃ§Ã£o**: 6-8 horas  
**Prioridade**: P0 (CRÃTICO - core feature)

### Tasks

#### 2.1 Criar `train/config/train_config.yaml`
- [ ] ConfiguraÃ§Ã£o de treinamento:
  ```yaml
  # XTTS-v2 Fine-tuning Configuration
  
  model:
    name: xtts_v2
    # Base model - auto-download via Coqui TTS
    checkpoint: tts_models/multilingual/multi-dataset/xtts_v2
    # Or custom checkpoint:
    # checkpoint: ./models/xtts_pretrained/model.pth
    
    # LoRA config (memory-efficient)
    use_lora: true
    lora_rank: 8
    lora_alpha: 16
    lora_dropout: 0.1
  
  training:
    # Hardware
    device: cuda
    mixed_precision: true  # FP16
    num_workers: 4
    
    # Optimization
    batch_size: 4          # adjust for VRAM
    gradient_accumulation_steps: 2  # effective batch = 8
    epochs: 50
    learning_rate: 1.0e-5
    warmup_steps: 100
    scheduler: cosine
    
    # Regularization
    weight_decay: 0.01
    gradient_clip_norm: 1.0
    
    # Constraints
    max_text_length: 200   # characters
    max_audio_length: 12   # seconds
    
    # Checkpointing
    save_every_n_epochs: 5
    keep_last_n_checkpoints: 3
    early_stopping_patience: 10
  
  dataset:
    path: ./train/data/MyTTSDataset
    metadata_train: metadata_train.csv
    metadata_val: metadata_val.csv
    language: pt-BR
    sample_rate: 22050
    
    # Data augmentation (optional)
    augmentation:
      enabled: false
      pitch_shift_semitones: [-2, 2]
      time_stretch_factor: [0.9, 1.1]
  
  logging:
    tensorboard_dir: ./train/output/tensorboard
    log_every_n_steps: 10
    sample_every_n_epochs: 5  # generate audio samples
    num_samples: 3
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… YAML vÃ¡lido e bem documentado
- âœ… Suporta LoRA e full fine-tune
- âœ… Valores reasonable para RTX 3090 (23GB VRAM)

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas arquivo de configuraÃ§Ã£o

---

#### 2.2 Baixar Modelo Pretrained XTTS-v2
- [ ] Script para download automÃ¡tico:
  ```python
  # train/scripts/download_pretrained.py
  
  from TTS.api import TTS
  import shutil
  
  def download_xtts_v2(output_dir: Path):
      """Download XTTS-v2 pretrained model via Coqui TTS."""
      print("Downloading XTTS-v2 base model...")
      tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
      
      # Model files sÃ£o salvos em ~/.local/share/tts/
      # Copiar para projeto para garantir versionamento
      src = Path.home() / ".local/share/tts/tts_models--multilingual--multi-dataset--xtts_v2"
      dst = output_dir / "xtts_v2_base"
      
      if dst.exists():
          print(f"Model already exists at {dst}")
          return dst
      
      shutil.copytree(src, dst)
      print(f"âœ… Model downloaded to {dst}")
      return dst
  ```
- [ ] Salvar em `models/xtts_pretrained/`
- [ ] Adicionar verificaÃ§Ã£o de integridade (checksum)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Modelo baixado automaticamente
- âœ… Arquivos copiados para `models/xtts_pretrained/`
- âœ… Checksum validado (MD5 ou SHA256)

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Download pode falhar (retry logic)
- ðŸŸ¡ **MÃ‰DIO**: Modelo grande (~2GB) pode demorar

---

#### 2.3 Implementar `xtts_train.py`
- [ ] Script de treinamento completo:
  ```python
  # train/scripts/xtts_train.py
  
  import torch
  from TTS.tts.configs.xtts_config import XttsConfig
  from TTS.tts.models.xtts import Xtts
  from TTS.tts.datasets import load_tts_samples
  from torch.utils.data import DataLoader
  from torch.utils.tensorboard import SummaryWriter
  
  class XTTSTrainer:
      def __init__(self, config_path: Path):
          self.config = load_config(config_path)
          self.device = torch.device(self.config.training.device)
          
          # Load model
          self.model = self._load_model()
          
          # Setup LoRA if enabled
          if self.config.model.use_lora:
              self._setup_lora()
          
          # Optimizer & Scheduler
          self.optimizer = self._create_optimizer()
          self.scheduler = self._create_scheduler()
          
          # Data loaders
          self.train_loader, self.val_loader = self._create_data_loaders()
          
          # Logging
          self.writer = SummaryWriter(self.config.logging.tensorboard_dir)
      
      def _load_model(self):
          """Load XTTS-v2 base model."""
          config = XttsConfig()
          config.load_json(self.config.model.checkpoint + "/config.json")
          
          model = Xtts.init_from_config(config)
          model.load_checkpoint(
              config,
              checkpoint_path=self.config.model.checkpoint,
              eval=False
          )
          model.to(self.device)
          return model
      
      def _setup_lora(self):
          """Configure LoRA layers."""
          from peft import LoraConfig, get_peft_model
          
          lora_config = LoraConfig(
              r=self.config.model.lora_rank,
              lora_alpha=self.config.model.lora_alpha,
              lora_dropout=self.config.model.lora_dropout,
              target_modules=["q_proj", "v_proj"],  # XTTS attention layers
              bias="none"
          )
          self.model = get_peft_model(self.model, lora_config)
          print(f"LoRA enabled: {self.model.print_trainable_parameters()}")
      
      def train_epoch(self, epoch: int):
          """Train one epoch."""
          self.model.train()
          total_loss = 0
          
          for batch_idx, batch in enumerate(self.train_loader):
              # Move to device
              text_input = batch["text_input"].to(self.device)
              audio_target = batch["audio"].to(self.device)
              
              # Forward pass
              loss = self.model(text_input, audio_target)
              
              # Backward pass
              loss = loss / self.config.training.gradient_accumulation_steps
              loss.backward()
              
              # Optimizer step
              if (batch_idx + 1) % self.config.training.gradient_accumulation_steps == 0:
                  torch.nn.utils.clip_grad_norm_(
                      self.model.parameters(),
                      self.config.training.gradient_clip_norm
                  )
                  self.optimizer.step()
                  self.scheduler.step()
                  self.optimizer.zero_grad()
              
              total_loss += loss.item()
              
              # Logging
              if batch_idx % self.config.logging.log_every_n_steps == 0:
                  self.writer.add_scalar("Loss/train", loss.item(), epoch * len(self.train_loader) + batch_idx)
          
          return total_loss / len(self.train_loader)
      
      def validate(self, epoch: int):
          """Validate on val set."""
          self.model.eval()
          total_loss = 0
          
          with torch.no_grad():
              for batch in self.val_loader:
                  text_input = batch["text_input"].to(self.device)
                  audio_target = batch["audio"].to(self.device)
                  loss = self.model(text_input, audio_target)
                  total_loss += loss.item()
          
          avg_loss = total_loss / len(self.val_loader)
          self.writer.add_scalar("Loss/val", avg_loss, epoch)
          return avg_loss
      
      def generate_samples(self, epoch: int):
          """Generate audio samples for validation."""
          self.model.eval()
          sample_texts = [
              "OlÃ¡, este Ã© um teste de sÃ­ntese de voz.",
              "O fine-tuning estÃ¡ funcionando corretamente.",
              "PortuguÃªs brasileiro com XTTS versÃ£o dois."
          ]
          
          for idx, text in enumerate(sample_texts):
              audio = self.model.inference(text, language="pt")
              self.writer.add_audio(
                  f"Sample_{idx}",
                  audio,
                  epoch,
                  sample_rate=22050
              )
              # Save to disk
              output_path = self.config.logging.tensorboard_dir / f"epoch_{epoch}_sample_{idx}.wav"
              save_wav(audio, output_path, 22050)
      
      def train(self):
          """Main training loop."""
          best_val_loss = float('inf')
          patience_counter = 0
          
          for epoch in range(self.config.training.epochs):
              print(f"\nEpoch {epoch+1}/{self.config.training.epochs}")
              
              # Train
              train_loss = self.train_epoch(epoch)
              print(f"Train Loss: {train_loss:.4f}")
              
              # Validate
              val_loss = self.validate(epoch)
              print(f"Val Loss: {val_loss:.4f}")
              
              # Generate samples
              if epoch % self.config.logging.sample_every_n_epochs == 0:
                  self.generate_samples(epoch)
              
              # Save checkpoint
              if epoch % self.config.training.save_every_n_epochs == 0:
                  self.save_checkpoint(epoch, val_loss)
              
              # Early stopping
              if val_loss < best_val_loss:
                  best_val_loss = val_loss
                  patience_counter = 0
                  self.save_checkpoint(epoch, val_loss, is_best=True)
              else:
                  patience_counter += 1
                  if patience_counter >= self.config.training.early_stopping_patience:
                      print(f"Early stopping at epoch {epoch}")
                      break
          
          print("âœ… Training complete!")
      
      def save_checkpoint(self, epoch: int, val_loss: float, is_best: bool = False):
          """Save model checkpoint."""
          checkpoint_dir = Path("train/output/checkpoints")
          checkpoint_dir.mkdir(parents=True, exist_ok=True)
          
          checkpoint_path = checkpoint_dir / f"checkpoint_epoch_{epoch}.pth"
          torch.save({
              'epoch': epoch,
              'model_state_dict': self.model.state_dict(),
              'optimizer_state_dict': self.optimizer.state_dict(),
              'val_loss': val_loss,
              'config': self.config
          }, checkpoint_path)
          
          if is_best:
              best_path = checkpoint_dir / "best_model.pth"
              shutil.copy(checkpoint_path, best_path)
              print(f"âœ… Best model saved: {best_path}")
  
  
  if __name__ == "__main__":
      trainer = XTTSTrainer("train/config/train_config.yaml")
      trainer.train()
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Carrega modelo XTTS-v2 base
- âœ… Suporta LoRA e full fine-tune
- âœ… Training loop funcional com val loss
- âœ… Checkpoints salvos a cada N epochs
- âœ… TensorBoard logging
- âœ… Audio samples gerados para validaÃ§Ã£o manual
- âœ… Early stopping implementado

**Riscos**:
- ðŸ”´ **ALTO**: XTTS API pode mudar (pinned version)
- ðŸŸ¡ **MÃ‰DIO**: OOM se batch_size muito grande
- ðŸŸ¡ **MÃ‰DIO**: ConvergÃªncia pode ser lenta (ajustar LR)

---

#### 2.4 Testes de Treinamento
- [ ] Criar `tests/test_xtts_train.py`:
  ```python
  def test_load_model():
      """Test XTTS-v2 model loads correctly."""
      
  def test_lora_setup():
      """Test LoRA layers are added correctly."""
      
  def test_forward_pass():
      """Test forward pass with dummy data."""
      
  def test_checkpoint_save_load():
      """Test checkpoint save/load cycle."""
  ```
- [ ] Teste de smoke (1 epoch com mini-dataset)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Testes passam sem erros
- âœ… Smoke test completa em <5min

**Riscos**:
- ðŸŸ¢ **BAIXO**: Testes unitÃ¡rios sÃ£o isolados

---

### Sprint 2 - Deliverables

- [x] `train_config.yaml` completo
- [x] Modelo XTTS-v2 base baixado
- [x] `xtts_train.py` implementado e testado
- [x] Primeiro fine-tune rodado (smoke test)
- [x] Checkpoints salvos em `train/output/checkpoints/`
- [x] Audio samples em `train/output/samples/`
- [x] TensorBoard logs funcionando

---

## ðŸŽ¤ Sprint 3: IntegraÃ§Ã£o API + InferÃªncia

**Objetivo**: Integrar modelo fine-tunado na API existente e criar endpoints de inferÃªncia.

**DuraÃ§Ã£o**: 3-4 horas  
**Prioridade**: P1 (IMPORTANTE)

### Tasks

#### 3.1 Criar `train/scripts/xtts_inference.py`
- [ ] Wrapper de inferÃªncia:
  ```python
  # train/scripts/xtts_inference.py
  
  class XTTSInference:
      def __init__(self, checkpoint_path: Path = None):
          """
          Load XTTS-v2 model for inference.
          
          Args:
              checkpoint_path: Path to fine-tuned checkpoint.
                               If None, uses base model.
          """
          if checkpoint_path and checkpoint_path.exists():
              self.model = self._load_finetuned(checkpoint_path)
              logger.info(f"Loaded fine-tuned model: {checkpoint_path}")
          else:
              self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
              logger.info("Loaded base XTTS-v2 model")
          
          self.model.to("cuda" if torch.cuda.is_available() else "cpu")
      
      def synthesize(
          self,
          text: str,
          language: str = "pt",
          speaker_wav: Path = None,
          speed: float = 1.0
      ) -> np.ndarray:
          """
          Generate speech from text.
          
          Args:
              text: Input text
              language: Language code (pt, en, es, etc)
              speaker_wav: Reference audio for voice cloning
              speed: Speech speed multiplier
          
          Returns:
              Audio array (22050 Hz mono)
          """
          if speaker_wav:
              audio = self.model.tts_to_file(
                  text=text,
                  speaker_wav=str(speaker_wav),
                  language=language,
                  speed=speed,
                  file_path=None  # return array
              )
          else:
              audio = self.model.tts(text=text, language=language)
          
          return audio
      
      def clone_voice(
          self,
          text: str,
          reference_wav: Path,
          language: str = "pt"
      ) -> np.ndarray:
          """
          Clone voice from reference audio.
          
          Args:
              text: Text to synthesize
              reference_wav: Path to reference audio (3-10s)
              language: Target language
          
          Returns:
              Cloned audio (22050 Hz mono)
          """
          return self.synthesize(
              text=text,
              speaker_wav=reference_wav,
              language=language
          )
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Carrega modelo base ou fine-tunado
- âœ… SÃ­ntese de texto funciona
- âœ… Voice cloning funciona
- âœ… Retorna audio em 22050 Hz mono

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Checkpoint loading pode falhar (validar formato)

---

#### 3.2 Modificar `app/engines/xtts_engine.py`
- [ ] Adicionar suporte a checkpoint custom:
  ```python
  # app/engines/xtts_engine.py
  
  class XTTSEngine(TTSEngine):
      def __init__(self, config: Dict[str, Any]):
          super().__init__(config)
          
          # Check for custom checkpoint
          custom_checkpoint = os.getenv("XTTS_CUSTOM_CHECKPOINT")
          if custom_checkpoint and Path(custom_checkpoint).exists():
              logger.info(f"Loading custom XTTS checkpoint: {custom_checkpoint}")
              self.model = self._load_custom_checkpoint(custom_checkpoint)
          else:
              logger.info("Loading base XTTS-v2 model")
              self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
      
      def _load_custom_checkpoint(self, checkpoint_path: str):
          """Load fine-tuned checkpoint."""
          # Implementation similar to xtts_inference.py
          ...
  ```
- [ ] Atualizar `.env.example`:
  ```bash
  # XTTS Configuration
  XTTS_CUSTOM_CHECKPOINT=/app/train/output/checkpoints/best_model.pth
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Suporta modelo base e fine-tunado
- âœ… Env var funciona
- âœ… Fallback para base se checkpoint nÃ£o existir

**Riscos**:
- ðŸŸ¢ **BAIXO**: LÃ³gica simples

---

#### 3.3 Criar Endpoint SÃ­ncrono `/tts/synthesize`
- [ ] Adicionar em `app/main.py`:
  ```python
  @app.post("/tts/synthesize", response_class=StreamingResponse)
  async def synthesize_tts(
      text: str = Form(...),
      language: str = Form("pt-BR"),
      reference_audio: UploadFile = File(None),
      speed: float = Form(1.0)
  ):
      """
      SÃ­ntese de voz sÃ­ncrona (retorna WAV imediatamente).
      
      Args:
          text: Texto a ser sintetizado
          language: CÃ³digo do idioma (pt-BR, en-US, etc)
          reference_audio: (Opcional) Ãudio de referÃªncia para clonagem
          speed: Velocidade da fala (0.5-2.0)
      
      Returns:
          Audio WAV (22050 Hz mono)
      """
      try:
          # Get engine
          engine = engine_factory.get_engine("xtts")
          
          # Save reference audio if provided
          speaker_wav = None
          if reference_audio:
              speaker_wav = Path(f"/tmp/ref_{uuid.uuid4()}.wav")
              with open(speaker_wav, "wb") as f:
                  f.write(await reference_audio.read())
          
          # Synthesize
          audio = engine.synthesize(
              text=text,
              language=language.split("-")[0],  # pt-BR -> pt
              speaker_wav=speaker_wav,
              speed=speed
          )
          
          # Convert to WAV bytes
          wav_bytes = audio_array_to_wav(audio, sample_rate=22050)
          
          # Cleanup
          if speaker_wav and speaker_wav.exists():
              speaker_wav.unlink()
          
          return StreamingResponse(
              io.BytesIO(wav_bytes),
              media_type="audio/wav",
              headers={
                  "Content-Disposition": f"attachment; filename=synthesized_{int(time.time())}.wav"
              }
          )
      
      except Exception as e:
          logger.exception("Synthesis failed")
          raise HTTPException(status_code=500, detail=str(e))
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Endpoint `/tts/synthesize` funciona
- âœ… Aceita texto + optional reference audio
- âœ… Retorna WAV diretamente
- âœ… Tratamento de erros

**Riscos**:
- ðŸŸ¡ **MÃ‰DIO**: Requests grandes podem timeout (limitar texto)

---

#### 3.4 Documentar API
- [ ] Atualizar `docs/api-reference.md`:
  ```markdown
  ## POST /tts/synthesize
  
  SÃ­ntese de voz sÃ­ncrona usando XTTS-v2.
  
  ### Request
  
  **Form Data:**
  - `text` (required): Texto a sintetizar (max 500 chars)
  - `language` (optional): CÃ³digo idioma (default: pt-BR)
  - `reference_audio` (optional): Arquivo WAV para clonagem
  - `speed` (optional): Velocidade (0.5-2.0, default: 1.0)
  
  ### Example
  
  ```bash
  # Simple TTS
  curl -X POST http://localhost:8005/tts/synthesize \
    -F "text=OlÃ¡, este Ã© um teste" \
    -F "language=pt-BR" \
    -o output.wav
  
  # Voice cloning
  curl -X POST http://localhost:8005/tts/synthesize \
    -F "text=Texto com voz clonada" \
    -F "reference_audio=@reference.wav" \
    -o cloned.wav
  ```
  
  ### Response
  
  - Status: 200 OK
  - Content-Type: audio/wav
  - Body: WAV file (22050 Hz mono)
  ```

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Docs claras com exemplos curl
- âœ… Swagger UI atualizado (automÃ¡tico via FastAPI)

**Riscos**:
- ðŸŸ¢ **BAIXO**: Apenas documentaÃ§Ã£o

---

### Sprint 3 - Deliverables

- [x] `xtts_inference.py` implementado
- [x] `xtts_engine.py` suporta checkpoint custom
- [x] Endpoint `/tts/synthesize` funcionando
- [x] API documentada
- [x] Testes manuais com curl
- [x] README atualizado com link para Swagger

---

## âœ… Sprint 4: Qualidade & Testes

**Objetivo**: Garantir qualidade de cÃ³digo e cobertura de testes.

**DuraÃ§Ã£o**: 4-5 horas  
**Prioridade**: P1 (IMPORTANTE)

### Tasks

#### 4.1 Testes do Pipeline de Dados
- [ ] `tests/test_download_youtube.py`
- [ ] `tests/test_segment_audio.py`
- [ ] `tests/test_transcribe.py`
- [ ] `tests/test_build_metadata.py`
- [ ] `tests/test_pipeline_integration.py` (end-to-end)

**CritÃ©rios de AceitaÃ§Ã£o**:
- âœ… Coverage > 80% nos scripts de pipeline
- âœ… Testes passam em CI

---

#### 4.2 Testes de Treinamento
- [ ] `tests/test_xtts_train.py`
- [ ] Smoke test (1 epoch, mini-dataset)

---

#### 4.3 Testes de API
- [ ] `tests/test_api_synthesize.py`
- [ ] `tests/test_voice_cloning_endpoint.py`
- [ ] `tests/test_custom_checkpoint_loading.py`

---

#### 4.4 Configurar Linting
- [ ] Adicionar pre-commit:
  ```yaml
  # .pre-commit-config.yaml
  repos:
    - repo: https://github.com/psf/black
      rev: 24.3.0
      hooks:
        - id: black
    
    - repo: https://github.com/pycqa/isort
      rev: 5.13.2
      hooks:
        - id: isort
    
    - repo: https://github.com/pre-commit/pre-commit-hooks
      rev: v4.5.0
      hooks:
        - id: trailing-whitespace
        - id: end-of-file-fixer
        - id: check-yaml
  ```
- [ ] Rodar em todo o cÃ³digo:
  ```bash
  black .
  isort .
  ```

---

#### 4.5 Type Hints
- [ ] Adicionar hints em todos os scripts de `train/`
- [ ] Configurar mypy:
  ```ini
  [mypy]
  python_version = 3.11
  warn_return_any = True
  warn_unused_configs = True
  disallow_untyped_defs = True
  ```

---

### Sprint 4 - Deliverables

- [x] Cobertura de testes > 80%
- [x] Linting configurado (black, isort)
- [x] Type hints completos
- [x] CI pipeline bÃ¡sico (GitHub Actions)

---

## ðŸ“š Sprint 5: Docs & DevOps

**Objetivo**: DocumentaÃ§Ã£o completa e melhorias de DevOps.

**DuraÃ§Ã£o**: 2-3 horas  
**Prioridade**: P2 (NICE TO HAVE)

### Tasks

#### 5.1 Criar `ENV_SETUP.md`
- [ ] Guia de setup completo:
  - InstalaÃ§Ã£o de dependÃªncias
  - CriaÃ§Ã£o de venv
  - ConfiguraÃ§Ã£o VSCode
  - Download de modelos

---

#### 5.2 Criar `CONTRIBUTING.md`
- [ ] Guidelines para contribuidores
- [ ] Code style
- [ ] Commit conventions

---

#### 5.3 Atualizar README.md
- [ ] SeÃ§Ã£o "Quick Start" atualizada
- [ ] Link para Swagger docs
- [ ] Badges (build status, coverage)

---

#### 5.4 Criar Diagramas
- [ ] Pipeline de dados (Mermaid)
- [ ] Arquitetura do sistema
- [ ] Fluxo de treinamento

---

#### 5.5 GitHub Actions CI
- [ ] Workflow bÃ¡sico:
  ```yaml
  # .github/workflows/ci.yml
  name: CI
  
  on: [push, pull_request]
  
  jobs:
    test:
      runs-on: ubuntu-latest
      steps:
        - uses: actions/checkout@v3
        - uses: actions/setup-python@v4
          with:
            python-version: '3.11'
        - run: pip install -r requirements.txt -r requirements-dev.txt
        - run: pytest
        - run: black --check .
        - run: mypy .
  ```

---

### Sprint 5 - Deliverables

- [x] `ENV_SETUP.md` completo
- [x] `CONTRIBUTING.md` criado
- [x] README atualizado com quick start
- [x] Diagramas de arquitetura
- [x] CI pipeline funcionando

---

## ðŸ“ˆ MÃ©tricas de Sucesso

### Sprint 1
- [ ] Dataset LJSpeech gerado (mÃ­n 1h de Ã¡udio pt-BR)
- [ ] Pipeline roda end-to-end sem erros
- [ ] Tempo de processamento < 2h para 1h de Ã¡udio

### Sprint 2
- [ ] Modelo fine-tuna sem OOM
- [ ] Val loss converge (decresce por > 10 epochs)
- [ ] Audio samples tÃªm qualidade aceitÃ¡vel (validaÃ§Ã£o manual)

### Sprint 3
- [ ] API `/tts/synthesize` responde em < 5s para texto curto
- [ ] Voice cloning funciona com latÃªncia < 10s
- [ ] Modelo custom carrega corretamente

### Sprint 4
- [ ] Coverage > 80%
- [ ] Zero warnings de linting
- [ ] Todos os testes passam

### Sprint 5
- [ ] Docs completas (README + contributing)
- [ ] CI passa em todos os PRs
- [ ] Novos devs conseguem setup em < 30min

---

## ðŸš§ Riscos Globais

### TÃ©cnicos
- **XTTS API breaking changes** â†’ Pinned version (coqui-tts==0.27.0)
- **OOM durante treino** â†’ LoRA + gradient accumulation
- **Whisper lento** â†’ Usar GPU, modelo small
- **Dataset pequeno** â†’ ComeÃ§ar com 1-2h, expandir depois

### Processo
- **Sprints muito longas** â†’ Dividir tasks maiores
- **Falta de validaÃ§Ã£o manual** â†’ Audio samples a cada epoch
- **DocumentaÃ§Ã£o desatualizada** â†’ Update docs em cada sprint

---

## ðŸ“ Notas Finais

- **Priorizar Sprint 0 e 1** antes de qualquer outra coisa
- **Commits frequentes** (atomic commits por task)
- **Testes manuais** antes de marcar task como done
- **Documentar decisÃµes tÃ©cnicas** em comentÃ¡rios/docstrings

**PrÃ³xima aÃ§Ã£o**: Executar **Sprint 0 - Task 1** (auditoria de secrets).

---

**Ãšltima atualizaÃ§Ã£o**: 2025-12-06  
**Mantido por**: Tech Lead (Claude Sonnet 4.5)
