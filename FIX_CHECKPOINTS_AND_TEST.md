# ğŸ”§ CorreÃ§Ãµes Implementadas - Checkpoints e Test Script

**Data:** 2025-12-06  
**Status:** âœ… **RESOLVIDO**

---

## ğŸ“‹ Problemas Corrigidos

### 1. âŒ Checkpoint `pretrained_model_last.pt` criado incorretamente

**Problema:**
Arquivo `pretrained_model_last.pt` sendo criado na pasta de output junto com `model_last.pt`.

**Causa:**
A biblioteca F5-TTS (`/root/.local/lib/python3.11/site-packages/f5_tts/train/finetune_cli.py` linha 147-150) automaticamente adiciona o prefixo `pretrained_` ao copiar checkpoints para fine-tuning:

```python
file_checkpoint = os.path.basename(ckpt_path)
if not file_checkpoint.startswith("pretrained_"):
    file_checkpoint = "pretrained_" + file_checkpoint  # â† Adiciona prefixo
file_checkpoint = os.path.join(checkpoint_path, file_checkpoint)
if not os.path.isfile(file_checkpoint):
    shutil.copy2(ckpt_path, file_checkpoint)
```

**Comportamento:**
Quando passamos `model_last.pt` para continuar o treinamento, a lib copia para `pretrained_model_last.pt`.

**SoluÃ§Ã£o:**
Isso Ã© **comportamento esperado** da lib F5-TTS original. NÃ£o Ã© um bug, mas sim um "backup" que a biblioteca cria. 

**AÃ§Ã£o:** Script de limpeza criado (`train/scripts/cleanup_checkpoints.sh`) para remover checkpoints duplicados.

---

### 2. âŒ Test script usando sample inexistente (hardcoded)

**Problema Original:**
```python
ref_audio_path = SAMPLES_DIR / "update_33200_ref.wav"  # âŒ Hardcoded e nÃ£o existe
```

**Erro:**
```
âŒ Ãudio de referÃªncia nÃ£o encontrado: .../samples/update_33200_ref.wav
```

**Causa:**
- Path hardcoded para `update_33200_ref.wav` 
- Samples reais sÃ£o: `update_25200_ref.wav`, `update_25400_ref.wav`, etc.
- Update number muda a cada treinamento

**SoluÃ§Ã£o Implementada:**

```python
# âœ… CORREÃ‡ÃƒO: Buscar sample mais recente automaticamente
samples_list = sorted(SAMPLES_DIR.glob("update_*_ref.wav"), reverse=True)

if not samples_list:
    print(f"âŒ Nenhum sample de referÃªncia encontrado em: {SAMPLES_DIR}")
    print(f"\nğŸ’¡ Dica: Execute o treinamento primeiro para gerar samples:")
    print(f"   python3 -m train.run_training --epochs 1000 --batch-size 2")
    return 1

ref_audio_path = samples_list[0]  # Mais recente
update_num = ref_audio_path.stem.split("_")[1]  # Extrair nÃºmero do update

print(f"\nâœ… Ãudio de referÃªncia (update {update_num}): {ref_audio_path.name}")
```

**BenefÃ­cios:**
- âœ… Sempre usa o sample mais recente disponÃ­vel
- âœ… Funciona independente do nÃºmero de updates
- âœ… Mensagem de erro Ãºtil se nÃ£o houver samples
- âœ… Exibe qual update estÃ¡ sendo usado

---

## ğŸ¯ Estrutura de Arquivos Correta

### Checkpoints (train/output/ptbr_finetuned2/)

```
âœ… CORRETOS (mantidos):
â”œâ”€â”€ model_last.pt              # Checkpoint mais recente (sempre sobrescrito)
â”œâ”€â”€ model_25200.pt            # Checkpoint do update 25200
â”œâ”€â”€ model_25400.pt            # Checkpoint do update 25400
â””â”€â”€ model_{N}.pt              # Checkpoints numerados (rotaÃ§Ã£o de 5)

âš ï¸ DUPLICADOS (podem ser removidos):
â”œâ”€â”€ pretrained_model_last.pt       # CÃ³pia feita pela lib F5-TTS
â””â”€â”€ pretrained_model_200000.pt     # Modelo inicial baixado
```

### Samples (train/output/ptbr_finetuned2/samples/)

```
âœ… CORRETOS:
â”œâ”€â”€ update_25200_gen.wav      # Ãudio gerado pelo modelo (update 25200)
â”œâ”€â”€ update_25200_ref.wav      # Ãudio de referÃªncia (ground truth)
â”œâ”€â”€ update_25400_gen.wav
â”œâ”€â”€ update_25400_ref.wav
â””â”€â”€ update_{N}_*.wav          # Samples de cada checkpoint
```

**PadrÃ£o de nomenclatura:**
- `update_{N}_gen.wav`: Ãudio **gerado** pelo modelo
- `update_{N}_ref.wav`: Ãudio de **referÃªncia** (do dataset)

---

## ğŸ§¹ Limpeza de Checkpoints Duplicados

### Script AutomÃ¡tico

```bash
# Executar limpeza
./train/scripts/cleanup_checkpoints.sh
```

**O que faz:**
- Remove todos os `pretrained_model_*.pt`
- Remove metadatas correspondentes (`.metadata.json`)
- Exibe espaÃ§o liberado
- MantÃ©m checkpoints importantes (`model_*.pt`)

### Limpeza Manual

```bash
cd train/output/ptbr_finetuned2

# Remover checkpoints com prefixo pretrained_
rm -f pretrained_model_*.pt
rm -f pretrained_model_*.metadata.json

# Verificar espaÃ§o liberado
du -sh .
```

**EspaÃ§o economizado:** ~5-10GB por checkpoint duplicado

---

## âœ… ValidaÃ§Ã£o das CorreÃ§Ãµes

### Teste 1: Script de GeraÃ§Ã£o de Ãudio

```bash
# Testar com checkpoint especÃ­fico
python3 -m train.test --checkpoint model_25400.pt
```

**Resultado:**
```
âœ… Ãudio de referÃªncia (update 25400): update_25400_ref.wav
ğŸ“Š Sample rate: 24000 Hz | Duration: 9.99s

================================================================================
âœ… ÃUDIO GERADO COM SUCESSO!
================================================================================
ğŸ’¾ Arquivo: train/f5tts_test_20251206_112056.wav
â±ï¸  Tempo de geraÃ§Ã£o: 7.34s
ğŸ“Š DuraÃ§Ã£o do Ã¡udio: 31.52s
ğŸ“Š RTF (Real-Time Factor): 0.23x (4.3x mais rÃ¡pido que tempo real!)
```

### Teste 2: Checkpoint Auto-Resume

```bash
# Reiniciar treinamento - deve continuar do model_last.pt
python3 -m train.run_training --epochs 1000 --batch-size 2
```

**Resultado:**
```
âœ… Using Last model
   Path: train/output/ptbr_finetuned2/model_last.pt
   
âœ… Auto-resume from: model_last.pt
ğŸ”„ Modo: Continuar treinamento do checkpoint

Epoch 29/1000: ... [loss=0.868, update=25422]  âœ…
```

---

## ğŸ“Š ComparaÃ§Ã£o: Antes vs Depois

| Aspecto | âŒ Antes | âœ… Depois |
|---------|----------|-----------|
| **Test script** | Hardcoded `update_33200_ref.wav` | Auto-detecta sample mais recente |
| **Samples** | Erro se nÃ£o existir | Mensagem Ãºtil + dica de comando |
| **Checkpoints** | `pretrained_model_last.pt` duplicado | Entendido como backup da lib |
| **Limpeza** | Manual | Script automatizado |
| **Mensagens de erro** | GenÃ©rica | EspecÃ­fica com update number |

---

## ğŸš€ Uso Atualizado

### Gerar Ãudio com Checkpoint EspecÃ­fico

```bash
# Usar checkpoint especÃ­fico
python3 -m train.test --checkpoint model_25400.pt

# Usar Ãºltimo checkpoint (padrÃ£o)
python3 -m train.test

# Custom text
python3 -m train.test --text "OlÃ¡, teste de voz em portuguÃªs brasileiro."

# CPU fallback
python3 -m train.test --device cpu
```

### Limpar Checkpoints Duplicados

```bash
# Script automÃ¡tico
./train/scripts/cleanup_checkpoints.sh

# OU manual
cd train/output/ptbr_finetuned2
rm -f pretrained_model_*.pt pretrained_model_*.metadata.json
```

### Monitorar Samples Gerados

```bash
# Ver samples disponÃ­veis
ls -lht train/output/ptbr_finetuned2/samples/

# Ouvir Ãºltimo sample gerado
ls -t train/output/ptbr_finetuned2/samples/*_gen.wav | head -1 | \
  xargs -I {} ffplay -nodisp -autoexit {}

# Comparar com referÃªncia
LAST=$(ls -t train/output/ptbr_finetuned2/samples/*_gen.wav | head -1)
UPDATE=$(basename $LAST | cut -d_ -f2)
echo "Generated: $LAST"
echo "Reference: train/output/ptbr_finetuned2/samples/update_${UPDATE}_ref.wav"
```

---

## ğŸ“š Arquivos Modificados

### 1. `train/test.py`

**Linhas 153-167:** DetecÃ§Ã£o automÃ¡tica de sample de referÃªncia

```python
# Antes
ref_audio_path = SAMPLES_DIR / "update_33200_ref.wav"
if not ref_audio_path.exists():
    print(f"âŒ Ãudio de referÃªncia nÃ£o encontrado: {ref_audio_path}")
    return 1

# Depois
samples_list = sorted(SAMPLES_DIR.glob("update_*_ref.wav"), reverse=True)
if not samples_list:
    print(f"âŒ Nenhum sample encontrado em: {SAMPLES_DIR}")
    print(f"\nğŸ’¡ Execute treinamento primeiro: python3 -m train.run_training")
    return 1

ref_audio_path = samples_list[0]  # Mais recente
update_num = ref_audio_path.stem.split("_")[1]
print(f"\nâœ… Ãudio de referÃªncia (update {update_num}): {ref_audio_path.name}")
```

### 2. `train/scripts/cleanup_checkpoints.sh` (NOVO)

Script para remover checkpoints duplicados com prefixo `pretrained_`.

---

## ğŸ” Entendendo o Prefixo `pretrained_`

### Por que existe?

A lib F5-TTS usa essa convenÃ§Ã£o para diferenciar:

- **`model_*.pt`**: Checkpoints gerados durante o treinamento atual
- **`pretrained_model_*.pt`**: Checkpoints usados como base (fine-tuning)

### Quando Ã© criado?

Sempre que vocÃª inicia fine-tuning com `--pretrain path/to/checkpoint.pt`:

```bash
# VocÃª passa: model_last.pt
python3 -m train.run_training --pretrain model_last.pt

# Lib cria cÃ³pia: pretrained_model_last.pt
# E treina a partir dela
```

### Ã‰ necessÃ¡rio?

**NÃ£o para usuÃ¡rio final.** Ã‰ um backup interno da biblioteca. Pode ser removido apÃ³s treinamento comeÃ§ar.

### Impacto no disco

Cada checkpoint: **~5GB**
- `model_last.pt`: 5.1GB âœ… NecessÃ¡rio
- `pretrained_model_last.pt`: 5.1GB âš ï¸ Duplicado (pode remover)
- `pretrained_model_200000.pt`: 5.1GB âš ï¸ Modelo base (pode remover apÃ³s treinar)

**Total recuperÃ¡vel:** ~10GB

---

## âœ… Status Final

- âœ… **Test script corrigido** - Auto-detecta samples mais recentes
- âœ… **Checkpoints duplicados** - Entendidos como backup da lib
- âœ… **Script de limpeza** - Automatiza remoÃ§Ã£o de duplicados
- âœ… **Mensagens Ãºteis** - Erros explicam o que fazer
- âœ… **ValidaÃ§Ã£o completa** - Teste passou com sucesso

---

## ğŸ¯ PrÃ³ximos Passos

1. âœ… **CorreÃ§Ãµes aplicadas** - Test script e limpeza funcionando
2. ğŸ”„ **Continuar treinamento** - Sistema stable, pode treinar
3. ğŸ§ **Testar qualidade** - Comparar `_gen.wav` vs `_ref.wav`
4. ğŸ§¹ **Limpar disco** - Executar `cleanup_checkpoints.sh` periodicamente

---

**DocumentaÃ§Ã£o gerada automaticamente**  
**Data:** 2025-12-06 11:22
