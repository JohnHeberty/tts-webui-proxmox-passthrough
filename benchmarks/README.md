# Sprint 8: Benchmarks PT-BR

## ğŸ“‹ VisÃ£o Geral

Sprint 8 implementa **benchmarking qualitativo** para comparar XTTS vs F5-TTS em portuguÃªs brasileiro.

### Objetivos

1. **MOS Testing** (Mean Opinion Score) - AvaliaÃ§Ã£o humana de qualidade
2. **ComparaÃ§Ã£o Quantitativa** - MÃ©tricas objetivas (RTF, VRAM, latÃªncia)
3. **AnÃ¡lise PT-BR** - Foco em caracterÃ­sticas do portuguÃªs brasileiro
4. **RecomendaÃ§Ãµes** - Quando usar cada engine

---

## ğŸ¯ Metodologia

### 1. Dataset PT-BR

**ComposiÃ§Ã£o:**
- 20 textos variados (curtos, mÃ©dios, longos)
- 10 vozes de referÃªncia (5 masculinas, 5 femininas)
- Cobertura de sotaques brasileiros (SP, RJ, MG, RS, NE)
- Casos de uso: narraÃ§Ã£o, diÃ¡logo, podcast, audiobook

### 2. MOS Testing

**Mean Opinion Score (1-5):**
- 1: PÃ©ssimo (ininteligÃ­vel)
- 2: Ruim (inteligÃ­vel mas com muitos problemas)
- 3: Regular (aceitÃ¡vel, alguns problemas)
- 4: Bom (alta qualidade, poucos problemas)
- 5: Excelente (qualidade profissional)

**CritÃ©rios avaliados:**
- **Naturalidade**: QuÃ£o natural soa a voz
- **Inteligibilidade**: Clareza e compreensÃ£o
- **ProsÃ³dia**: Ritmo, entonaÃ§Ã£o, pausas
- **Fidelidade** (cloning): SemelhanÃ§a com voz original
- **PreferÃªncia Geral**: Qual vocÃª escolheria?

### 3. MÃ©tricas Objetivas

- **RTF** (Real-Time Factor): Velocidade de processamento
- **VRAM**: Uso de memÃ³ria GPU
- **LatÃªncia**: Tempo atÃ© primeiro Ã¡udio
- **Qualidade de Ãudio**: Sample rate, normalizaÃ§Ã£o, artefatos

---

## ğŸ“Š Estrutura de Arquivos

```
benchmarks/
â”œâ”€â”€ README.md                 # Este arquivo
â”œâ”€â”€ dataset_ptbr.json        # Dataset de teste
â”œâ”€â”€ run_benchmark.py         # Script principal
â”œâ”€â”€ analyze_results.py       # AnÃ¡lise estatÃ­stica
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ xtts_outputs/        # Ãudios XTTS
â”‚   â”œâ”€â”€ f5tts_outputs/       # Ãudios F5-TTS
â”‚   â”œâ”€â”€ metrics.csv          # MÃ©tricas quantitativas
â”‚   â””â”€â”€ mos_scores.csv       # Scores MOS
â””â”€â”€ reports/
    â”œâ”€â”€ benchmark_report.pdf # RelatÃ³rio final
    â””â”€â”€ visualizations/      # GrÃ¡ficos comparativos
```

---

## ğŸš€ Como Executar

### 1. Preparar Dataset

```bash
cd benchmarks
python prepare_dataset.py
```

Isso irÃ¡:
- Criar `dataset_ptbr.json` com textos PT-BR
- Baixar vozes de referÃªncia (se configurado)
- Validar estrutura do dataset

### 2. Rodar Benchmark

```bash
# Gerar todos os Ã¡udios (XTTS + F5-TTS)
python run_benchmark.py --all

# Apenas XTTS
python run_benchmark.py --engine xtts

# Apenas F5-TTS
python run_benchmark.py --engine f5tts

# Com GPU especÃ­fica
python run_benchmark.py --all --device cuda:0
```

### 3. Coletar MOS Scores

**OpÃ§Ã£o A: Interface Web (Recomendado)**
```bash
python mos_webapp.py
# Acesse http://localhost:8080
```

**OpÃ§Ã£o B: Manual CSV**
- Editar `results/mos_scores.csv`
- Adicionar scores para cada Ã¡udio

### 4. Analisar Resultados

```bash
python analyze_results.py

# Gera:
# - reports/benchmark_report.pdf
# - reports/visualizations/*.png
# - EstatÃ­sticas no terminal
```

---

## ğŸ“ˆ Exemplo de Dataset

```json
{
  "texts": [
    {
      "id": "short_01",
      "text": "OlÃ¡, como vai vocÃª?",
      "category": "short",
      "use_case": "dialogue"
    },
    {
      "id": "medium_01",
      "text": "O Brasil Ã© um paÃ­s de dimensÃµes continentais...",
      "category": "medium",
      "use_case": "narration"
    },
    {
      "id": "long_01",
      "text": "Era uma vez, em um reino muito distante...",
      "category": "long",
      "use_case": "audiobook"
    }
  ],
  "voices": [
    {
      "id": "voice_m_sp_01",
      "gender": "male",
      "accent": "sao_paulo",
      "audio_path": "voices/male_sp_01.wav",
      "ref_text": "Esta Ã© uma amostra de voz masculina..."
    }
  ]
}
```

---

## ğŸ“Š MÃ©tricas Esperadas

### Performance (GPU RTX 3090)

| Engine  | RTF (mÃ©dia) | VRAM (GB) | LatÃªncia (s) |
|---------|-------------|-----------|--------------|
| XTTS    | 0.3 - 0.8x  | 2-4 GB    | 3-5s         |
| F5-TTS  | 0.5 - 1.2x  | 3-5 GB    | 5-8s         |

### Qualidade Esperada (MOS)

| CritÃ©rio        | XTTS | F5-TTS |
|-----------------|------|--------|
| Naturalidade    | 3.8  | 4.2    |
| Inteligibilidade| 4.2  | 4.0    |
| ProsÃ³dia        | 3.5  | 4.3    |
| Fidelidade      | 4.0  | 3.8    |
| **Geral**       | **3.9** | **4.1** |

**Nota:** Valores estimados, resultados reais podem variar.

---

## ğŸ¯ CritÃ©rios de AceitaÃ§Ã£o Sprint 8

### Funcional
- [ ] Dataset PT-BR preparado (20 textos + 10 vozes)
- [ ] Script `run_benchmark.py` funcional
- [ ] Ãudios gerados (XTTS + F5-TTS)
- [ ] MOS scores coletados (min 5 avaliadores)
- [ ] AnÃ¡lise estatÃ­stica completa

### Qualidade
- [ ] Cobertura PT-BR representativa
- [ ] MÃ©tricas objetivas validadas
- [ ] AnÃ¡lise estatÃ­stica robusta (t-test, p-value)
- [ ] VisualizaÃ§Ãµes claras

### DocumentaÃ§Ã£o
- [ ] README completo
- [ ] RelatÃ³rio PDF com conclusÃµes
- [ ] RecomendaÃ§Ãµes de uso

---

## âš ï¸ LimitaÃ§Ãµes

### Sprint 8 Ã© Semi-Operacional

Diferente dos Sprints 1-7 (100% automatizados), Sprint 8 requer:

1. **Infraestrutura:**
   - GPU para gerar Ã¡udios
   - Storage para Ã¡udios (pode ser ~1-2GB)

2. **Recursos Humanos:**
   - Painel de avaliadores (5-10 pessoas)
   - Tempo de avaliaÃ§Ã£o (~30min por pessoa)

3. **Dados Reais:**
   - Vozes de referÃªncia PT-BR reais
   - Textos representativos

### Alternativa: Benchmark Simplificado

Se recursos limitados, executar versÃ£o simplificada:
- 5 textos em vez de 20
- 2 vozes em vez de 10
- Auto-avaliaÃ§Ã£o MOS (1 pessoa)

---

## ğŸ“ PrÃ³ximos Passos

ApÃ³s Sprint 8:
- **Sprint 9:** Documentation (README, API docs, migration guide)
- **Sprint 10:** Gradual Rollout (staging, monitoring, production)

---

**Autor:** Sistema F5-TTS Multi-Engine  
**Data:** 27 de Novembro de 2025  
**Sprint:** 8/10 - Benchmarks PT-BR
